 %Bismillahi-r-Rahmani-r-Rahim
% \documentclass[12pt]{report}


% \begin{document}
 

 \chapter{Mathematical Methods for Computational Linguistics}
 \section{Introduction}
 
In this chapter, we give an overview of the areas of mathematics that are essential to understanding what follows, defining the key concepts, and concentrating specifically on those that are currently already used within computational linguistics.
 
The reader may prefer to skim this chapter, returning as necessary as the concepts are required in the sequel. In this section, we motivate the remainder of the chapter, explaining the importance of the concepts introduced both generally and for computational linguistics.

In section \ref{vectors} we introduce vector spaces. These are used almost ubiquitously in modern computational linguistics and information retrieval, often without a thorough understanding of the underlying definitions, and the corresponding generality of the class of objects studied. For example, the real and complex numbers, polynomials, functions and matrices all form vector spaces; the study of vector spaces is thus integral to wide areas of mathematics.

Our interpretation of vectors is this: meaning has a vector nature. This is justified by the use of vectors to represent meaning, for example via latent semantic analysis \cite{Deerwester:90}, or measures of distributional similarity \cite{Lee}, where words are commonly represented as a vector of ``features'' in order to obtain measures of similarity. It is also justified by theoretical work of ours to be discussed later (see section \ref{ontologies}), in which we show that the structures used in traditional ontological representations of meaning, \emph{semilattices} and \emph{lattices}, can be incorporated into a \emph{vector lattice}.

If meaning has a vector nature, then words seem to ``move'' meanings. If we assume certain properties of these movements, we can represent words as \emph{linear transformations} or \emph{operators} on vector spaces. These are discussed in section \ref{operators}.

Meaning has traditionally been represented (in work on ontologies) by structures resembling those studied in section \ref{lattices}; namely \emph{lattices}. Lattices and semilattices generalise Boolean algebras, and so can be thought of a generalised logic. They are also important in the algebraic formulation of the Lambek calculus \cite{Lambek}, namely \emph{residuated lattices}.

These two concepts of meaning---the vector one and the lattice one---are merged in the study of \emph{vector lattices} which are introduced in section \ref{vector-lattices}. These enable us to provide a unifying framework in which to study meaning.
 
 \section{Preliminaries}

\begin{defn}[Cartesian Product]
\index{Cartesian product|textbf}
The Cartesian product $A \times B$ of two sets $A$ and $B$ is the set of all pairs $(a,b)$ such that $a \in A$ and $b \in B$.
\end{defn}

\begin{defn}[Relation, Function]
\index{relation|textbf}
A relation $R$ between two sets $A$ and $B$ is a subset of the Cartesian product $A \times B$. if $(a,b) \in R$ then we write $aRb$. A \emph{relation on} $A$ is a subset of $A \times A$. A \emph{function} $f$ between $A$ and $B$ is a relation between $A$ and $B$ such that if $(a,b_1) \in f$ and $(a,b_2) \in f$ then $b_1 = b_2$. The unique element of $B$ associated with an element $a$ of $A$ is denoted $f(a)$.
\end{defn}

\begin{defn}[Semigroup]
A \emph{binary operation} on a set $S$ is a function from $S\times S$ to $S$. The value of the binary operation $\cdot$ on two elements $x$ and $y$ in $S$ is denoted $x \cdot y$. A semigroup $(S,\cdot)$ is a set $S$ with a binary operation $\cdot$ which is \emph{associative}:
$$(x\cdot y)\cdot z = x \cdot (y \cdot z).$$
This product is often denoted $x\cdot y \cdot z$ or simply $xyz$.
\end{defn}
 
\section{Vector Spaces}
\label{vectors}

In this section, we define a series of concepts relating to different types of vector space. The most important is the immediately following general definition. The remainder of this section defines concepts that are of varying importance in different parts of the thesis.

It is our view that a definition in itself is of little value on its own, and that in trying to understand them time is much better spent getting a feel for concepts by examining examples; we try where possible therefore to introduce these following definitions.

\begin{defn}[Vector Space]
A vector space over a field $F$ is a set $V$ with two operations: addition, $V \times V \rightarrow V$, denoted $u + v$ where $u,v \in V$, and scalar multiplication: $F \times V \rightarrow V$, denoted $\alpha v$ where $\alpha \in F$ and $v \in V$,
satisfying the following conditions:
\begin{itemize}
\item $V$ is closed under addition and scalar multiplication;
\item the vector space under addition forms an \emph{abelian group}: addition is associative and commutative and there is an additive identity $0 \in V$ such that for every element $v \in V$ there is an element $-v$ such that $v + (-v) = 0$;
\item scalar multiplication is associative: $\alpha (\beta v) = (\alpha \beta) v$ for $\alpha, \beta \in F$ and $v \in V$;
\item $1v = v$ where $1$ is the multiplicative identity of $F$;
\item scalar multiplication is distributive with respect to vector and scalar addition:
\begin{eqnarray*}
\alpha(u + v) & = & \alpha u + \alpha v\\
(\alpha + \beta)v & = & \alpha v + \beta v
\end{eqnarray*}
\end{itemize}
When the field $F$ is that of the complex numbers $\mathbb{C}$, the vector space is called `complex'.
\end{defn}

\begin{example}
The most important examples for computational linguists are the $n$-dimensional real vector spaces, denoted $\R^n$. An element of $\R^n$ is denoted
$$x = (x_1,x_2,\ldots x_n),$$
where the $x_i$ are the real valued \emph{components} of $x$. The operations on $\R^n$ are defined as follows:
\begin{eqnarray*}
x + y & = & (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n)\\
\alpha x & = & (\alpha x_1, \alpha x_2, \ldots, \alpha x_n)\\
0 & = & (0,0,\ldots, 0)\\
-x & = & (-x_1, -x_2, \ldots, -x_n)
\end{eqnarray*}
\end{example}

\subsection{Notions of Distance}

The following sequence of definitions are to do with the notion of ``distance'' and ``size'' of objects. These concepts are of key importance in computational linguistics because we are often interested in ``distances'' between words---for example semantic distance. The types of space, in order of generality, are \emph{metric space}, \emph{normed space} and \emph{inner product space}.
\begin{defn}[Metric]
A metric $d$ is a function on a set $X$ satisfying:
\begin{center}
\begin{tabular}{ll}
$d(x,y) \ge 0$ & (non-negativity)\\
$d(x,y) = 0$ if and only if $x=y$ & (identity of indiscernibles)\\
$d(x,y) = d(y,x)$ & (symmetry)\\
$d(x,z) \le d(x,y) + d(y,z)$ & (triangle inequality)
\end{tabular}
\end{center}
for all $x,y,z \in X$. A metric space is a set $X$ together with a metric $d$.
\end{defn}

The definition of metric is very general: it does not require the set $X$ to be a vector space. In contrast, a more common way of defining distances on a vector space is via a \emph{norm}:
\begin{defn}[Norm]
If $V$ is a vector space over a field $F$ which is a subfield of the complex numbers, a norm $\|\cdot\|$ is a function from $V$ to the real numbers satisfying:
\begin{center}
\begin{tabular}{ll}
$\|x\| \ge 0$ & (positivity)\\
$\|\alpha x\| = |\alpha|\cdot\|x\|$ & (positive scalability)\\
$\|x + y\| \le \|x\| + \|y\|$ & (triangle inequality)\\
$\|x\| = 0$ if and only if $x = 0$ & (positive definiteness)
\end{tabular}
\end{center}
A \emph{normed vector space} is a vector space together with a norm.
\end{defn}
It is fairly straightforward to see that a norm $\|\cdot\|$ on a vector space $V$ defines a metric $d$ on $V$ by $d(x,y) = \|x - y\|$.


Some of the most important instances of vector spaces, namely the Hilbert spaces, are those with an \emph{inner product}, which corresponds to the familiar dot product on finite dimensional vector spaces.
\begin{defn}[Inner Product]
An inner product on a complex vector space is a function $\langle \cdot ,\cdot \rangle : V \times V \rightarrow \mathbb{C}$ satisfying for all $u,v,w \in V$ and $\alpha \in F$:
\begin{center}
\begin{tabular}{lc}
Additivity: &
$\inprod{u}{v+w}  =  \inprod{u}{v} + \inprod{u}{w}$\\
\vspace{0.1cm}
&$\inprod{u+v}{w}  =  \inprod{u}{w} + \inprod{v}{w}$\\
\vspace{0.1cm}
Nonnegativity: &
$\inprod{v}{v} \ge 0$\\
\vspace{0.1cm}
Nondegeneracy: &
$\inprod{v}{v} = 0\quad \textrm{iff}\quad  v = 0$\\
\vspace{0.1cm}
Conjugate symmetry: &
$\inprod{u}{v} = \overline{\inprod{v}{u}}$\\
\vspace{0.1cm}
Sesquilinearity: &
$ \inprod{u}{\alpha v} = \alpha\inprod{u}{v}$
%& $ \inprod{u}{v + w} = \inprod{u}{v} + \inprod{u}{w}$
\end{tabular}
\end{center}
where $\overline{\alpha}$ denotes the complex conjugate of $\alpha$. The definition clearly also holds when $V$ is a real vector space. A vector space with an inner product defined is called an \emph{inner product space}.

Note that conjugate symmetry implies that $\inprod{x}{x}$ is real for all $x$, and that conjugate symmetry and sesquilinearity together imply that
$$\inprod{\alpha x}{y} = \overline{\alpha}\inprod{x}{y}.$$
An inner product naturally defines a norm $\|\cdot\|$ on a vector space, by $\|x\| = \sqrt{\inprod{x}{x}}$.
\end{defn}

\begin{example}
The inner product or dot product on $\R^n$ is defined by
$$\inprod{x}{y}  = \sum_{i = 1\ldots n} x_iy_i.$$
The complex valued functions on a real interval $[a,b]$ is an inner product space (it is a vector space: for example addition is defined by $(f + g)(x) = f(x) + g(x)$), with the inner product
$$\inprod{f}{g} = \int_a^b \overline{f(x)}g(x) \mathrm{d}x$$
The norm of a vector in $\R^n$ corresponds to its length: $\|x\| = \sqrt{\sum_{i=1\ldots n} x_i^2}$.
\end{example}

\subsection{Bases}

Almost every vector space considered in computational linguistics comes with some basis, which can usually be conceptually linked to the notion of context. The notion of a \emph{basis} in a vector space is also very important in relation to \emph{vector lattices} (see section \ref{vector-lattices}).

\begin{defn}[Basis]
A basis is a set $B$ of elements of a vector space $V$ over a field $F$, such that the elements are \emph{independant}, i.e., if
$$\sum_{b_i \in B} \alpha_i b_i = 0$$
for some set of $\alpha_i \in F$, then necessarily $\alpha_i = 0$ for all $i$; and $B$ \emph{spans} $V$, i.e., for each element $x \in V$,
$$x = \sum_{b_i \in B} \beta_i b_i$$
for some set of values $\beta_i \in F$.

Two elements $x,y$ in an inner product space are called \emph{orthogonal} if $\inprod{x}{y} = 0$. An orthonormal basis
\end{defn}

\begin{example}
An orthonormal basis for the vector space $\R^n$ is given by the set $\{e_1,e_2,\ldots e_n\}$ where $e_1 = (1,0,0,\dots 0)$, $e_2 = (0,1,0,\dots 0)$, \ldots, $e_n = (0,0,0,\dots 1)$.
\end{example}

\subsection{Completeness}

Completeness is a property of vector spaces which is difficult to grasp conceptually, and is not that important to understand in relation to applications in computational linguistics. However, it is a property that is possessed by a lot of interesting vector spaces, and is often required of vector spaces since it leads to things being mathematically very ``well behaved''.

\begin{defn}[Limit]
Let $a_1,a_2\ldots$ be an infinite sequence of real numbers. A real number $a$ is said to be the limit of the sequence if and only if for every real number $\epsilon > 0$, there is a natural number $n_0$ such that for all $n > n_0$, $|a_n - a| < \epsilon$.
\end{defn}

\begin{defn}[Completeness]
Given a metric space $X$ with metric function $d$, a sequence $x_1, x_2, \ldots$ is called \emph{Cauchy} if for every positive real number $a$, there is an integer $n_0$ such that for all integers $m,n > n_0$, $d(x_m,x_n) < a$. If every Cauchy sequence has a limit in $X$, the metric space is called \emph{complete}.

A \emph{Banach space} is a normed vector space which is complete with respect to the metric $d$ defined by $d(x,y) = \|x - y\|$. A \emph{Hilbert space} is a vector space with an inner product which is complete with respect to the metric defined by the inner product norm, $d(x,y) = \sqrt{\inprod{x-y}{x-y}}$. A Hilbert space is thus a special kind of Banach space.
\end{defn}


\subsection{New vector spaces from old}
\begin{defn}[Direct Sum]
Given two vector spaces $U$ and $V$ we can construct a vector space $U \oplus V$ called the \emph{direct sum} of $U$ and $V$. The direct sum is simply the cartesian product $U \times V$ with vector operations defined component-wise:
\begin{eqnarray*}
(u_1,v_1) + (u_2,v_2) & = & (u_1+u_2,v_1+v_2)\\
\alpha(u,v) & = & (\alpha u, \alpha v)
\end{eqnarray*}
where $u_i \in U, v_i \in V, \alpha \in F$. If $U$ and $V$ are Hilbert spaces, then $U \oplus V$ denotes the Hilbert space with the inner product defined by
$$\inprod{(u_1,v_1)}{(u_2,v_2)} = \inprod{u_1}{u_2} + \inprod{v_1}{v_2}$$
The dimension of $U\oplus V$ is equal to the sum of the dimensions of $U$ and $V$.
\end{defn}
\begin{defn}[Tensor Product]

The \emph{tensor product} $U \otimes V$ of two vector spaces $U$ and $V$ is constructed by taking the vector space generated by the cartesian product $U \times V$ and factoring out the subspace generated by the equations:
\begin{eqnarray*}
(u_1 + u_2) \otimes v & = & u_1 \otimes v + u_2 \otimes v\\
u \otimes (v_1 + v_2) & = & u \otimes v_1 + u \otimes v_2\\
\alpha u\otimes v & =  &u \otimes \alpha v = \alpha(u \otimes v) 
\end{eqnarray*}
where $u_i,u \in U$, $v_i,v \in V$ and $\alpha \in F$.

If $U$ and $V$ are Hilbert spaces, the tensor product is again a Hilbert space, with inner product defined by
$$\inprod{u_1\otimes v_1}{u_2 \otimes v_2} = \inprod{u_1}{u_2}\inprod{v_1}{v_2}.$$
The dimension of $U \otimes V$ is equal to the product of the dimensions of $U$ and $V$.
\end{defn}
\begin{defn}[Fock Space]
\index{Fock space|textbf}
Let $H$ be a finite dimensional complex Hilbert space and $\Omega$ a distinguished vector in $H$ with norm 1. The Fock space $\mathcal{F}$ of $H$ is then defined as
$$\mathcal{F} = \mathbb{C}\Omega \oplus H \oplus (H \otimes H) \oplus (H \otimes H \otimes H) \oplus \cdots$$
i.e.~it is the direct sum of all finite tensor product powers of $H$, where $\mathbb{C}\Omega$ is a one dimensional Hilbert space which is viewed as the zeroth power of $H$.
\end{defn}

\subsection{Linear Operators}
\label{operators}

The linear operators on a vector space have proven to be of extreme importance in many applications. For example in imaging, scaling and rotation are represented as linear operators, while in quantum mechanics, an observable may be represented as a linear operator on a vector space with certain characteristics. Linear operators are defined as follows:
\begin{defn}[Linear Operator]
A linear operator from a vector space $U$ to a vector space $V$ both over a field $F$ is a function $A$ from $U$ to $V$ satisfying
$$A(\alpha x + \beta y) = \alpha Ax + \beta Ay$$
for all $x,y \in U$ and $\alpha, \beta \in F$.
\end{defn}
Note that the operation of $A$ on an element $x$ is denoted simply $Ax$ (without brackets). In addition, we shall often refer to a linear operator simply as an ``operator''---in this case linearity is assumed.

Linear operators themselves form a vector space, with vector space operations defined by
\begin{eqnarray*}
(A + B)x&=&Ax + Bx\\
(\alpha A)x&=&\alpha Ax\\
0x&=&0
\end{eqnarray*}

\section{Lattice Theory}
\label{lattices}

The concepts described in this section deal with relationships between objects. One of the most important types of relationship that we consider on sets of objects is that of a \emph{partial ordering}. An example of this is the hypernymy relation between words (or equivalently the \textbf{is-a} or subsumption relation between concepts), discussed in section \ref{taxonomy}. Another example is the subset relation on a set of sets.

These relations often satisfy much stronger conditions, which we classify in sequence: semilattices, lattices, modular lattices, distributive lattices and Boolean algebras. All of these have important characteristics which may also be expressed in algebraic terms.

%\begin{example}
%A \emph{directed graph} is a set $V$ of ``vertices'' or ``nodes'' together with a relation $E$ on $V$. The set $E$ describes ``edges'' between nodes. For example, the graph $V = \{a,b,c\}$ and $E$
%\end{example}

\begin{defn}[Partial Ordering]
\index{partial ordering|textbf}
A partial ordering on a set $S$ is a relation $\le$ that satisfies, for all $x,y,z \in S$:
%\begin{itemizes}
%\items \emph{reflexive}: $x \le x$,
%\items \emph{antisymmetric}: if $x \le y$ and $y \le x$ then $x = y$, and
%\items \emph{transitive}: if $x \le y$ and $y \le z$ then $x \le z$.
%\end{itemizes}
\begin{center}
\begin{tabular}{ll}
$x \le x$ & (reflexivity)\\
if $x \le y$ and $y \le x$ then $x = y$ & (antisymmetry) \\
if $x \le y$ and $y \le z$ then $x \le z$ & (transitivity)
\end{tabular}
\end{center}
\end{defn}
If $a \le b$ then we say $a$ is \emph{contained in} or \emph{is less than} $b$. An example of a partial ordering is the set inclusion relation, $\subseteq$ on a set of subsets of a set, or the `less than or equal' relation on the natural numbers.

The following definition is useful for describing properties of partial orderings, and drawing diagrams of them:
\begin{defn}[Preceding elements]
    Write $x < y$ if $x \le y$ and $x \neq y$ in $L$. We say that $x$ 
    \emph{precedes} $y$ and write $x \prec y$ if $x < y$ and there is no 
    element $z$ such that $x < z < y$.
\end{defn}

Partial orderings are often depicted using \emph{Hasse diagrams}. Some examples are shown in figure \ref{hasse}. Elements of the lattice are shown as nodes, while the relation $\prec$ between elements is shown by connecting nodes with an edge, such that the lesser element is below the greater element in the diagram. For example, figure \ref{hasse:notlattice} shows a four element set with a partial ordering which may be described by the relation $\le$ on the set $\{a,b,c,d\}$ defined by $a \le c$, $b \le c$, $a \le d$, $b\le d$. Hasse diagrams such as these are used to show partial orderings \emph{up to isomorphism}, that is, when we are not interested in the labeling of the nodes, only the nature of the partial order itself. 
\begin{figure}
\begin{center}

%\input{pentagon.tex}
\subfigure[A partial ordering that is not a lattice]{
	\label{hasse:notlattice}
	\begin{graph}(4,3.5)(-0.25,-3.5)
	\graphnodesize{0.15}
	\roundnode{x1}(1,-1)
	\roundnode{x2}(2.5,-1)
	\roundnode{x3}(1,-2.5)
	\roundnode{x4}(2.5,-2.5)
	\edge{x1}{x3}
	\edge{x1}{x4}
	\edge{x2}{x3}
	\edge{x2}{x4}
	\end{graph}
}
\hfill
\subfigure[An embedding of the partial ordering in a lattice]{
	\label{hasse:lattice}
	\begin{graph}(4,3.5)(-0.25,-3.5)
	\graphnodesize{0.15}
	\roundnode{x1}(1,-1)
	\roundnode{x2}(2.5,-1)
	\roundnode{x3}(1,-2.5)
	\roundnode{x4}(2.5,-2.5)
	\roundnode{x5}(1.75,-1.75)
	\edge{x1}{x5}
	\edge{x2}{x5}
	\edge{x5}{x3}
	\edge{x5}{x4}
	\end{graph}
}
\hfill
\subfigure[The five element non-modular lattice]{
	\label{hasse:notmodular}
	\begin{graph}(4,3.5)(0,-3.25)
	\graphnodesize{0.15}
	%Nodes:
	\roundnode{x1}(2,-0.25)
	\roundnode{x2}(1,-1)
	\roundnode{x3}(1,-2)
	\roundnode{x4}(3,-1.5)
	\roundnode{x5}(2,-2.75)
	%Edges:
	\edge{x1}{x2} \edge{x2}{x3} \edge{x3}{x5}
	\edge{x1}{x4} \edge{x4}{x5}
	\end{graph}}
	%\caption{Hasse diagrams.}}
	%\label{hasse}}
\hfill
\caption{Hasse diagrams}
\label{hasse}
\end{center}
\end{figure}

\begin{defn}[Semilattice and Lattice]
An \emph{upper bound} of a subset $T$ of a partially ordered set $S$ is an element $s$ such that $t \le s$ for all $t \in T$. The \emph{least upper bound} of $T$ if it exists (also called \emph{supremum} or \emph{join}) is the upper bound which contains every upper bound. The join of a set $T$ is denoted $\bigvee T$, or if $T$ consists of two elements $x$ and $y$ their join is denoted $x \lor y$.

Similarly a \emph{lower bound} of $T$ is an element $s'$ such that $s' \le t$ for all $t \in T$. The \emph{greatest lower bound} if it exists (also called the \emph{infimum} or \emph{meet} of $T$) is the lower bound which is contained in every other lower bound. The meet of $T$ is denoted $\bigwedge T$; the meet of two elements $x$ and $y$ is denoted $x\land y$.

\index{lattice|textbf}
A \emph{meet semilattice} (or simply \emph{semilattice}) is a partially ordered set in which every pair of elements has a greatest lower bound. Similarly, a \emph{join semilattice} is a partially ordered set in which every pair of elements has a greatest lower bound.

A \emph{lattice} is a partially ordered set in which any two elements have a least upper bound and a greatest lower bound; a lattice is thus both a join and a meet semilattice. A lattice is called \emph{complete} if every subset of $S$ has a least upper bound and greatest lower bound; all finite lattices are complete.
\end{defn}

Figure \ref{hasse:notlattice} shows a partial ordering that is \emph{not} a lattice: the join of the two lesser elements is not well defined, similarly, the meet of the two greater elements is not defined. Figure \ref{hasse:lattice} does show a lattice: the new element acts as the missing join and meet.

\begin{remark}
A semilattice can be characterised as a semigroup $S$ with the binary operation $\land$ satisfying \emph{idempotence} and \emph{commutativity}:
\begin{eqnarray*}
x \land x & = & x\\
x \land y & = & y \land x
\end{eqnarray*}
respectively. The partial ordering can be recovered by defining $x \le y$ iff $x\land y = x$. Similarly, a lattice can be characterised as a set $S$ together with two operations $\land$ and $\lor$ such that $(S,\land)$ and $(S,\lor)$ are semilattices (according to the above characterisation), satisfying the \emph{absorption} laws:
\begin{eqnarray*}
x \lor (x \land y) & = & x\\
x \land (x \lor y) & = & x
\end{eqnarray*}
\end{remark}

\begin{defn}[Modularity]
\index{lattice!modular|textbf}
A modular lattice is a lattice $L$ satisfying the \emph{modular identity}: if $x \le z$ then
$$x \lor (y \land z) = (x \lor y) \land z,$$
for all $x,y,z \in L$.
\end{defn}

Figure \ref{hasse:lattice} shows a five element modular lattice, while \ref{hasse:notmodular} shows a lattice that is not modular; it is the only five element non-modular lattice (up to isomorphism).

The proof of the following proposition is in \cite{Birkhoff:48}:
\begin{prop}
The modular lattices are those which do not have the five element non-modular lattice of figure \ref{hasse:notmodular} as a sub-lattice.
\end{prop}

\begin{defn}[Distributivity, Complement and Boolean Algebra]
\index{lattice!distributive|textbf}
A lattice is called \emph{distributive} if it satisfies
\begin{eqnarray*}
x\lor(y\land z) & = & (x\lor y)\land(x\lor z)\\
x\land(y\lor z) & = & (x\land y)\lor(x\land z)
\end{eqnarray*}
A lattice is \emph{complemented} if for every element $a$ there is an element $a'$ such that $a \lor a' = 1$ and $a \land a' = 0$. A complemented distributive lattice is called a \emph{Boolean algebra}.
\end{defn}

\subsection{Functions between partial orders}

It is very important for our work to characterise the nature of functions between partial orderings. Of special importance are those that preserve the partial ordering, and in the case of lattices, preserve meets and joins. We define some important types of functions, and give examples.

\begin{defn}[Order Embeddings]
A function $f$ from one partially ordered set $S$ to another $T$ is called \emph{monotone} or \emph{order-preserving} if $a \le b$ in $S$ implies $f(a) \le f(b)$ in $T$. Conversely, if $f(a) \le f(b)$ implies $a \le b$ then $f$ is called \emph{order-reflecting}. An \emph{order embedding} is a function that is both order-preserving and order-reflecting. A \emph{completion} of a partially ordered set $S$ is an order embedding of $S$ into a complete lattice.
\end{defn}

\begin{defn}[Lattice Homomorphisms]
If $S$ and $T$ are semilattices, a function $f$ from $S$ to $T$ is a \emph{semilattice homomorphism} if $f(a \land b) = f(a) \land f(b)$ (where $\land$ can represent the meet or the join operation). If $S$ and $T$ are lattices, a \emph{lattice homomorphism} is a function that is both a meet semilattice and join semilattice homomorphism, i.e.~$f(a \land b) = f(a) \land f(b)$, and $f(a \lor b) = f(a) \lor f(b)$. A \emph{lattice isomorphism} is a bijective lattice homomorphism, i.e.~for each element $b$ in $T$ there is exactly one element $a$ in $S$ such that $f(a) = b$. If a lattice isomorphism exists between two lattices they are said to be \emph{isomorphic}.
\end{defn}

Often we may be dealing with partial orders but require something with more structure than that relation provides. For example, we may like to be able to define meets and joins to make the partial ordering into a lattice. The concepts of \emph{principal ideals} and their duals, \emph{principal filters}, allow us to do this:

\begin{defn}[Ideals and Filters]
A \emph{lower set} in a partially ordered set $S$ is a set $T$ such that for all $x,y \in T$, if $y \le x$ then $y \in T$. Similarly, an upper set in $S$ is a set $T'$ such that for all $x,y \in T'$, if $y \ge x$ then $y \in T$.

The \emph{principal ideal generated by an element $x$} in a partially ordered set $S$ is defined to be the lower set $\down{x} = \{y \in S : y \le x\}$. Similarly, the \emph{principal filter generated by $x$} is the upper set $\up{x} = \{y \in S : y \ge x\}$.
\end{defn}

\begin{prop}[Ideal Completion]
If $S$ is a partially ordered set, then $\down{\cdot}$ can be considered as a function from $S$ to the powerset $2^S$. Under the partial ordering defined by set inclusion, the set of lower sets form a complete lattice, and $\down{\cdot}$ is a completion of $S$, the \emph{ideal completion}. Similarly, the function $\up{\cdot}$ is the \emph{filter completion} of $S$: it is an embedding into the complete lattice of upper sets, again ordered by inclusion.
\end{prop}

%Dedekind completion?

\section{Riesz Spaces and Positive Operators}
 \label{vector-lattices}
 
The previous sections have described formalisms commonly used to describe meaning: broadly speaking, that of vector spaces and that of lattices. Until now, little attention within computational linguistics has been paid to how to combine these two areas. There is a large body of research within mathematical analysis into an area which merges the two formalisms: the study of \emph{partially ordered vector spaces}, \emph{vector lattices} (or \emph{Riesz spaces}), and \emph{Banach lattices}, and a special class of operators on these spaces called \emph{positive operators}.

It is our belief that this area provides exciting new opportunities for combining the new, vector based representations, with old, ontological representations of meaning, and also provides a means of defining entailment between vector based representations.

The definitions and propositions of this section can be found in \cite{Abramovich:02} and \cite{Aliprantis:85}.

\begin{defn}[Partially ordered vector space]
A partially ordered vector space $V$ is a real vector space together with a partial ordering $\le$ such that:
\vspace{0.1cm}\\
\indent if $x \le y$ then $x + z \le y + z$\\
\indent if $x \le y$ then $\alpha x \le \alpha y$
\vspace{0.1cm}\\
for all $x,y,z \in V$. Such a partial ordering is called a \emph{vector space order} on $V$. If $\le$ defines a lattice on $V$ then the space is called a \emph{vector lattice} or \emph{Riesz space}.

A vector $x$ in $V$ is called \emph{positive} if $x \ge 0$. The \emph{positive cone} of a partially ordered vector space $V$ is the set $V^+ = \{x \in V : x \ge 0\}$
\end{defn}

The positive cone has the following properties:
\begin{eqnarray*}
X^+ + X^+ \subseteq X^+\\
\alpha X^+ \subseteq X^+\\
X^+ \cap (-X^+) = \{0\}
\end{eqnarray*}
Any subset $C$ of $V$ satisfying the above three properties is called a \emph{cone} of $V$.

\begin{prop}
If $C$ is a cone in a real vector space $V$, then the relation $\le$ defined by $x \le y$ iff $y - x \in C$ is a vector space order on $V$, with $X^+ = C$.
\end{prop}

Part of our thesis is that meanings can be represented as positive elements of a vector space. If this is the case, then we can view words as operating on these meanings. Operators which map positive elements to positive elements are called \emph{positive}; there is a large body of work studying such operators.

This idea leads to some useful definitions of particular positive elements of a vector lattice corresponding to an arbitrary element $x$. The \emph{positive part} of $x$ is denoted $x^+$ and is defined by $x^+ = x \lor 0$. Similarly the \emph{negative part} is $x^- = (-x)\lor 0$, and the \emph{absolute value} is $|x| = x \lor -x$. There are a number of useful identities concerning these definitions:
\begin{prop}
The following identities hold for elements $x,y$ in a vector lattice:
\begin{enumerate}[\indent(a).]
\item $x = x^+ - x^-$
\item $|x| = x^+ + x^-$
\item $x\land y = \frac{1}{2}(x + y - |x - y|)$
\item $x\lor y =  \frac{1}{2}(x + y + |x - y|)$
\end{enumerate}
\end{prop}

\begin{defn}[Positive Operators]
An operator $A$ on a vector space $V$ is called \emph{positive} if $x \ge 0$ implies $Ax \ge 0$. It is called \emph{regular} if it can be denoted as the difference between two positive operators.
\end{defn}


Surprisingly, the set of regular operators on a vector lattice themselves form a vector lattice:
\begin{prop}[Riesz-Kantarovi\v{c}]
The positive cone defines a vector space order on the vector space of operators on $V$. This order makes the space of regular operators a vector lattice. Specifically the meet and join of two regular operators $A$ and $B$ are given by
\begin{eqnarray*}
(A \land B)x & = & \inf\{Ay + Bz : y,z \in V^+ \text{and } y + z = x\}\\
(A \lor B)x & = & \sup\{Ay + Bz : y,z \in V^+ \text{and } y + z = x\}.
\end{eqnarray*}
\end{prop}

\begin{defn}[Lattice Homomorphism]
A positive operator $A$ between two vector lattices is called \emph{lattice homomorphism} if $A(x \lor y) = Ax \lor Ay$. An lattice homomorphism that is a one-to-one function is called a \emph{lattice isomorphism}.
\end{defn}

The following proposition shows the importance of lattice homomorphisms:
\begin{prop}
For a positive operator $A$ between two Riesz spaces $U$ and $V$, the following statements are equivalent:
\begin{enumerate}[\indent(a).]
\item $A$ is a lattice homomorphism.
\item $A(x^+) = (Ax)^+$ for each $x \in U$.
\item $A(x \land y) = Ax \land Ay$ for all $x,y \in U$.
\item $|Ax| = A|x|$ for each $x \in U$.
\item $x\land y = 0$ in $U$ implies $Ax \land Ay = 0$ in $V$. 
\end{enumerate}
\end{prop}


% \bibliographystyle{plainnat}
% \bibliography{contexts}
 
% \end{document}