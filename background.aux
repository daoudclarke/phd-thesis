\relax 
\citation{Firth:57}
\citation{Wittgenstein:53}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{background}{{2}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Philosophy}{13}}
\newlabel{philosophy}{{2.1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Wittgenstein}{13}}
\citation{Honeybone:05}
\citation{Firth:57}
\citation{Firth:57a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Firth}{14}}
\citation{Harris:68}
\citation{Harris:85}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Harris}{15}}
\citation{Weeds:04}
\citation{Geffet:05}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Later Developments}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Discussion}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Vector Based Representations of Meaning}{18}}
\newlabel{vector-based}{{2.2}{18}}
\citation{Deerwester:90}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Latent Semantic Analysis}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Occurrences and some context of occurrences of the word \emph  {fruit} in the British National Corpus.}}{20}}
\newlabel{fruit}{{2.1}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Matrix decomposition and dimensionality reduction in latent semantic analysis.}}{21}}
\newlabel{reduce}{{2.2}{21}}
\citation{Hofmann:99}
\citation{Hofmann:99}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces A table of hypothetical occurrences of words in a set of documents, $d_1$ to $d_8$.}}{22}}
\newlabel{fruittable}{{2.1}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The matrices $U'$, $D'$ and $V'$ formed from singular value decomposition and dimensionality reduction. The product approximates the original matrix in table 2.1\hbox {}. Here $A^\mathrm  {T}$ is used to mean the transpose of matrix $A$.}}{22}}
\newlabel{decompose}{{2.3}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Probabilistic Latent Semantic Analysis}{22}}
\citation{Hofmann:99}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces An approximation to the table obtained from a singular value decomposition followed by a dimensionality reduction to two dimensions.}}{23}}
\newlabel{approx}{{2.2}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The probabilistic latent semantic analysis model of words $w$ and documents $d$ modelled as dependent on a latent variable $z$.}}{23}}
\newlabel{plsa}{{2.4}{23}}
\citation{Hofmann:99}
\citation{Hofmann:99}
\citation{Blei:03}
\citation{Blei:03}
\citation{Blei:03}
\citation{Blei:03}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Most probable words given two topic variables relating to the term ``power'' (taken from \cite  {Hofmann:99}).}}{24}}
\newlabel{power-table}{{2.3}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Latent Dirichlet Allocation}{24}}
\newlabel{lda-section}{{2.2.3}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Graphical representation of the Dirichlet model, adapted from \cite  {Blei:03}. The inner box shows the choices that are repeated for each word in the document; the outer box the choice that is made for each document; the parameters outside the boxes are constant for the model.}}{25}}
\newlabel{graphical}{{2.5}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Generative process assumed in the Dirichlet model}}{25}}
\newlabel{generative}{{2.6}{25}}
\citation{Blei:03}
\citation{Blei:03}
\citation{Cai:07}
\citation{Boyd-Graber:07}
\citation{Lin:98a}
\citation{Lee:99}
\citation{Curran:02}
\citation{Kilgarriff:03}
\citation{Weeds:04}
\citation{Hindle:90}
\citation{Pereira:93}
\citation{Dagan:94}
\citation{Grefenstette:94}
\citation{Lin:98a}
\citation{Curran:02}
\citation{Dagan:97}
\citation{McCarthy:04}
\citation{Lin:01}
\citation{McCarthy:03}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Measures of Distributional Similarity}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Eight measures of similarity and distance: geometric measures between vectors $u$ and $v$, where $u_i$ indicates the components of vector $u$, $u\cdot v$ indicates the dot product and $\delimiter "026B30D u\delimiter "026B30D $ denotes the Euclidean norm of $u$; measures based on the Kullback-Leibler divergence, where $p$ and $q$ are estimates of probability distributions describing the occurrences of words in contexts $c$; and measures based on the features of a word, either defined with respect to probability of occurrence, $F(w) = \{c : P(c|w) > 0\}$ or with respect to mutual information (this is also called the support of $w$), $S(w) = \{c : I(c,w) > 0\}$, where the mutual information $I$ is given by $I(c,w) = \qopname  \relax o{log}(P(c|w)/P(c))$.}}{27}}
\newlabel{simmeasures}{{2.4}{27}}
\citation{Lin:98a}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Discussion}{28}}
\@setckpt{background}{
\setcounter{page}{30}
\setcounter{equation}{0}
\setcounter{enumi}{2}
\setcounter{enumii}{2}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{1}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{4}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{ALC@line}{0}
\setcounter{ALC@rem}{0}
\setcounter{ALC@depth}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{FancyVerbLine}{29}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{prop}{0}
\setcounter{assumption}{0}
}
