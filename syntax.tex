%%Bismillahi-r-Rahman-r-rahim
%%Wa-s-salatu wa-s-salamu alai sayyidina Muhammad
%%Wa ala alihi wa sahbihi ajma'in
%
%\documentclass{kluwer}    % Specifies the document style.
%
%%\usepackage{amssymb}
%%\usepackage[leqno]{amsmath}
%%\usepackage{amsthm}
%%\usepackage{eufrak}
%\usepackage{graphs}
%
%\newcommand{\bra}[1]{\langle #1|}
%\newcommand{\ket}[1]{|#1 \rangle}
%\newcommand{\bracket}[2]{\langle #1| #2\rangle}
%
%\newcommand{\inprod}[2]{\langle #1, #2 \rangle}
%
%\newcommand{\Tr}[0]{\mathrm{Tr}}
%
%\newcommand{\newcite}[1]{\citeauthor{#1} \shortcite{#1}}
%
%\newtheorem{defn}{Definition}
%\newtheorem{prop}{Proposition}

%\documentclass[12pt]{report}

%\input{head}

%
%\begin{document}                

\chapter{Context Theories and Syntax} 
\label{syntax-chapter}

In this chapter we look at ways of describing syntactic properties of language in terms of vector space operators and algebra. This will allow us to incorporate such properties into context theories for natural language. The ability to view syntax from a context-theoretic perspective has many potential benefits, for example, we describe a method to represent syntax in terms of matrices that may lead to fast computational methods for statistical parsing, and at the end of the chapter we describe some ideas for how separate context theories for syntax and semantics may be combined using a generalisation of the notion of independence to create a new form of natural language semantics in which both the semantic and syntactic aspects of a word may be represented as a single element of an algebra.

%\section{Introduction}

The context-theoretic framework places specific requirements on the nature of an implementation; these mean that certain grammar formalisms are more suited to the context-theoretic approach. We can identify several properties of the framework that are relevant:
\begin{itemize}
\item the framework requires that all information about the properties of a word are incorporated into its vector representation. This leads us to lexicalised formalisms for syntax in which syntactic properties of a word can be encapsulated independently of any external grammar. This makes a generative grammar less attractive for example, since the properties of a word are spread throughout the rules of the grammar, whereas categorial grammar can encapsulate the syntactic properties of a word purely by specifying its category.
\item the algebra of a context theory must be associative: $(ab)c = a(bc)$ for all $a,b$ and $c$ in the algebra, thus the grammatical formalism should be compatible with this idea.
\end{itemize}
Consideration of these properties has led us to two syntactic formalisms that are particularly suited to the context-theoretic approach, namely categorial grammars and link grammar. It is likely that other syntactic formalisms can also be described in terms of context theories, however we have concentrated on these two since they have the above properties and thus promise to be closest to the context-theoretic approach.

The contributions of this chapter are as folows:
\begin{itemize}
\item In Section \ref{syntax-background-section} we summarise various forms of categorial grammar including those that are algebraic in nature. We describe Bar-Hillel's and Lambek's formulations, bilinear logic and pregroups; in Section \ref{categorial-context} we discuss the relationship between categorial grammar formalisms and the context-theoretic framework, showing how the Lambek calculus can be incorporated into a context theory, and explaining why the other formalisms are not so suited to the framework.
\item The context-theoretic description of the Lambek calculus is difficult to handle: it is not obvious how to compute with the resulting representation. We have found that link grammar (introduced in Section \ref{link-grammar-section}) can be described as context theories in ways that do not have this limitation:
\begin{itemize}
\item We give a description of link grammar in terms of operators on an infinite dimensional vector space called Fock space in Section \ref{operator-formulation-section}. This gives us a new description of a simple form of stochastic link grammar (Section \ref{stochastic-link-grammar-section}) and enables us to describe link grammars in terms of matrices (Section \ref{lg-matrix-section}).
\item We give a context theory for link grammar in terms of semigroups in Sections \ref{alg-lg-section} to \ref{semigroup-context}. This brings to light a useful relationship between link grammar and inverse semigroups, allowing us to describe a link grammar parse as the Munn tree of a free inverse semigroup; this may ultimately have the potential to incorporate semantic information into the representation. This section also demonstrates the usefulness of using semigroups to construct context theories: a context theory can be tailored by building the required properties into a semigroup.
\end{itemize}
\item We discuss potential directions for future research in Section \ref{discussion-syntax-section}.
\end{itemize}
It is our hope that the contributions of this chapter will lead to new ways of combining vector representations of words to form representations of larger constituents in a way that incorporates syntactic structure, allowing complex vector-based representations of meaning to be built up from smaller ones. Such approaches could form a useful alternative to logic-based representations of meaning.

\section{Categorial Grammars}
\label{syntax-background-section}
\index{categorial grammar|(}

%This chapter places special emphasis on one syntactic formalism; namely that of \emph{link grammars}\index{link grammar}. In order to see why these grammars are particularly suited to the  context-theoretic approach, it is useful to consider other grammars however. The methods for describing syntax in natural language are numerous; we concentrate here on those that we have found to be closest to the algebraic approach, namely variations on categorial grammar\index{categorial grammar|(} \citep{Bar-Hillel:50, Lambek:58} and later variations on this formalism. We discuss the problems involved in expressing these within the context-theoretic framework, then discuss link grammars, and show two alternative ways of describing these algebraically within the framework.

\subsection{Bar-Hillel Categorial Grammar}
\index{Bar-Hillel categorial grammar}

The simplest form of categorial grammar is due to \citeauthor{Bar-Hillel:50} (\citeyear{Bar-Hillel:50};\citeyear{Bar-Hillel:64} 1964) (based on earlier work of Ajdukiewicz) and is described as a deductive system with the following rewrite rules:
\begin{eqnarray*}
(A/B)\ B &\rightarrow& A\\
B\ (B\backslash A) &\rightarrow& A
\end{eqnarray*}
In a categorial grammar, words in a language are assigned one or more \emph{categories}, built up out of a number of \emph{basic types} and the operations $/$ and $\backslash$. For example, a transitive verb might be assigned the category $(\mathit{NP}\backslash S)/\mathit{NP}$, where $\mathit{NP}$ and $S$ are basic types representing the categories of noun phrases and sentences respectively. The category $(\mathit{NP}\backslash S)/\mathit{NP}$ can be thought of as describing those strings which form a sentence when they are both preceded and followed by a noun phrase.

\subsection{Lambek Calculus}
\index{Lambek calculus|(}
%\subsection{Lambek Calculus}

Based on Bar-Hillel's categorial grammar, \cite{Lambek:58}\index{Lambek,  Joachim} developed a calculus specifically for describing natural language. In its original form, it is defined as a deductive system, whose axioms\footnote{See also \cite{Wood:93}.} are:
\begin{eqnarray*}
A & \rightarrow & A\\
(AB)C & \leftrightarrow  & A(BC),
\end{eqnarray*}
where $A \leftrightarrow B$ is shorthand for $A\rightarrow B$ and $B\rightarrow A$, with the following rules of inference:
\begin{eqnarray*}
AB \rightarrow C & \mathrm{iff} & A\rightarrow C/B\\
AB \rightarrow C & \mathrm{iff} & B\rightarrow A\backslash C,
%\mathrm{if}\ x\rightarrow z/y& \mathrm{then} & xy \rightarrow z\\
%\mathrm{if}\ x\rightarrow z\backslash y & \mathrm{then} &  xy \rightarrow z
\end{eqnarray*}
and
$$\mathrm{if}\ A\rightarrow B\ \mathrm{and}\  B\rightarrow C\ \mathrm{then}\ A\rightarrow C$$
%In application to NLP, each word is assigned to one or more categories (elements in the calculus); the category of a phrase can then be determined from the deductive system, by taking the product of the categories associated with each word in the phrase.
Using these rules, it possible to deduce many theorems of the calculus, for example
$$\begin{aligned}
(A/B)\ B \rightarrow A& &\quad &\text{(Ajdukiewicz's law)}\\
A \rightarrow (B/A)\backslash B & & &\text{(Type raising)}\\
(A/B)(B/C) \rightarrow A/C & & &\text{(Composition)}
\end{aligned}$$
and their equivalents with $/$ exchanged with $\backslash$; many of these are useful in describing features of natural language.

One way of modeling the Lambek calculus is with free semigroups\index{free semigroup!and Lambek calculus} (also called \emph{L-models}) --- the completeness of the Lambek calculus with respect to such models is described in \cite{Pentus:95}. The calculus can be viewed as operations on subsets  of a monoid $M$, with
\begin{eqnarray*}
XY &=& \{xy : a \in X, b \in Y\} \\
X \backslash Y &=& \{m \in M : Xm \subseteq Y\} \\
Y / X &=& \{m \in M : mX \subseteq Y\}
\end{eqnarray*}
where $X,Y\subseteq M$ and we also use $m$ as a shorthand for $\{m\}$.

More generally, the operations $/$ and $\backslash$ can be defined for certain semigroups called \emph{residuated lattices} \citep{Birkhoff:48}. The connection between the Lambek calculus and residuated lattices was noted in Lambek's original paper \citep{Lambek:58}.

\begin{defn}[Partially Ordered Semigroup]\index{semigroup!partially ordered|textbf}
A semigroup $S$ together with a partial ordering $\le$ is called \emph{partially ordered} if $x \le y$ implies $xz \le yz$ for all $x,y,z \in S$.
\end{defn}

\begin{defn}[Lattice Ordered Semigroup]\index{semigroup!lattice ordered|textbf}
A lattice ordered semigroup is a partially ordered semigroup $S$ in which the partial ordering defines a lattice with operations $\lor$ and $\land$ such that
\begin{eqnarray*}
x\cdot (y\lor z) &=& x\cdot y\  \lor\ x\cdot z\\
(y\lor z)\cdot x &=& y\cdot x\ \lor\ z\cdot x
\end{eqnarray*}
\end{defn}

\begin{defn}[Residuated Lattice]\index{residuated lattice|textbf}
A lattice ordered semigroup $S$ is called a residuated lattice, if for each $x,y \in S$ there exists a greatest element $x/y$ such that
$$x/y \cdot y \le x$$
and a greatest element $x\backslash y$ such that
$$y \cdot y\backslash x \le x.$$
The elements $x/y$ and $y\backslash x$ are called the right and left \emph{residuals} or \emph{quotients}.
\end{defn}

As \cite{Birkhoff:48} notes, if $S$ has a zero which is also the least element of the lattice then the residuation operations $/$ and $\backslash$ can be defined by
\begin{eqnarray*}
x/y &=& \bigvee\{z : zy \le x\}\\
y\backslash x &=& \bigvee\{z : yz \le x\}
\end{eqnarray*}

The notion of residuated lattice is useful for our purposes because it allows us to think of categorial grammar in purely algebraic terms, allowing us to see how it relates to the context theoretic framework, and how it compares to other algebraic approaches.

\index{Lambek calculus|)}

\subsection{Bilinear Logic}
\index{bilinear logic}

\cite{Lambek:93}\index{Lambek,  Joachim} and \cite{Abrusci:91}\index{Abrusci, V.~M.}, based on earlier work of \cite{Girard:87}\index{Girard, J.~Y.}, developed a new version of Lambek's calculus called \emph{(classical) bilinear logic}. This adds two constants, 1 (introduced at an earlier stage by Lambek) and 0, to Lambek's original definition, which satisfy
\begin{gather*}
1A \leftrightarrow A \leftrightarrow A1\\
(0/A)\backslash 0 \leftrightarrow A \leftrightarrow 0/(A\backslash 0) 
\end{gather*}
As a shorthand notation, $A\backslash 0$ is written $A^r$ and $0/A$ is written $A^l$. It can be shown that
$$(B^rA^r)^l \leftrightarrow  (B^lA^l)^r$$
which is written as $(A\oplus B)$. Some theorems of bilinear logic \citep{Casadio:02} are
\begin{gather*}
1^r \leftrightarrow 0 \leftrightarrow 1^l\\
A\oplus 0 \leftrightarrow A \leftrightarrow 0\oplus A\\
(A\oplus B)\oplus C \leftrightarrow A\oplus(B\oplus C)\\
\begin{aligned}
A^lA \rightarrow 0& &\quad &AA^r \rightarrow 0\\
1 \rightarrow A\oplus A^l& & &1 \rightarrow A^r\oplus A\\
A/B  \leftrightarrow A\oplus B^l& & &B \backslash A \leftrightarrow B^r \oplus A\\
(A\oplus B) C  \rightarrow A\oplus BC& & &C(A\oplus B)  \rightarrow CA\oplus B
\end{aligned}
\end{gather*}
%\begin{align*}
%A^lA \rightarrow 0 && AA^r \rightarrow 0\\
%1 \rightarrow A\oplus A^l && 1 \rightarrow A^r\oplus A\\
%A/B  \leftrightarrow A\oplus B^l && B \backslash A \leftrightarrow B^r \oplus A\\
%(A\oplus B) C  \rightarrow A\oplus BC && C(A\oplus B)  \rightarrow CA\oplus B
%\end{align*}

\subsection{Pregroups}

Pregroups \citep{Lambek:01} arose as a simplification of bilinear logic called \emph{compact bilinear logic}, in which it is additionally assumed that $0 \leftrightarrow 1$ and $AB \leftrightarrow A\oplus B$. In this case there is a simpler description in terms of partially ordered monoids:

\begin{defn}[Pregroup]\index{pregroup|textbf}
Let $S$ be a partially ordered monoid. Then $S$ is called a pregroup if for each $x\in S$ there are elements $x^l$ and $x^r$ in $S$ such that
\begin{eqnarray*}
x^lx \le &1& \le xx^l\\
xx^r \le &1& \le x^rx
\end{eqnarray*}
\end{defn}

%We have recently come across the unpublished work of Yu Jiangsheng\footnote{\texttt{http://icl.pku.edu.cn/yujs/papers/pdf/fcg.pdf}} where syntax and semantics are combined in a formalism that has its basis in categorial grammars and feature structures.

%\subsection{Algebraic Parsing}

%In unpublished work, Mark Hopkins has reformulated standard results in language theory in algebraic as opposed to set-theoretic terms, with dioids (idempotent semirings) as a basis.\footnote{\texttt{http://www.uwm.edu/$\sim$whopkins/compalg/}} He shows that a grammar can be viewed as a system of inequalities on the dioid.

%Work in progress is an algebraic formulation of context free languages by use of ``context free expressions'' --- regular expressions over a monoid giving context free languages. We have been in communication about the best way to formulate and prove the theory.

\subsection{Categorial Grammar and Context Theories}
\label{categorial-context}
\index{categorial grammar!and context theories|(}


We would like to be able to describe the syntactic formalisms we have discussed within the context-theoretic framework; firstly to demonstrate the generality of the framework, and secondly, because we hope new techniques in parsing and semantic representation to arise by doing so. When it comes to categorial grammars, we seem to be well-placed since there are algebraic interpretations of many versions of the formalism. However, on closer inspection, making direct use of these formalisms within the context theoretic framework appears difficult.

For example, if we want to make use of a residuated lattice\index{residuated lattice} $S$, we could try and represent the structure within a lattice ordered algebra\index{lattice ordered algebra}. Like any semigroup, the vector space $L^1(S)$ can be considered as a lattice ordered algebra (see Section \ref{algebras}). However, the lattice ordering of $L^1(S)$ is not connected to the lattice ordering of $S$. If we wished to connect them, we may try to use one of the constructions described in the previous chapter to embed partial orderings within vector lattices. However, then it is not clear how we are to define multiplication on the vector lattice in a way that is consistent with multiplication in $S$.

We face similar problems with pregroups:\index{pregroup} it is not clear how we can incorporate the pregroup partial order into a vector lattice partial order whilst maintaining the multiplication defined in the pregroup.

Bilinear logic\index{bilinear logic} appears closer to being a vector space with an ``addition'' operation, $\oplus$, however, this operation is not defined to be commutative, something which is essential for a vector space. Requiring $\oplus$ to be commutative results in multiplication also being commutative, something not generally desirable for describing natural language syntax.

There is one way to represent categorial grammars within the framework however: we can make use of free semigroup\index{free semigroup} models to describe the Lambek calculus. Instead of using subsets of a free monoid $A^*$, however, we use elements of the algebra $L^\infty(A^*)$. A set $X \subset A^*$ is represented as the element $\tilde{X} \in L^\infty(A^*)$:
$$\tilde{X}(z) = \begin{cases}
1 & \text{if } z\in X\\
0 & \text{otherwise,}
\end{cases}$$
for $z \in A^*$. Multiplication in this algebra is defined by multiplication of the underlying free monoid, while vector space and lattice operations are defined since $L^\infty(A^*)$ is a vector lattice. We are thus able to represent the syntactic properties of a word by taking weighted sums of the representation of its syntactic categories, with weights corresponding to the probability that a word will take the respective category.

We can use this idea to make a context theory if we define a linear functional $\phi$ on $L^\infty(A^*)$ by
$$\phi(u) = \sum_{x \in A^*} p(x)u(x)$$
where $p$ is a probability distribution over elements of $A^*$. In this way, the context-theoretic probability of a category is the sum of the probabilities of all the strings in that category.


This representation raises computational issues similar to the ones that arose in dealing with logical semantics in Section \ref{practical-issues}; and a similar solution can be used. The problem again is that a word may be represented as a sum of categories whose vector representations are not disjoint in the vector lattice. The same method for computing a lower bound for the degree of entailment\index{entailment!degree of} between sentences can be used to estimate a degree of entailment between a desired parse and the syntactic representation of a sentence, or  to estimate a syntactic ``entailment'' between sentences.

Note that the algebra $L^1(A^*)$ is \emph{not} a residuated lattice\index{residuated lattice} under the vector lattice ordering, since it is not a lattice ordered semigroup under this ordering. The subsemigroup of elements of this algebra generated by the representation of categories does, however, form a lattice ordered semigroup under this ordering, and is also a residuated lattice, since it is isomorphic as a lattice ordered semigroup to the semigroup of subsets of the free monoid $A^*$. This means, that while we can represent categories within the algebra and take weighted sums of them, we cannot form new categories from these weighted sums --- something that is not a limitation for representing natural language syntax.

%Superficially, the categorial approach seems to be very well suited to our framework, since there are algebraic formulations of several types of the grammar, making use of semigroups and partial orderings. 
%These problems have led us to consider other syntax formalisms, including those not previously described in an algebraic manner. In the case of link grammar, we were able to find an algebraic description particularly well suited to the context theoretical approach.

\index{categorial grammar!and context theories|)}
\index{categorial grammar|)}

\section{Link Grammar}
\label{link-grammar-section}
\index{link grammar|(}

Link grammar \citep{Sleator:91}\index{Sleator and Temperly} is a lexicalised syntactic formalism which describes properties of words in terms of \emph{links} formed between them, and which is context-free in terms of its generative power. Apart from determining which sequences are grammatical, the links also encapsulate the nature of the relationships between words.

As an example, a transitive verb in English may link (simultaneously) to a subject on the left and an object on the right. This is represented in link grammar as the \emph{disjunct} $\ket{s}\bra{o}$ where $s$ and $o$ stand for `subject' and `object' respectively.\footnote{We are introducing our own, quantum mechanical, notation for link grammars from the beginning so as to be consistent, however we will describe the intended interpretation of this notation later.}

\begin{defn}[Link Grammar]
Let $L$ be a set of \emph{link types}. Then we define a set of \emph{left connectors} $D_l(L) = \{\ket{x} : x \in L\}$ and a set of \emph{right connectors} $D_r(L) = \{\bra{x} : x \in L\}$.

A disjunct is an element of $D_l(L)^*D_r(L)^*$. That is, a disjunct consists of a string of left connectors $\ket{x_1}\ket{x_2}\ldots\ket{x_n}$ followed by a string of right connectors $\bra{y_1}\bra{y_2}\ldots\bra{y_m}$.

The syntactic representation of a word is a set of disjuncts, each one corresponding to a different syntactic r\^ole played by the word. A sequence of words is in the language generated by the grammar if there is a corresponding sequence of disjuncts and a set of arcs, or \emph{links} drawn above the disjuncts such that:
\begin{itemize}
\item each disjunct in the sequence is a disjunct of the corresponding word in the sequence of words;
\item each left connector is connected to a right connector of the same type at any position to the right of it by drawing a link from one to the other;
\item each connector in each disjunct in the sequence is connected to exactly one other connector;
\item no links cross.
\end{itemize}
\end{defn}

Table \ref{link-table} shows a fragment of a link grammar. The grammar is clearly highly simplified, and is presented merely to explain the concept; for example in our fragment, \emph{way} and \emph{mud} can only occur as objects. Link grammars generally include a special symbol called the `wall' to indicate the beginning of the sequence \citep{Sleator:91}, which is then included in the grammar, but again we have omitted this for simplicity.

\begin{table}
\begin{center}
\caption{A small link grammar.}
\begin{tabular}{lp{7cm}}
\hline\noalign{\smallskip}
\emph{word} & \emph{disjuncts}\\
\noalign{\smallskip}
\hline\hline
\noalign{\smallskip}
%\lcline{1-1}\rcline{2-2}
they	& $\bra{s}$\\
mashed & $\ket{s}\bra{o}\qquad \ket{s}\bra{m}\bra{o}$ \\
way, mud & $\ket{d}\ket{o}\qquad \ket{d}\ket{j}\qquad \ket{a}\ket{d}\ket{o}\qquad \ket{a}\ket{d}\ket{j}$ \\
their, the & $\bra{d}$\\
through & $\ket{m}\bra{j}$\\
thick & $\bra{a}$\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
%\multicolumn{2}{p{7cm}}
Link types: &  $s$: subject, $o$: object, $m$: modifying phrases, \mbox{$a$: adjective}, $j$: preposition, $d$:~determiner.\\
\noalign{\smallskip}
\hline
\end{tabular}
\label{link-table}
\end{center}
\end{table}


A parse for a sentence is drawn as a set of links above the sentence, as in Figure \ref{parse} for the sentence `they mashed their way through the thick mud'. The disjuncts that are used in the parse are not generally drawn, but can be inferred from the links drawn above the sentence.


\begin{figure}[b]
\begin{center}
\begin{graph}(10,4)(-1,-1)
	\graphlinecolour{1}
\newcommand{\ntext}[4]{\roundnode{#1}(#2,#3)[\graphlinecolour{0}]  \nodetext{#1}(0,-.5){\raisebox{0pt}[1pt][1pt]{#4}}}
\newcommand{\btext}[3]{\bow{#1}{#2}{0.5}[\graphlinecolour{0}] \bowtext{#1}{#2}{0.5}{#3}}
\ntext{They}{0}{0}{they}
\ntext{Mashed}{1.3}{0}{mashed}
\ntext{Their}{2.6}{0}{their}
\ntext{Way}{3.7}{0}{way}
\ntext{Through}{5}{0}{through}
\ntext{The}{6.3}{0}{the}
\ntext{Thick}{7.4}{0}{thick}
\ntext{Mud}{8.5}{0}{mud}
\btext{Thick}{Mud}{$a$}
\btext{The}{Mud}{$d$}
\btext{Through}{Mud}{$j$}
\btext{Their}{Way}{$d$}
\btext{Mashed}{Way}{$o$}
\btext{Mashed}{Through}{$m$}
\btext{They}{Mashed}{$s$}
\end{graph}
\caption{A link grammar parse.}
\label{parse}
\end{center}
\end{figure}


An efficient parsing\index{parsers} algorithm for link grammar based on dynamic programming is described by \cite{Sleator:91}. Their link grammar for English can handle transitive, ditransitive and modal verbs; prepositions, adverbs, complex noun phrases and relative clauses; questions and question inversion; number agreement is also taken into account.

\subsection{Operator Formulation of Link Grammar}
\label{operator-formulation-section}

In this section we begin our description of link grammar in terms of operators on a vector space. The mathematics we will make use of is in fact derived from that of quantum mechanics: links are described as combinations of ``creation''\index{creation and annihilation operators} and ``annihilation'' operators referring to the creation and annihilation of a particle in a quantum mechanical\index{quantum mechanics} system.

%In information retrieval, the idea of representing words as vectors has been popular since the discovery of latent semantic analysis (LSA) \cite{Deerwester:90} in which vectors representing meanings of words are determined by analysing a corpus of documents. Such representations have proved to be effective in satisfying users' queries, because they generalise the query in a very flexible manner, returning documents whose vector representations are closest to the vector representation of the query.  More recently,

The mathematics of quantum mechanics has proved useful in retrieval\index{retrieval} applications for removing unwanted components of meaning in a search query \citep{Widdows:03} on latent semantic analysis\index{latent semantic analysis} vectors.
%Our ultimate goal is to be able to represent all aspects of language in terms of linear algebra, allowing a much more flexible definition of meaning than that traditionally used.
%This paper describes part of a much larger project which aims at describe all aspects of language including syntax and semantics in terms of algebra. Our hope is that this would provide a more flexible definition of meaning than previous approaches, and enable deeper and more subtle theoretical models and statistical analyses of language, also potentially leading the way to more efficient computational methods of handling statistical features of language.
%In order to reach this goal, it needs to be shown, for example, that natural language syntax can be adequately defined in terms of linear algebra. The goal of this paper is to show that this can be achieved using some of the mathematics of quantum mechanics.
Quantum mechanics deals with a kind of vector space that is particularly well behaved and frequently occurring, so called \emph{Hilbert space}\index{Hilbert space|(} (see Section \ref{completeness-section} for details).
%If we consider that more general aspects of meaning in language may be represented using such a space, then the mathematics of quantum mechanics becomes a natural place to look for the representation of other linguistic phenomena.
We make use of a special kind of infinite dimensional Hilbert space called \emph{Fock space}\index{Fock space}. As we will show, we can describe syntactic properties of words in terms of link grammars as operators on such a space.

One immediate benefit of this discovery is an entirely new perspective on link grammars, which may open up research on this type of grammar. For example, we will show how this view of link grammars can be used to describe the grammar in terms of matrix\index{matrices} operations, opening up the possibility of (potentially very efficient) computational procedures for statistical parsing using matrices.
%Our hope is that it will pave the way to a merging of vector representations of meaning with syntax, bringing the same flexibility to representation of phrases and sentences in computational linguistics as has been brought to that of words in LSA.

%In the remainder of this paper, we first introduce link grammar, then describe the mathematics that forms the foundation of our presentation, describe link grammar in quantum mechanical terms, and finally discuss our conclusions and further work. The paper is written with the non-technical reader in mind, and requires only an elementary knowledge of linear algebra.



%\subsection{Quantum Mechanics and Syntax}

%\subsection{Operators on Fock Space}

%We now begin our algebraic formulation of link grammar in terms of operators on Hilbert space. The required definitions are given in the appendix (section \ref{vectors}); a more complete description, is given for example by \cite{Kreyszig:89} which also applies the mathematics to quantum mechanics.

%This requires some important constructions of Hilbert spaces and an understanding of operators on Hilbert spaces. While it may seem like a lot of mathematics is required in order to represent something straightforward, the constructions are mathematical natural; moreover the importance of the Hilbert space formalism cannot be over-emphasised, for it occurs naturally in many areas of mathematics: many vector spaces can be considered as Hilbert spaces.

Our exposition is inspired by the study of \emph{free probability}\index{free probability} \citep{Voiculescu:97}, wherein the study of non-crossing diagrams is very closely connected to link grammars; our main result in this section is more or less a direct translation of a standard result in free probability theory.

Our syntactic vectors will reside in \emph{Fock Space}, a Hilbert space which is like the sum of an infinite series of Hilbert spaces.

%\subsubsection{Fock Space}

Let $H$ be a finite dimensional complex Hilbert space and $\Omega$ a distinguished vector in $H$ with norm 1. The Fock space\index{Fock space|textbf} $\mathcal{F}$ of $H$ is then defined as
$$\mathcal{F} = \mathbb{C}\Omega \oplus H \oplus (H \otimes H) \oplus (H \otimes H \otimes H) \oplus \cdots$$
i.e.~it is the direct sum of all finite tensor product powers of $H$, where $\oplus$ denotes the direct sum and $\otimes$ the tensor product (see section \ref{new-vector-spaces}), and $\mathbb{C}\Omega$ is a one dimensional Hilbert space which is viewed as the zeroth power of $H$.

%\subsubsection{Operators}


%\subsection{Creation and Annihilation Operators}
\index{creation and annihilation operators|(}
We are now able to form the connection between quantum mechanics and syntax.
In the physical interpretation of Fock space, different powers of the Hilbert space $H$ correspond to states of different numbers of particles. Special operators called \emph{creation operators} map states in $n$ powers of $H$ to states in $n+1$ powers of $H$, effectively `creating' an additional particle. Similarly, \emph{annihilation operators} reduce the number of powers of $H$ in a state by one, `annihilating' a particle. It is these operators that we will use to represent syntax.

Let $u$ be a vector in $H$. The creation operator $\ket{u}$ on $\mathcal{F}$ is defined such that
$$\ket{u} v_1\otimes v_2 \otimes \cdots \otimes v_n = u \otimes v_1\otimes v_2 \otimes \cdots \otimes v_n.$$
The \emph{dual} of $\ket{u}$ is the annihilation operator $\bra{u}$ and maps vectors according to:
$$\bra{u}v_1\otimes v_2 \otimes \cdots \otimes v_n = \inprod{u}{v_1}v_2 \otimes \cdots \otimes v_n$$
and $\bra{u}\Omega = 0$. The action of the operators on sums of tensor products can be deduced from their linearity.

The effect of `creating' and then `annihilating' is just a scalar product times the identity operator, 1:
$$\bracket{u}{v} = \inprod{u}{v}1;$$
the notation $\bracket{u}{v}$ is used whenever a creation operator follows an annihilation operator.


\subsection{Syntactic Interpretation}
\label{link-context-1}

In the syntactic interpretation of Fock space, the set of links $L$ are represented as a set of vectors $L_H$ which are assumed to form an \emph{orthonormal basis} for $H$. Disjuncts for words are then formed by concatenating creation and annihilation operators, in exactly the same way that left and right connectors are concatenated in link grammar. The representation of the syntactic characteristics of a word can then be represented by taking the sum of its disjuncts. For example the word \emph{mashed} in our simple link grammar in Table \ref{link} can be represented as the operator
$$\hat{\mathit{mashed}} = \ket{s}\bra{o} + \ket{s}\bra{m}\bra{o},$$
where we assume the vectors $s,o,m,a,j \in H$ form an orthonormal basis for $H$.

Our formulation will require that the link grammar parses are ``strict'' in the following sense: there must not be any connectors left unlinked; thus the parse must start with a right connector and end with a left connector.

In order to determine whether a sequence of words is in the language determined by the link grammar, we define a linear functional $\phi$ on $B(\mathcal{F})$ (the set of bounded linear operators on $\mathcal{F}$) by
$$\phi(\hat{a}) = \inprod{\Omega}{\hat{a}\Omega},$$
where $\hat{a} \in B(\mathcal{F})$.  We then have the following:
\begin{prop}
Let $W$ be a set of words, and $\Gamma$ a function that assigns a set of link grammar disjuncts to every word in $W$, with link types from a set $L$.

For every $w \in W$ we denote its corresponding Fock space operator $\hat{w}$ on the Fock space generated by the Hilbert space with basis vectors $L_H$  corresponding to the link types in $L$. Then $w_1w_2\ldots w_n$ is in the link grammar language defined by $\Gamma$ if and only if $\phi(\hat{s}) \ge 1$, where $s = \hat{w_1}\hat{w_2}\ldots\hat{w_n}$. $\phi(\hat{s})$ indicates the number of valid link grammar parses.
\end{prop}
\begin{proof}
Let us first assume each word has only one disjunct.
%In this case $\phi(\hat{s})$ is equal either to 0 or 1. If $\phi(\hat{s}) = 1$ then

The product of an annihilation operator with a creation operator satisfies
$$\bracket{x}{y} = \left\{\begin{array}{ll}
0 & \textrm{if $x \neq y$}\\
1 & \textrm{if $x = y$}\end{array}\right.,$$
where $x, y \in L_H$. Thus any operator $\hat{s}$ which is given by a product of creation and annihilation operators reduces either to $0$, $1$, or a product of a (possibly empty) sequence of creation operators followed by a (possibly empty) sequence of annihilation operators. In the latter case, as in the case of $0$, $\phi(\hat{s})$ will be zero since if there are annihilation operators in the sequence their operation on $\Omega$ will give zero (they operate on $\Omega$ first as they are on the right), and if there are no annihilation operators the creation operators will operate on $\Omega$ to give a vector disjoint with $\Omega$.

If the sequence satisfies any of the following the product will be zero and the sentence will not parse:
\begin{itemize}
\item A left connector is not matched by a right connector; in this case the product of the corresponding operator will map $\Omega$ to a different dimension in the Fock space and $\phi(\hat{s})$ will be zero.
\item The left connector is matched by a right connector of a different type; in this case the product of the corresponding operators will be zero.
\item The connectors match but the corresponding links cross; in the case there will again be a product of the form $\bracket{x}{y}$ where $x\neq y$ and the product will be zero.
\end{itemize}
Conversely, $\phi(\hat{s})$ will be zero just in case one of the above conditions holds and thus the sentence will not parse.

On the other hand, if none of the above conditions are met the sentence must parse and if the parse is strict the corresponding operator must map $\Omega$ to itself, so $\phi(\hat{s}) = 1$.

If words are now allowed more than one disjunct, then since these are added as operators and distribute with respect to multiplication each possible parse will be a term in the resulting sum of disjuncts, and thus $\phi(\hat{s})$ will indicate the number of valid link grammar parses.
\end{proof}

Note that this representation defines a strong context theory: the original Hilbert\index{Hilbert space|)} space $H$ is a vector lattice under the ordering induced by the basis associated with the set of link types, and thus $\mathcal{F}$ is also a vector lattice since we can define a basis for it using the basis of $H$. Thus the space of operators on this space also form a vector lattice, as well as an algebra; specifically we are interested in the algebra $\mathcal{A}$ generated by creation and annihilation operators. Together with the linear functional $\phi$ and the translation from strings to operators, where we assume that the empty string translates to the identity operator, we have a context theory. Moreover, the subspace $I = \{u \in \mathcal{A} : \phi(u) = 0\}$ is a sub-vector lattice of $\mathcal{A}$ since it is the space formed from all linear combinations of sequences of creation and annihilation operators which do not map $\Omega$ onto itself, thus we have a strong context theory.

\index{creation and annihilation operators|)}

\subsection{Stochastic Link Grammar}
\label{stochastic-link-grammar-section}
\index{link grammar!stochastic}

In applications requiring robust parsing of natural language stochastic grammars are vital in order to help in dealing with the large number of parses, which in general for wide coverage parsers increases exponentially with sentence length \citep{Manning:99}.

In the case of our implementation of link grammar we are not restricted to using sums of the basis vectors $L_H$, but can take any linear combination of these vectors when constructing the grammar, enabling us to form a type of stochastic link grammar similar to the supertagging models of \cite{Bangalore:99}. The representation of a word would be a weighted sum of the representation of its disjuncts; the weight attached to each disjunct can be interpreted as the probability that the word occurs in that syntactic r\^ole. For products of words, the weights attached to disjuncts will in general sum to less than 1 since some disjuncts will have a product of zero; it is thus necessary to renormalise the weights after taking the product to account for disjuncts whose product is zero in order to interpret them as probabilities.

Probabilistic link grammars were described by \cite{Lafferty:92}\index{Lafferty et al.}, where the probability of each link occurring with a word is conditioned on several factors, including the words occurring on either side. Such a model provides a probability distribution over the language generated by the grammar. They showed their formalism to be a generalisation of \emph{trigrams}\index{trigrams} which have proved very successful in language modelling. Our formalism does not allow conditioning of the probability directly, as Lafferty et al's does, however this information can be incorporated by including extra links describing the features one wishes to condition the probability on, and weighting these links accordingly.

%To see how stochastic link grammar may be formulated within our framework, define the \emph{trace} $\Tr(\hat{a})$ of an operator $\hat{a}$ as
%$$\Tr(\hat{a}) = \sum_{k} \inprod{\hat{a}e_k}{e_k}$$
%where the $e_k$ are vectors forming an orthonormal basis for $\mathcal{F}$. If we restrict our grammar to consider operators with trace $\le 1$ then the trace can be considered to correspond to the (unnormalised) probability of the word or sequence.

%The question of how such a stochastic grammar could be determined is, unfortunately, unanswered.


An advantage of this simpler formulation of stochastic link grammar in comparison to that of \cite{Lafferty:92} is that it allows an entirely lexicalised description of syntax: the grammar can be described by assigning each word its disjuncts and corresponding probabilities. The ultimate advantage however, we believe, will be in opening up new computational procedures for statistical parsing using matrices.

\subsection{Link Grammar and Matrices}
\label{lg-matrix-section}
\index{matrices!and link grammar|(}

The operators described in the previous section operate on an infinite-dimensional vector space --- something that is clearly difficult to implement. In practice, it may be possible to consider a finite-dimensional subspace of this vector space. This can be done by placing a limit on the number of left or right links that can be concatenated together. For example, we could use the subspace
$$\mathcal{F}_3 = \mathbb{C}\Omega \oplus H \oplus (H \otimes H) \oplus (H \otimes H \otimes H)$$
of the Fock space which is made up of $1 + n + n^2 + n^3$ dimensions, where $n$ is the number of dimensions of $H$. This would allow up to three left links and up to three right links to be concatenated. In general, allowing the concatenation of $k$ links would need $\sum_{i=0}^k n^i = \frac{n^{k+1} - 1}{n - 1}$ dimensions. 

The matrix representation of a link grammar can be built up using the standard definitions of tensor product and direct sum for matrices. For example, for a two dimensional vector space with basis vectors $a$ and $b$, for $k = 2$ we can assign the seven dimensions the following interpretations:
$$[\Omega, a, b, a\otimes a, a\otimes b, b\otimes a, b\otimes b]$$
The creation operator (left link) $\ket{a}$ would then have the matrix representation
$$\left(\begin{array}{ccccccc}
0&0&0&0&0&0&0\\
1&0&0&0&0&0&0\\
0&0&0&0&0&0&0\\
0&1&0&0&0&0&0\\
0&0&1&0&0&0&0\\
0&0&0&0&0&0&0\\
0&0&0&0&0&0&0
\end{array}\right)$$
since it maps $\Omega$ to $a$, $a$ to $a\otimes a$ and $b$ to $a\otimes b$. The corresponding annihilation operator $\bra{a}$ is represented by the matrix transpose of the representation of $\ket{a}$.

An important question to be addressed in future work is what the maximum number of concatenations is likely to be for a particular grammar and application; if this number is high the technique may become impractical because of the exponential increase in the number of required dimensions. One way to get around this problem may be to make use of a dimensionality reduction\index{dimensionality reduction}, such as that of random projections\index{random projections} \citep{Papadimitriou:98, Sahlgren:02}. In this technique, each basis vector in the original vector space is represented as a random vector in a new vector space of much lower dimensionality; this defines a transformation (a random projection) from the old vector space to the new. If the dimensionality of the new vector space is sufficiently high, it is highly likely that distances and scalar products between vectors will be preserved to within some threshold, however some further work is required to investigate the suitability of this technique for representing syntax.

\index{matrices!and link grammar|)}

%In order to apply this to the representation of syntax however, it is also important that lattice properties are preserved, thus some further work is required to guarantee that the projections that have this property.



%There is also a potential for the algebraic approach to open up new computational methods for dealing with this widespread ambiguity. Currently words are represented as operators on an infinite dimensional vector space and thus can be viewed as infinite matrices. It is possible that a way could be found to approximate these infinite matrices by finite matrices, for example by a dimensionality reduction of the kind used in random indexing \cite{Sahlgren:02}, in which each dimension of a high dimensionality space is projected onto a random subspace of a lower dimensionality space. This would open up the computational tools developed for matrix multiplication to tackling the problem of natural language parsing.

%\section{Conclusions}

%We have shown a deep connection between link grammar and the mathematics of quantum mechanics, specifically, that of creation and annihilation operators on Fock space, leading us to a new type of stochastic link grammar.

%It is our hope that this work will open up new avenues of research in the application of algebra to linguistics, allowing us to explore and make use of the vast body of work that exists in studies of linear algebra.

\subsection{Parsing with Operators}
\index{parsers!and operators}

So far we have only really treated the problem of acceptance of a language defined by a link grammar: we can tell if a sentence is in the language, but we are left with no record of the parse itself. This is not very useful in applications, since we are normally interested in finding out the structure of the sentence. In order to determine this structure as we multiply the operator representations, we need to be able to keep a record of which disjunct was used with each word. This can be done by defining a new vector space $H_d$ of dimensionality $d$, where $d$ is the greatest number of disjuncts that any word has in the grammar. We then form the Fock space\index{Fock space} $\mathcal{F}_d$ of this vector space and take the tensor product with the original Fock space in which the link grammar is represented. We now alter our original operators so that they operate on the new space $\mathcal{F} \otimes \mathcal{F}_d$. If a word has the original representation $x_1 + x_2 + \ldots x_d$ where the $x_i$ are the representations of the individual disjuncts, then in the new representation it becomes
$$x_1\otimes \ket{e_1} + x_2 \otimes \ket{e_2} + \ldots x_d\otimes \ket{e_d},$$
where the $e_i$ are basis vectors for $H_d$.

As these representations are multiplied, the product will be a sum of disjuncts; the right hand side of each disjunct will be a product of creation operators, each specifying the number of the disjunct used in the corresponding word. Those disjuncts of a word which cannot be used to form sentences will have a product of zero, and thus will not feature in the sum; nor will their tensor product with $\mathcal{F}_d$, thus only those disjuncts that can be used to form valid sentences will be represented in the product.


\subsection{Algebraic Formulation of Link Grammars}
\label{alg-lg-section}
\label{link}

The vector space formulation of link grammar we have just described provides us with a way to describe syntax within a context theory; it has also provided us with a way of computing with link grammars using matrices. However we are interested in combining representations of syntax with representations of meaning, and the formulation just described does not seem to be ideally suited to this. Describing words as operators on Fock space\index{Fock space} would allow meanings of larger constituents to be built up using tensor products only in a limited fashion: Fock space vectors work like a stack, and vectors can only be ``pushed'' or ``popped'' on this stack.

If we can describe syntax in algebraic terms, specifically in terms of semigroups, then we will be on much stronger ground because of the tools available for combining such representations. In particular, free inverse semigroups allow the representation of trees in algebraic terms. As we will see, we will not lose the flexibility of vector space representations; the vector space nature will be regained by considering the algebra $L^1(S)$ that can be associated with each semigroup $S$.

First we will describe a semigroup to represent link grammar in terms of strings of left and right connectors.
\begin{defn}[Bracket Semigroup]\index{bracket semigroup|textbf}
We define
$$D(L) = D_l(L) \cup D_r(L) \cup \{0\},$$
and let $\equiv$ be the minimal congruence on $D(L)^*$ satisfying
$$\bracket{x}{y} \equiv \left\{ \begin{array}{ll}
	0 & \text{if }x \neq y \\
	1 & \text{if }x = y
\end{array}\right.,$$
for all $x,y \in L$ and $0x \equiv x0 \equiv 0$ for all $x \in D(L)^*$, where $1$ is the empty string. Then the \emph{bracket semigroup on $L$} is defined as $D(L)^*/ \equiv$. We identify the equivalence classes of the bracket semigroup by their shortest elements.
\end{defn}

Note that the identities that form the congruence are similar to those satisfied by the creation and annihilation operators; in fact, the bracket semigroup is not more than an algebraic description of these operators. By combining this representation with the one we are about to describe we will have a description of syntax that combines the best of both the representations.



%As before, we can associate with each word $w$ in a set $W$ 

%\begin{prop}

%\end{prop}

%We can allow words to have more than one disjunct by defining an additional binary operation, denoted by $+$ and called addition, which is commutative and idempotent, and for which $0$ is a unity. Addition is otherwise defined freely with respect to the elements of the underlying semigroup. The resulting structure is an \emph{idempotent semiring}.

\subsection{Inverse Semigroups}
\label{inverse}


The bracket semigroup defined previously falls within a more general category of semigroups: that of inverse semigroups.

\begin{defn}[Inverse Semigroup]\index{inverse semigroups|textbf}
An inverse semigroup $S$ is a semigroup such that each element $x \in S$ has a unique element $x^{-1} \in S$ such that $xx^{-1}x = x$ and $x^{-1}xx^{-1} = x^{-1}$.
\end{defn}

\begin{prop}
A bracket semigroup is an inverse semigroup.
\end{prop}

\begin{proof}
Define $\bra{x}^{-1} = \ket{x}$ and $\ket{x}^{-1} = \bra{x}$. Let $x_1x_2\ldots x_n$ be a representative element of an equivalence class of a bracket algebra, then define
$$(x_1x_2\ldots x_n)^{-1} = x_n^{-1}x_{n-1}^{-1}\ldots x_1^{-1}.$$
Then the operation as given defines a unique inverse satisfying the requirements of an inverse semigroup.
\end{proof}

The identification of link grammars as a type of inverse semigroup  has led us to consider other kinds of inverse semigroup as a possible means of incorporating semantics into the formalism. We recount some basic properties of inverse semigroups \citep{Howie:76}.

Let $S$ be an inverse semigroup with set of idempotents\index{idempotents} $E(S)$. Then:
\begin{itemize}
\item $(a^{-1})^{-1} = a$ for all $a\in S$.
\item $aa^{-1} \in E(S)$ for all $a \in S$.
\item $aea^{-1} \in E(S)$ for all $a \in S$, $e\in E(S)$.
\item $e^{-1} = e$ for all $e \in E(S)$.
\item $ef=fe$ for all $e,f \in E(S)$, i.e.~idempotents commute, and thus form a subsemigroup of $S$.
\item A partial order $\le$ can be defined on $S$ by $a \le b$ if there exists $e \in E(S)$ such  that $a=eb$. If $a \le b$ then:
\begin{itemize}
\item[$\diamond$] $aa^{-1} = ba^{-1}$
\item[$\diamond$] $a = ab^{-1}a$
\item[$\diamond$] There exists $e \in E(S)$ such that $a=be$
\end{itemize}
\item The partial order is easily seen to be a generalisation of the semilattice order on a commutative semigroup of idempotents, defined by $e \le f$ if $ef = e$, and $e \land f = ef$.
\end{itemize}

\subsection{Free Inverse Semigroups}
\index{inverse semigroups!free}

The bracket semigroup does not store the `parse' of a sentence, it merely informs us whether a sentence parses or not. An alternative construction that is of great importance for our studies is the notion of a \emph{free} inverse semigroup. We can use this structure to represent syntax; as we will see, a link grammar parse of a sentence corresponds to an idempotent in a corresponding free inverse semigroup. In this representation, the parse can be deduced from the idempotent itself; the semigroup effectively stores information about the parse of the sentence. This allows us to build context theories in which the sentence structure is built up as words are concatenated; the sentence structure is represented by the context theory, which is an important step towards incorporating semantic information into this structure.

The crucial work on free inverse semigroups was done by \cite{Munn:74}\index{Munn, W.~D.} in which he proves that free inverse semigroups are isomorphic to \emph{birooted word-trees}\index{birooted word-trees}, also called Munn trees\index{tree!Munn}.

Informally, the free inverse semigroup on a set $A$ is formed from elements of $A$ and their inverses, $A^{-1} = \{a^{-1} : a \in A\}$, satisfying no other condition than those of an inverse semigroup. Formally, the free inverse semigroup is defined in terms of a congruence relation on $(A \cup A^{-1})^*$ specifying the inverse property and commutativity of idempotents --- see \cite{Munn:74} for details. We denote the free inverse semigroup on $A$ by $\FIS(A)$.


\subsection{Equivalence to Birooted Word-Trees}

A birooted word-tree on a set $A$ is a directed acyclic graph whose edges are labelled by elements of $A$ which does not contain any subgraphs of the form $\bullet \stackrel{a}{\longrightarrow} \bullet \stackrel{a}{\longleftarrow} \bullet$ or $\bullet \stackrel{a}{\longleftarrow} \bullet \stackrel{a}{\longrightarrow} \bullet$, together with two distinguished nodes, called the start node, $\Box$ and finish node, $\circ$.

A element in the free semigroup $\FIS(A)$ is denoted as a sequence $x_1^{d_1}x_2^{d_2}\ldots x_n^{d_n}$ where $x_i \in A$ and $d_i \in \{1,-1\}$.

 We construct the birooted word tree by starting with a single node as the start node, and for each $i$ from 1 to $n$:
\begin{itemize}
\item Determine if there is an edge labelled $x_i$ leaving the current node if $d_i = 1$, or arriving at the current node if $d_i = -1$.
\item If so, follow this edge and make the resulting node the current node.
\item If not, create a new node and join it with an edge labelled $x_i$ in the appropriate direction, and make this node the current node.
\end{itemize}
The finish node is the current node after the $n$ iterations.

As an example consider the set $A = \{a,b,c,d\}$, and the element in $\FIS(A)$ given by the sequence
$$aaa^{-1}bcdbb^{-1}aa^{-1}d^{-1}c^{-1}ac.$$
This has the following graph:

\begin{center}
\begin{graph}(10,6)(-1,-5)
\newcommand{\diredgetext}[3]{\diredge{#1}{#2}\edgetext{#1}{#2}{#3}}

\squarenode{R1}(0,0)[\graphnodecolour{1}]
\roundnode{R2}(2,0)
\roundnode{R3}(2,-2)
\roundnode{R4}(4,0)
\roundnode{R5}(4,-2)
\roundnode{R6}(6,-2)
\roundnode{R7}(6,-4)
\roundnode{R8}(8,-2)
\roundnode{R9}(6,0)
\roundnode{R10}(8,0)[\graphnodecolour{1}]
%\diredge{R1}{R2}
\diredgetext{R1}{R2}{$a$}
\diredgetext{R2}{R3}{$a$}
\diredgetext{R2}{R4}{$b$}
\diredgetext{R4}{R5}{$c$}
\diredgetext{R5}{R6}{$d$}
\diredgetext{R6}{R7}{$b$}
\diredgetext{R6}{R8}{$a$}
\diredgetext{R4}{R9}{$a$}
\diredgetext{R9}{R10}{$c$}
\end{graph}
\end{center}

The product of two elements $x$ and $y$  in the free inverse semigroup can be computed by finding the birooted word-tree of $x$ and that of $y$, joining the graphs by equating the start node of $y$ with the finish node of $x$ (and making it a normal node), and merging any other nodes and edges necessary to remove any subgraphs of the form  $\bullet \stackrel{a}{\longrightarrow} \bullet \stackrel{a}{\longleftarrow} \bullet$ or $\bullet \stackrel{a}{\longleftarrow} \bullet \stackrel{a}{\longrightarrow} \bullet$.

The inverse of an element has the same graph with start and finish nodes exchanged.

\subsection{Syntactic Equivalence}

We can represent parses of sentences in link grammar by translating words to syntactic categories in the \emph{free inverse semigroup} instead of the bracket algebra. In this case sentences are represented as idempotents. For example, the parse shown earlier for ``they mashed their way through the thick mud'' can be represented in the inverse semigroup on $A = \{s,m,o,d,j,a\}$ as
$$ss^{-1}modd^{-1}o^{-1}m^{-1}jdaa^{-1}d^{-1}j^{-1}$$
which has the following birooted word-tree:

\begin{center}
\begin{graph}(8,8)(0,-7.5)
\newcommand{\diredgetext}[3]{\diredge{#1}{#2}\edgetext{#1}{#2}{#3}}
\squarenode{R1}(4,0)[\graphnodecolour{1}]
\roundnode{R2}(0,-2)
\roundnode{R3}(4,-3)
\roundnode{R4}(4,-5)
\roundnode{R5}(4,-7)
\roundnode{R6}(8,-2)
\roundnode{R7}(8,-4)
\roundnode{R8}(8,-6)
\roundnode{R1Copy}(4,0)[\graphnodecolour{1}]
%Edges:
\freetext(0,-1){$s(\text{they},\text{mashed})$}
\diredge{R1}{R2}
\diredgetext{R1}{R3}{$m(\text{mashed},\text{through})$}
\diredgetext{R3}{R4}{$o(\text{mashed},\text{way})$}
\diredgetext{R4}{R5}{$d(\text{their},\text{way})$}
\diredge{R1}{R6}
\freetext(8,-1){$j(\text{through},\text{mud})$}
\diredgetext{R6}{R7}{$d(\text{the},\text{mud})$}
\diredgetext{R7}{R8}{$a(\text{thick},\text{mud})$}
\end{graph}
\end{center}

%\begin{center}
%\begin{graph}(6,5)(0,-0.5)
%\newcommand{\diredgetext}[3]{\diredge{#1}{#2}\edgetext{#1}{#2}{#3}}
%\squarenode{R1}(0,2)[\graphnodecolour{1}]
%\roundnode{R2}(0,4)
%\roundnode{R3}(2,2)
%\roundnode{R4}(4,2)
%\roundnode{R5}(6,2)
%\roundnode{R6}(0,0)
%\roundnode{R7}(2,0)
%\roundnode{R8}(4,0)
%\roundnode{R1Copy}(0,2)[\graphnodecolour{1}]
%%Edges:
%\diredgetext{R1}{R2}{$s$}
%\diredgetext{R1}{R3}{$m$}
%\diredgetext{R3}{R4}{$o$}
%\diredgetext{R4}{R5}{$d$}
%\diredgetext{R1}{R6}{$j$}
%\diredgetext{R6}{R7}{$d$}
%\diredgetext{R7}{R8}{$a$}
%\end{graph}
%\end{center}
In this graph, the fact that start and finish nodes overlap indicates that the element is idempotent. The nodes linked by the grammar are indicated in brackets; later we will be able to attach the meanings of these words to the links in the grammar.

We formalise the equivalence with the following proposition:
\begin{prop}
Let $S$ be the free inverse semigroup on the set of link types. The inverse semigroup representation of a disjunct is the element of $S$ formed by replacing each left and right link of type $a$ with elements $a \in S$ and $a^{-1} \in S$ respectively. Then if a sequence of disjuncts is a link grammar parse the product of the inverse semigroup representation of the disjuncts is idempotent.
\end{prop}
\begin{proof}
Let $x$ be the concatenation of disjuncts (we can also interpret $x$ as an element of $S$), and let $a$ be the first (leftmost) element of the sequence $x$. If the sequence of disjuncts is a link grammar parse then for each left connector there is a corresponding right connector on its right, and each connector is connected to exactly one other connector, so the first connector must be a left connector and there must be a corresponding $a^{-1}$ to represent its right connector on the right. Let $y$ be the subsequence of $x$ such that $x = aya^{-1}z$ for some sequence $z$. If $y$ and $z$ are both the empty string then $x$ is idempotent since $aa^{-1}$ is idempotent. Since no links cross, both $y$ and $z$ must satisfy the same conditions as $x$, and hence by induction, $x$ is idempotent, since $aea^{-1}$ and $aa^{-1}e$ are both idempotent for any idempotent $e$ in the inverse semigroup.
\end{proof}

Note that the converse implication does not hold in general since $a^{-1}a$ is also idempotent; thus this formulation allows right connectors to precede left connectors just as well as succeed them. In practice this should not be a problem since it is likely that the grammar can be redesigned in such a way that unwanted idempotents do not occur.

\subsection{A Semigroup for Syntax}

Both the bracket semigroup\index{bracket semigroup} and the free inverse semigroup\index{inverse semigroup!free} accurately represent syntax according to link grammar, however both have advantages and disadvantages for practical application in representing syntax. The free inverse semigroup stores information about the parse in a Munn tree\index{tree!Munn}, however combinations which don't parse will be `left over'. In the bracket semigroup, combinations which don't parse have a product of zero, so are ignored, but there is no memory of the parse.

For example, suppose nouns may optionally be preceded by an adjective ($a$) before taking a determiner ($d$) which we represent as $n_f = a^{-1}d^{-1} + d^{-1}$ in the $L^1$ algebra of the free inverse semigroup, and as $n_b = \ket{a}\ket{d} + \ket{d}$ in the $L^1$ algebra of the bracket semigroup. If the noun is now preceded by a determiner, $d$ or $\bra{d}$ respectively, then in the free inverse semigroup we have
$$dn_f = da^{-1}d^{-1} + dd^{-1}$$
while in the bracket semigroup we have $\bra{d}n_b = 1$ since $\bracket{d}{a} = 0$. Thus the free inverse semigroup correctly stores the idempotent $dd^{-1}$ but leaves the non-syntactic construction $da^{-1}d^{-1}$, while the bracket semigroup correctly cancels out this construction, but has no memory of the parse.

To get around this problem we combine the two structures; to do this we will need the \emph{direct product}. Given two semigroups $S_1$ and $S_2$ the direct product is the cartesian product $S_1 \times S_2$ with the semigroup product defined by
$$ (x_1,y_1)\cdot (x_2,y_2) = (x_1x_2,y_1y_2). $$
If $S_1$ and $S_2$ are inverse semigroups, then $S_1 \times S_2$ is an inverse semigroup, with inverse $(x,y)^{-1} = (x^{-1},y^{-1})$.

Given a set $A$ of links, we take the direct product of the free inverse semigroup on $A$ and the bracket semigroup on $A$, modulo an equivalence which makes elements zero in the bracket semigroup zero in the product. That is, the semigroup for syntax  is defined as
$$\FIS(A) \times B(A)\ / \equiv,$$
where $\equiv$ is defined by $(x,0) \equiv (y,0)$ for all $x,y \in \FIS(A)$. We are actually interested in the subsemigroup $S_s(A)$ generated by elements of the form $(a, \bra{a})$ and $(a^{-1},\ket{a})$ for all elements $a \in A$. We denote these elements $\sbra{a}$ and $\sket{a}$ respectively, and the idempotent $(aa^{-1},1)$ as $\sbracket{a}{a}$.

Our example then becomes
$$\sbra{d}n_s = \sbra{d} \Big(\sket{a}\sket{d} + \sket{d}\Big) = \sbracket{d}{d}$$
where $n_s = \sket{a}\sket{d} + \sket{d}$ is the representation of the noun in $S_s(A)$.

\subsection{From Semigroups to Context Theories}
\label{semigroup-context}

In this section we show how a context theory can be constructed from a semigroup. First we construct an algebra $L^1(S)$ of functions on a semigroup $S$ with multiplication defined by convolution (see section \ref{algebras}). This makes an algebra from a semigroup as we would expect intuitively; if $a,b,c,d\in S$ and $\alpha,\beta,\gamma,\delta \in \R$ then in $L^1(S)$ we have
$$(\alpha e_a + \beta e_b)(\gamma e_c + \delta e_d) = \alpha\gamma e_ae_c + \alpha\delta e_ae_d + \beta\gamma e_be_c + \beta\delta e_be_d$$
where $e_a$ is the basis element of $L^1(S)$ corresponding to $a$, that is the function that is 1 on $a$ and 0 elsewhere.

If, however, $S$ possesses a zero $\theta$, then this will not automatically be the zero of the algebra, instead it will be a function of $\theta$. What we will do is effectively equate the part of the algebra relating to $\theta$ to zero. Let $\bm{\theta}$ denote the ideal generated by $\theta$,
$$\bm{\theta} = \{\alpha\theta : \alpha \in \R\},$$
(assuming a real vector space). Then we are interested in the vector space $L^1(S)/\bm{\theta}$, that is the vector space of equivalence classes $x + \bm{\theta} = \{x + y: y \in \theta\}$. Addition and scalar multiplication in this space is defined by
\begin{gather*}
(x + \bm{\theta}) + (y + \bm{\theta}) = x + y + \bm{\theta}\\
\alpha(x + \bm{\theta}) = \alpha x + \bm{\theta}
\end{gather*}
Since $L^1(S)$ is also an algebra, we can define multiplication on $L^1(S)/\bm{\theta}$ by
$$(x + \bm{\theta})(y + \bm{\theta}) = xy + \bm{\theta}.$$
The equivalence class $0 + \bm{\theta}$ is now a zero of the vector space and the algebra; when there is no ambiguity, we shall simply denote it by $0$. If $ab = \theta$ in $S$, then in the algebra $L^1(S)/\bm{\theta}$ we have $e_ae_b = 0$.

Since $\bm{\theta}$ is a vector lattice supspace of $L^1(S)$, the space $L^1(S)/\bm{\theta}$ is a vector lattice; clearly it is also a lattice ordered algebra under the multiplication of $S$.

Since the $L^1$ norm is finite in the space $L^1(S)$ we can use it to define a linear functional:
$$\phi(u) = \|u^+\|_1 - \|u^-\|_1$$
Thus together with an assignment from words to elements of $L^1(S)$, we have a context theory.

\subsection{Relating Link and Categorial Grammars}
\index{categorial grammar!and link grammar}

The inventors of link grammar describe a translation from Bar-Hillel type categorial grammars to link grammar \citep{Sleator:93}. They describe it recursively in terms of a function $E$ that takes a categorial expression and returns a link grammar expression. In our notation, it can be expressed as follows:
\begin{itemize}
\item The set of link types $L$ is the set of categorial expressions.
\item If a word has a set $\{x_1,x_2,\dots,x_n\}$ of categorial expressions, then it is represented by the sum
$$E(x_1)+E(x_2)+\ldots E(x_n).$$
\item The representation of a basic type $A$ is
$$E(A) = \ket{A} + \bra{A}.$$
\item The representation of other categories is given by
\begin{eqnarray*}
E(x/y) &=& \ket{x/y} + \bra{x/y} + E(x)\bra{y}\\
E(y\backslash x) &=& \ket{y\backslash x} + \bra{y\backslash x} + \ket{y}E(x)
\end{eqnarray*}
\end{itemize}
As \citeauthor{Sleator:93} note, the size of the link grammar representation is linear in the size of the categorial grammar representation, thus they expect that translating to link grammar would be an effective method of parsing categorial grammars. From our perspective, there is an additional potential use for the translation: the connection enables a new way of implementing statistical categorial grammars, using the statistical link grammar formalisms.

\index{link grammar|)}

\section{Discussion and Further work}
\label{discussion-syntax-section}

Using the constructions of the previous section, we have described a formalism that parses\index{parsers} sentences in a purely algebraic fashion. The advantage of this algebraic description over the operator-based description is that the parse itself is stored in algebraic form and does not need to be reconstructed from information about which disjunct was used with each word. This is due to the extra structure provided by the free inverse semigroup which allows tree-like structures to be represented. It is this structure that we believe will also be useful for constructing representations of meaning directly within the context theoretic framework. For example, it may be possible to find link grammars for natural language such that the Munn tree\index{tree!Munn} of a sentence describes relationships between the words in the sentence. This can already be seen to be true to some degree: for example, in the tree we showed for the sentence ``They mashed their way through the thick mud'', the branch relating to ``thick'' comes off the branches relating to ``mud'', in terms of idempotents, the idempotent representing ``the thick mud'' is more specific than that representing ``the mud''. The trees still bear little resemblance to the dependency trees that we are familiar with, however.

We have now described methods for representing meaning and syntax in algebra. The question arises how one may combine such methods to produce lexicalised algebraic representations of language incorporating both meaning and syntax. One may wish to choose a particular vector-based semantic formalism and a particular syntactic formalism and combine them. One way of doing this may be the mathematics of \emph{free probability}\index{free probability|textbf} \citep{Voiculescu:97}. The concept of freeness generalises the concept of independence to the case of non-commutative variables. Two sub-algebras $A_1$ and $A_2$ of an algebra are said to combine freely with respect to a linear functional $\phi$ if $\phi(x_1x_2\ldots x_n) = 0$ whenever all the $x_i$ satisfy $\phi(x_i) = 0$ and no two adjacent $x_i$ are in the same sub-algebra. Given two non-commutative probability spaces, one can construct their free product as an algebra which has sub-algebras isomorphic to the original algebras and satisfying the condition of freeness. Thus one could choose a context-theory to represent meaning and a context-theory to represent syntax and build a combined context-theory using the free product, in which each word would map to a product of its original syntactic and semantic representations. The idea that meaning and syntax combine freely is appealing since we are used to thinking of these two aspects of language separately; the concept of freeness may encapsulate this idea well, however exactly how it would work in practice remains to be seen.

We have left the question of how to compute with these new representations largely unanswered, however we are representing existing formalisms for which computational procedures already exist. Thus it may be possible to make use of existing algorithms with small adjustments to compensate for the differences that the context-theoretic perspective requires. It is our hope however that new and more efficient computational procedures will be brought to light by considering the algebraic approach, particularly in the area of statistical parsing. One area that seems particularly worthy of further investigation is the use of matrices\index{matrices} to approximate elements of algebras, along the lines of the description we gave for Fock space operators in terms of matrices.

%The complexity of the algebraic representation in this formulation means that the internal representation and computational procedures for parsing link grammar are unlikely to change with regards to existing implementations of link grammar parsers.

%
%\bibliographystyle{plainnat}
%\bibliography{contexts.bib}

%
%%\end{article}
%\end{document}
