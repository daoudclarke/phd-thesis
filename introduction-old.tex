%Bismillahi-r-Rahmani-r-Rahim
\documentclass{report}

\include{head}
\usepackage{fancyvrb}
\usepackage{pstricks}

\begin{document}

 \chapter{Introduction}
 
 In this chapter we set out our vision for the future of computational linguistics; what we propose is no less than a revolution in the philosophical and mathematical foundations of the subject. Nevertheless as we will try and show, our proposed philosophy and mathematical framework is a natural extension of the practical techniques already in use in computational linguistics; indeed our view is that it is the result of taking existing work to its logical ends.
 
We will argue that it is not consistent for existing research to make such heavy use of statistical techniques, while the underlying philosophy and mathematics are not directly compatible with the representations arising from statistical analysis. From this perspective, what we are proposing is not controversial, however controversy may arise if the philosophy and mathematics are considered on their own merits; this will be discussed in due course.

In this chapter we summarise our vision of an algebraic approach to meaning in natural language, putting forward  the case for such an approach. We then introduce the accompanying philosophy, and give an overview of the remainder of this document.

 \section{A vision of algebra and meaning}

In this section, we set out our view of a possible future for the foundations of computational linguistics. We believe that it is essential for a reasoned and logical long term development of this field to have strong theoretical foundations. It is our belief that computational linguistics has outgrown any foundations that might have once existed as the emphasis on statistical techniques  has increased, and no comprehensive formalism has yet been proposed as a replacement.

What we are proposing here is a mathematical formalism that we believe is particularly well suited to the representations resulting from statistical techniques. We view this formalism as a \emph{framework} identifying the boundaries within which exploration can begin. Having identified the boundaries, efforts can be concentrated on describing various aspects of language within the framework.

We believe the framework has many potential benefits within computational linguistics. Firstly, it aims to unify varying aspects of language from syntax to meaning within a single formalism. This has the benefit that language is viewed as a unified whole, giving us new perspectives on what meaning and language truly are.

Secondly, since the emphasis is on representing statistical aspects of meaning, there is a potential for building more robust systems. Robustness is a feature of statistical techniques, so a formalism that combines statistical representations of meaning and syntax can combine the robustness of both areas without compromise.

 \subsection*{Algebra}
 
It is informative, before we start, to discuss some of the potential meanings and origins of the word ``algebra''. The word originates from the arabic \emph{al-jabr}, meaning ``the reunion''  or ``the reduction'' after the work by ninth century mathematician al-Khw\=arizm\=\i{} (also responsible for the word ``algorithm''), \emph{al-Kit\=ab al--mukhta\d sar {f\=\i}  \d his\=ab al-jabr wa-l-muq\=abala}, or \emph{The Book of Summary Concerning Calculating by Reduction and Transposition}. The modern interpretations of the word within mathematics are varied, the most important of which for our purposes are as follows:
\begin{itemize}
\item the branch of mathematics in which the elementary operations of arithmetic are extended to \emph{variables}, symbols representing real numbers;
\item \textbf{abstract algebra:} the study of systems in which operations are defined with specific properties, for example \emph{fields}, \emph{vector spaces}, \emph{groups} and \emph{semigroups};
\item \textbf{algebra over a field:} a \emph{vector space} with a multiplication operation defined, making it a \emph{ring}, satisfying certain conditions.
\end{itemize}
The first of these is that we are all familiar with, while the second and third meanings are those that we shall be making use. When we use the term ``algebra'' we usually intend the second of these meanings, which is the most general sense, however almost all of the mathematical structures we consider have an underlying \emph{vector space} structure, which means that we will also often make use of the third sense above. In addition, because of this, when we make use of the general sense there is an implied emphasis on those structures in particular that have the properties of a vector space.

\subsection*{Meaning}

It is our belief that the recent developments in statistical techniques in computational linguistics require an accompanying development in our ideas of what meaning really is. Our idea of meaning is still connected to representations with their foundations in logic, and it is hard to imagine such representations capturing the subtleties of meaning that can be represented using the vector space representations acquired automatically.

What we propose then, is a theory of meaning that incorporates a vector space nature. Such a theory would be in better agreement with modern techniques in computational linguistics, and allow the more ``fine-grained'' descriptions of meaning that accompany them in vector representations.

The theory of meaning we propose has its foundations in the philosophy of Wittgenstein, who said ``Meaning just \emph{is} use'', and Firth, who said ``You shall know a word  by the company it keeps''. That is, the meaning of a sequence of symbols should be determined merely by looking at how they are used, or where they occur in a large amount of text.

The controversial part of what we propose is that meaning should be attached \emph{purely to context} without any assumed reference to the real world. It should be noted that this concept of meaning is quite different to our normal notion of meaning. An example that was proposed to me as an objection to such a definition of meaning is that we know that a \emph{man} is a \emph{human being} and a \emph{human being} is a \emph{mammal}. However, looking at the contexts that these words occur in, we would not expect significant overlap, for example, between the contexts that the word ``man'' and ``mammal'' occur in. There might be some overlap, but arguably not enough to demonstrate that a \emph{man} is a \emph{mammal}, that is that \emph{man} has all the properties of \emph{mammal}. We have to be able to accept, in our proposed definition of meaning that \emph{according to the definition} a man isn't a mammal! This is a consequence of the fact that we do not require our definition of meaning to be connected in any way to the real world. To separate this view of meaning from theories of meaning intended to represent the real world, we call the new interpretation of meaning \emph{context semantics}.\footnote{The term ``context semantics'' has already been used in the context of program analysis, as a less mathematical version of Girard's \emph{Geometry of Interaction} \citep[see for example][]{Girard:95}. Interestingly, the Geometry of Interaction makes use of Hilbert spaces and C* algebras for entirely different reasons to our own use of vector spaces. The use of the word ``context'' here is not related to our use of the term, so we think that no ambiguity will arise.} We may thus talk about the context semantics of a set of terms, in which it is not true that a \emph{man} is a \emph{mammal}, and their \emph{model-theoretic} semantics, in which this is true, as meanings are intended to be connected to a model of the real world.

Context semantics places emphasis on the words themselves, and ignores the fact that words may represent concepts. This emphasis on context coincides with the modern use of statistical techniques in computational linguistics which have proved so effective in many applications. Whilst context semantics may be dissatisfying from the perspective of some of our intuitive notions of meaning (a man is not a mammal) from a practical perspective, context semantics has the potential to successfully describe useful relationships in meaning. It is not often that it is practically \emph{useful} to know that a man is a mammal --- and the evidence from statistical techniques seem to show that the useful relationships very often \emph{are} those determined by context.

Of course this is not true in every situation; context semantics may not be useful for many applications. It seems likely from what we have found that context semantics would be good in situations where a limited form of reasoning is required, but in ``fuzzy'' manner; and may be of limited use in situations which require complex reasoning and inference.


 \section{Why Algebra?}


Our interest in the use of algebra stems from the ability of algebra to make effective use of vector representations. Vector representations are attractive for numerous reasons. Vectors are very flexible mathematical structures: they can be added, multiplied by scalars, averaged, rotated, we can measure angles between them; more generally we can operate on them (in certain cases) with matrices.

Our main reason for interest in vector representations, however is the demonstrated usefulness of techniques in computational linguistics and information retrieval that make use of vectors, most notably measures of \emph{distributional similarity} \citep{Lee:99} and \emph{latent semantic analysis} \citep{Deerwester:90} and its variations. These techniques have proved useful in many applications, including automatic thesaurus extraction, word sense disambiguation, textual entailment recognition and information retrieval. We believe that the usefulness of these techniques points to an underlying property of the nature of the meaning of words: \emph{the fact that vector-based techniques are successful is an indication that meanings of words have a vector nature}.

There is another reason for our interest in vector representations. We are particularly interested in representing statistical or probabilistic features of language, and vector spaces have already demonstrated their usefulness in another probabilistic setting: that of quantum mechanics. While the differences between the two situations outweigh the similarities, the analogy still provides hope for the usefulness of vector-based representations in representing meaning in language.
 
% \section{A new philosophy of meaning}
 
 \section{Words and Vectors --- Existing Work}
 
The purpose of this section is to review existing work on representing words using vectors. All of the work we are aware of focuses on automatic techniques to determine some aspect of the meaning of words, or relationships between their meanings, based on the contexts they occur in within some large corpus of text. It is this that has led us to formalise meaning in terms of context, and thus it is informative for us to study these techniques. In particular we wish to focus on the mathematical properties of vectors involved in such analyses; this will allow us to abstract from these techniques to gain an insight into the nature of meaning.
 
%Measures of distributional similarity attempt to put a measure of similarity on terms based on the contexts they occur in. Terms are represented by ``feature vectors'' whose nature depends on what features of the contexts we are interested in modeling, for example dependency relations might be used to form these vectors. Once these vectors are obtained, numerous mathematical formulae are available to measure similarity, for example, we can choose this to be the cosine of the angle between the vectors. 

%In latent semantic analysis, vectors are also used to represent contexts that terms occur in, although in this case, the context is more commonly defined by looking at terms that occur within a window, or simply considering what documents a term occur. A different kind of analysis is applied, which attempts to deduce ``latent'' information about the meaning of a term by means of a dimensionality reduction. The resulting vectors are a compact representation of the meaning of the term, and have proved useful in information retrieval applications.

\subsection{Latent Semantic Analysis and its Variants}

The technique of latent semantic analysis and the more probabilistic techniques that followed it, arose from the work of \cite{Deerwester:90}, in the context of the task of information retrieval. We will give only a brief overview here, since the details are not directly relevant to our work.

It is common in information retrieval to represent a term by the vector of documents it occurs in. Figure \ref{fruittable} gives a set of hypothetical occurrences of six terms in eight documents. Given a user query term, the information retrieval software will then return the documents that have the most occurrences of that term. From the vector perspective, the documents are identified with the \emph{components} of the vector representation of a term; given a query term, the most suitable document corresponds to the \emph{greatest component} of the term's vector representation.

It is often the case, however, that there are documents that are suitable matches for a given query which do not contain that query term often, or even at all. These will not be returned by the straightforward matching technique, and latent semantic analysis aims to get around this problem. It aims to deduce ``latent'' information about where terms may be expected to appear, by performing a dimensionality reduction. This is performed in such a way that the most important components of meaning are retained, while those thought to represent noise are discarded. For example, in table \ref{fruittable}, \emph{banana} and \emph{orange} never occur together, however they both occur with \emph{apple} and \emph{fruit} which provides evidence that they are related. Latent semantic analysis aims to deduce this relation.

Latent semantic analysis works by reducing the number of dimensions in which information can be stored, keeping only the most important information about the original matrix. This \emph{dimensionality reduction} has the effect of moving vectors that were unrelated closer together, as they are ``squashed'' into a space of lower dimensionality.

Figure \ref{reduce} is intended to give an idea of how this works. The outer rectangles represent  the matrices arrived at by singular valued decomposition, their product gives the original matrix representing the table of term-document co-occurrences. These matrices are arranged so that the most important information is stored in the top and left areas, with less important information being stored towards the bottom and right. In latent semantic analysis, a rectangle of the most important information is chosen (the inner rectangles); this information is kept and the remaining areas of the matrices are discarded --- these are assumed to contain only noise information.
 
Table \ref{approx} shows the latent semantic analysis approximation to table \ref{fruittable}. In this case we chose to keep only two dimensions for the inner rectangles. We can see that in the new table, \emph{banana} and \emph{orange} now have components in common --- latent semantic analysis has forced them into a shared space. Because there were only two dimensions available, the term \emph{computer}, which before only shared components with \emph{apple} has been forced nearer to all the other terms, but remains closest to the term \emph{apple} as we would expect.



%Latent semantic analysis works in as follows. The matrix $M$ representing the original table can be decomposed into three matrices, $M = UDV$, where $U$ and $V$ are unitary matrices and $D$ is a diagonal matrix containing the \emph{singular values} of $M$. Figure \ref{reduce} shows how the dimensionality reduction is performed. The decomposition can be rearranged so that the most important components --- those with the greatest singular values --- are in the top left of the matrix $D$, the dimensionality reduction is then performed by discarding the less important components, resulting in smaller matrices $U'$, $V'$ and $D'$. The matrix $M$ is then \emph{approximated} by the product of the new matrices, $M \simeq U'D'V'$.

%For example, if we take table \ref{fruittable} as matrix $M$, then the decomposed and reduced matrices are those in figure \ref{decompose}. In this case we chose two keep only two dimensions corresponding to the greatest singular values ($12.8$ and $9.46$); keeping more dimensions would mean that more features of the original matrix would be preserved.
\begin{table}
\newcommand\T{\rule[-1.2ex]{0pt}{3.7ex}}

\begin{center}
\begin{tabular}{|l|cccccccc|}
\hline
	 	& $d_1$\T	& $d_2$	& $d_3$	& $d_4$	& $d_5$ & $d_6$ & $d_7$ & $d_8$\\
\hline
banana\T	& 2		& --		& --		& --		& 5		& --		& 5		& --\\
apple\T	& 4		& 3		& 4		& 6		& 3		& --		& --		& --\\
orange\T	& --		& 2		& 1		& --		& --		& 7		& --		& 3\\
fruit\T	& --		& 1		& 3		& --		& 4		& 3		& 5		& 3\\
tree\T	& --		& --		& 5		& --		& --		& 5		& --		& --\\
computer\T& --		& --		& --		& 6		& --		& --		& --		& --\\
\hline
\end{tabular}
\caption{A table of hypothetical occurrences of words in a set of documents, $d_1$ to $d_8$.}
\label{fruittable}
\end{center}
\end{table}

%\begin{figure}
%$$\left( \begin{array}{cc}
%.335  &  -.175 \\
%.504   & -.619 \\
%.392   & .514\\
%.564   & .177\\
%.374   & .341\\
%.141   & -.415 
%\end{array}\right)
%\left( \begin{array}{cc}
%  12.8   &   0  \\
%   0   &   9.46
%\end{array}\right)
%\left( \begin{array}{cc}
%.209   &  -.298  \\
%.223    & -.0686 \\
%.466   & .0295 \\
%.302   &  -.655  \\
%.425   &  -.213  \\
%.492   &  .617 \\
%.351   & .00101\\
%.224    & .219 \\
%\end{array}\right)^\mathrm{T}$$
%\caption{The matrices $U'$, $D'$ and $V'$ formed from singular value decomposition and dimensionality reduction. The product approximates the original matrix in table \ref{fruittable}. Here $A^\mathrm{T}$ is used to mean the transpose of matrix $A$.}
%\label{decompose}
%\end{figure}

\begin{table}
\newcommand\T{\rule[-1.2ex]{0pt}{3.7ex}}

\begin{center}
\begin{tabular}{|l|cccccccc|}
\hline
	 	& $d_1$\T	& $d_2$	& $d_3$	& $d_4$	& $d_5$ & $d_6$ & $d_7$ & $d_8$\\
\hline
banana\T & 1.40 & 1.08 & 1.95 & 2.40 &  2.19 & 1.09 &  1.51 & .597 \\
apple\T	& 3.11 & 1.85 & 2.84 & 5.80 &  4.00 & -.44 &  2.26 &  .17 \\
orange\T	& -.40 & .795 & 2.48 & -1.68 & 1.10 & 5.49 &  1.77 & 2.20 \\
fruit\T	& 1.02 & 1.50 & 3.41 & 1.08 &  2.71 & 4.60 &  2.53 & 1.99 \\
tree\T	& .041 & .847 & 2.33 & -.68 &  1.35 & 4.36 &  1.68 & 1.78 \\
computer\T& 1.56 & .679 & .731 & 3.13 &  1.62 & -1.53 & .635 & -.455\\
\hline
\end{tabular}
\caption{An approximation to the table obtained from a singular-valued decomposition followed by a dimensionality reduction to two dimensions.}
\label{approx}
\end{center}
\end{table}

\begin{figure}
\begin{center}
\input{matrices.pst}
\end{center}
\caption{Matrix decomposition and dimensionality reduction in latent semantic analysis.}
\label{reduce}
\end{figure}

Latent semantic analysis in its original form has its problems many of which have now been resolved to a greater degree by new techniques. For example, the new, approximate matrix may contain negative values, as our example shows (figure \ref{approx}). This is undesirable, as the matrix is intended to represent expected co-occurrence frequencies, and these cannot be negative; this is a result of the techniques lack of grounding in a sound probabilistic analysis of the situation.

\emph{Probabilistic latent semantic analysis} \citep{Hofmann:99} is a technique which has the same aim, but solves this problem in a probabilistic fashion. It treats the occurrence of words and documents as random variables, and postulates the existence of a hidden variable (see figure \ref{plsa}). This resolves the issue of negative values, and puts the technique on a firmer theoretical foundation.

\emph{Latent Dirichlet Allocation} \citep{Blei:03} provides an even more in-depth Bayesian analysis of the situation. The problem with probabiliistic latent semantic analysis, the authors propose, is that there is an assumed finite number of documents. This is not the true situation: the documents available should be viewed as a sample from an infinite set of documents. In order to achieve this, they model documents as samples from a \emph{multinomial distribution} --- a generalisation of the binomial distribution.

\begin{figure}
\begin{center}
\input{plsa.pst}
\end{center}
\caption{The probabilistic latent semantic analysis model of words $w$ and documents $d$ modelled as dependent on a latent variable $z$.}
\label{plsa}
\end{figure}


\begin{figure}
\begin{Verbatim}[fontsize=\scriptsize]
end some medicine for her, but she will need fruit and  milk, and some other special things that
our own. Here we give you ideas for foliage, fruit and  various festive trimmings that you can i
part II). However, other strategies can bear fruit  and are described under three sections which
       supper Ñ tomatoes, potato chips, dried fruit and cake. And  they drank water out of tea-cu
erent days, as  the East Berliners queue for fruit and cheap stereos, a Turkish  beggar sleeps i
dening; and  Pests -- how to control them on fruit and vegetables. Both are  produced by the Hen
me,"Silver Queen" is male so will never bear fruit    At the opposite end of the prickliness sca
 lifted away    Like an orange lifted from a fruit-bowl    And darkness, blacker    Than an oil-
ed in your  wreath. Christmas ribbon and wax fruit can be added for colour.  Essentials are scis
e you need to start developing your very own fruit  collection    KEEPING OUT THE COLD    Need e
ly with Jeyes fluid    THE KITCHEN GARDEN    FRUIT    Cut out cankers on fruit trees, except tho
wn and watered    AUTUMN HUES    Foliage and fruit enrich the autumn garden, whether glowing  th
- have forgotten  the maxim: " tel arbre tel fruit ". If I were  willing  to  unstitch the past 
 of three children of Alfred Roger Ackerley, fruit importer  of London, and his mistress, Janett
rful didactic spirit, much that was to  bear fruit in his years as a mature artist. Although thi
e all made with natural vegetable, plant and fruit ingredients  such as chamomile, kukai nut and
ack in the soup.    He re-visits the Copella fruit juice farm in Suffolk, the  business he told 
rategic relationship" with Lotus, the first  fruit of which is a mail gateway between Office and
, choose your plants  carefully to enjoy the fruit of your labour all year round.    PLACES TO V
 and I love chips.  Otherwise I'll nibble on fruit or something to convince myself  that I'm eat
 tone and felt the  softness and warmth of a fruit ripening against a wall? If she  had she migh
ol place to set. Calories per  slice: 395    Fruit Scones with cinnamon Butter    (makes 12)    
ought me water. Another monster gave me some fruit  to eat. A few monsters lay against my body a
ney fungus.    Cut out diseased wood on most fruit trees    VEGETABLES    Continue winter diggin
age and chafing.    Remove old, unproductive fruit trees by cutting them down to  shoulder heigh
ITCHEN GARDEN    FRUIT    Cut out cankers on fruit trees, except those on peaches, plums  and ch
ps  remain, then stir in the sugar and dried fruit. Using a round-  ended knife, stir in the mil
 of a homeland, well others dream too,    De fruit was forbidden an now yu can't chew,    How ca
onnoisseurs. We take a bite from an unusual  fruit. We come away neither nourished nor ravished,
\end{Verbatim}

\caption{Occurrences and some context of occurrences of the word \emph{fruit} in the British National Corpus.}

\end{figure}

\subsection{Measures of Distributional Similarity}

\subsection{Properties of Vectors for Meaning}



\subsection{Outstanding Issues in Existing Work}
 
 
 \section{Overview}
 
 \bibliographystyle{plainnat}
 \bibliography{contexts}
 
 
 
 \end{document}