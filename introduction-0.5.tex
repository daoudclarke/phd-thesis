%%Bismillahi-r-Rahmani-r-Rahim
%\documentclass[12pt]{report}

%\include{head}
%\usepackage{fancyvrb}
%\usepackage{pstricks}

%\begin{document}

\part{Background and Development}
\chapter{Introduction}

This thesis deals with the philosophical and theoretical foundations of computational linguistics. We are interested in the nature of meaning in natural language and the ways in which meaning can be represented computationally, in particular the relationship between vector-based representations of meaning and logical representations.

In recent years, the abundance of text corpora and computing power has allowed the development of techniques to analyse statistical properties of words. These techniques have proved useful in many areas of computational linguistics, arguably providing evidence that they capture something about the nature of words that should be included in representations of their meaning. However, it is very difficult to reconcile these techniques with existing theories of meaning in language, which revolve around logical and ontological representations. The new techniques, almost without exception, can be viewed as dealing with vector-based representations of meaning, placing meaning (at least at the word level) within the realm of mathematics and algebra; conversely the older theories of meaning dwell in the realm of logic and ontology. There thus appears to be a gulf between theory and practice. Theory says that meaning looks logical, while in practice computational linguists make use of vector-based representations of meaning.

The problem appears to be a fundamental one in computational linguistics since the whole foundation of meaning seems to be in question. The older, logical theories often subscribe to a model-theoretic philosophy of meaning \citep{Kamp:93, Blackburn:05}. According to this approach, sentences should be translated to a logical form that can be interpreted as a description of the state of the world. The new vector-based techniques, on the other hand, are often closer in spirit to the philosophy of ``meaning as context'', that the meaning of an expression is determined by how it is used. This is an old idea with origins in the philosophy of \cite{Wittgenstein:53}, who said that ``meaning just \emph{is} use'' and \cite{Firth:57}, ``You shall know a word by the company it keeps'', and the distributional hypothesis of \cite{Harris:68} which states that words with similar meanings occur in similar contexts. Whilst the two philosophies are not obviously incompatible --- especially since the former applies mainly at the sentence level and the latter mainly at the word level --- it is not clear how they relate to each other.

While the model-theoretic philosophy of meaning provides us with theories which allow a complete description of natural language from the word level to the sentence level and beyond, the same cannot be said for the philosophy of meaning as context. It is this philosophy that has inspired vector based techniques, yet there is currently no theory explaining how these vectors can be used to represent phrases and sentences. This lack of a firm theoretical foundation has far-reaching implications for computational linguists or engineers implementing systems that represent expressions using vectors.

An application where such a theory may be useful is in recognising textual entailment. This is the task of determining, given two sentences or natural language expressions (called the \emph{text} and \emph{hypothesis} sentences), whether the first entails or implies the second, for example in the case of the two sentences
\begin{itemize}
\item \emph{Text:} Once called the ``Queen of the Danube,'' Budapest has long been the focal point of the nation and a lively cultural centre.
\item \emph{Hypothesis:} Budapest was once popularly known as the ``Queen of the Danube.''
\end{itemize}
the text sentence does entail the hypothesis.

Many tasks in natural language processing, such as questioning answering, summarisation, and information retrieval would benefit from a system that can accurately determine entailment. The task has recently received a lot of attention thanks to the PASCAL Recognising Textual Entailment Challenges \citep{Dagan:05,Bar-Haim:06}, which provided a method of evaluating textual entailment systems using a large number of text-hypothesis pairs. A large proportion, 22 of the 41 entered runs, made use of corpus or web-based statistics, yet there is no linguistic theory of meaning that explains how to determine entailment between sentences using such statistics. We might be able to find vector representations for words or multi-word expressions by statistical analysis, but we are left without any guidelines about how sentences should be represented. Entailment systems making use of such statistics thus have to resort to somewhat ad-hoc methods tuned and evaluated empirically by their performance at the task. While this is fine from an engineering perspective, it leaves a lot to be desired from a linguistic perspective, since we are left without a deeper understanding of the nature of language.

\begin{figure}
\begin{center}
\begin{graph}(8,8)(0,.5)
\newcommand{\ntext}[4]{\textnode{#1}(#2,#3){#4}[\graphlinecolour{1}]}
%\newcommand{\btext}[3]{\bow{#1}{#2}{0.5}[\graphlinecolour{0}] \bowtext{#1}{#2}{0.5}{#3}}
%\ntext{Rel}{0}{8}{Existing Work}
\ntext{Req}{0}{6}{Existing Work}
\ntext{Maths}{4}{7}{Mathematics}
\ntext{Phil}{8}{8}{Philosophy}
\ntext{Model}{8}{6}{Meaning as Context}
\ntext{Alg}{4}{4}{Context-theoretic Framework}
\ntext{Build}{4}{1}{Development of Context Theories}
%\diredge{Rel}{Req}
\diredge{Phil}{Model}
\dirbow{Req}{Alg}{-0.2}
\diredge{Maths}{Alg}
\dirbow{Build}{Alg}{-0.2}
\dirbow{Model}{Alg}{0.2}
\dirbow{Alg}{Build}{-0.2}
\end{graph}
\end{center}
\caption{Method of Approach}
\label{approach}
\end{figure}

Our approach to solving these problems is as follows (see Figure \ref{approach}):
\begin{itemize}
\item We examine the philosophy of meaning as context, looking at the ideas of Wittgenstein, Firth and Harris as well as later developments to these ideas (see Section \ref{philosophy}).
\item We review statistical techniques that analyse occurrences of words in corpora to determine something about their meaning; specifically we look at latent semantic analysis and similar techniques, and measures of distributional similarity (see Section \ref{vector-based}).
\item We develop a mathematical theory of meaning as context by making use of the abstract mathematical idea of a \emph{corpus model}, and we examine the mathematical properties of such models. This theory is vital in formulating the context theoretic framework; in fact the framework can be viewed as a mathematical abstraction of the properties of the theory (see Chapter \ref{meaning-context}).
\item 


\end{itemize}


\begin{itemize}
\item In the theory, the meaning of a string is a vector representing the contexts it occurs in in the corpus model (its \emph{context vector}).
\item We look at how the context vector of a string relates to the context vectors of its substrings and are able to show that 

\end{itemize}

Our contributions towards solving these problems are as follows:
%\begin{itemize}
%\item We develop a set of \emph{requirements} for a theoretical basis for computational linguistics based on an analysis of \emph{existing work} (in Chapter 2); we will call this basis the \emph{general formalism}.
%\item We introduce the \emph{mathematics} that is of importance in understanding theory to be introduced later, but which we believe is also of general importance for computational linguists. Because we were specifically interested in representing the vector nature of meaning, it was clear that a study of mathematical formalisms was going to be of great importance to the development of the theory. A large amount of work was devoted to searching mathematical literature for areas that seemed relevant to the task at hand; the most important of these are introduced in Chapter~3.
%\item We look at the \emph{philosophy} of Wittgenstein, Firth and Harris, and from these ideas we develop a mathematical theory of \emph{meaning as context}. According to the theory, meanings of words can be represented as elements of an ``algebra over a field'', that is, a vector space with a compatible form of multiplication. The theory specifies how the notions of entailment and the probability of a sequence should be incorporated into the representation; it is motivated and described in Chapter 4.
%\item We develop a \emph{general formalism}, abstracting from the theory of meaning as context. We selected those features of the theory that were deemed important to \emph{applications}, and which allowed us to meet the \emph{requirements} developed earlier, but without putting unnecessary restrictions on the nature of the formalism. This has resulted in a very general formalism (described in Chapter 5) which allows the description of a large number of phenomena.
%\item The second part of the thesis is devoted to applications of the theory, demonstrating its generality and benefits of applying it. Applications were developed in parallel with the theory, and allowed us to determine which features were of importance to be retained in the general formalism. Specifically,
%\begin{itemize}
%\item In Chapter 6 we show how the theory applies to language models such as $n$-grams, allowing such simple descriptions of language to be used to describe entailment.
%\item In Chapter 7 we show that logical representations can be incorporated into implementations of the theory, and that doing so allows for an elegant description of lexical and syntactic ambiguity, incorporating statistical features of ambiguity into the representation.
%\item In Chapter 8 we examine possible ways of incorporating representations of syntax into the theory, showing how categorial grammar relates to the representation. We also give a description of link grammar in algebraic terms, showing that it fits our formalism naturally. We also show how the use of random matrices has the potential to enable fast statistical parsing of text.
%\item In Chapter 9 we look at ontological representations of meaning and relate them to vector-based representations. We show that the mathematical concept of \emph{vector lattice} provides a unifying framework for representing meaning.
%\item In Chapter 10 we look at ways of combining implementations to build complex vector based systems. We predict that when words combine compositionally, their representations can be decomposed into syntactic and semantic parts that combine according to \emph{free probability}, a relatively new mathematical concept that forms part of the framework of non-commutative probability.
%\end{itemize}
%\item We conclude in Chapter 11 with a summary of possible directions for future research.
%\end{itemize}
%Our main contribution --- that the notion of meaning as context leads to a representation of words as elements of an algebra over a field --- may be summarised as follows:
%\begin{quote}
%\emph{If we are to represent meanings of words as vectors which are intended to indicate the contexts the words occur in, then the correct representation for the concatenation of two words is a vector whose components are formed from products of the components of the original words, given some product (an associative binary operation) defined on vector components.}
%\end{quote}
%It is this idea that forms the core of the thesis, and our thesis may be roughly described in terms of it as follows: The first part of our thesis is devoted to motivating and explaining this idea, whilst the second part is devoted to applying it.

%\section{Approach}

%Our approach is described schematically in figure \ref{approach}; each node of the graph represents a component of our work:
%\begin{itemize}
%\item \emph{Related work.} Our formalism is influenced by a wide variety of work in computational linguistics, most importantly the vector-based techniques summarised in the next chapter.
%\item \emph{Requirements.} In order to be explicit about what we expected of the formalism under development, we came up with a set of requirements (described in Chapter 4) influenced by the related work and our own intuitions.
%\item \emph{Mathematics.} Because we were specifically interested in representing the vector nature of meaning, it was clear that a study of mathematical formalisms was going to be of great importance to the development of the theory. A large amount of work was devoted to searching mathematical literature for areas that seemed relevant to the task at hand, the most important of these are introduced in Chapter 3.
%\item \emph{Philosophy.} The theory we have developed, like the vector-based techniques themselves, has its roots in the philosophy of Wittgenstein, Firth and Harris, described in Chapter 2.
%\item \emph{Meaning as Context.} Based on the above philosophy, we developed a mathematical theory of meaning as context, based on a statistical description of text corpora. This theory was instrumental in the development of the general formalism, and is described in Chapter 4.
%\item \emph{General Formalism.} The general formalism abstracts from the theory of meaning as context, as we selected those features of the theory that were deemed important to applications, and which allowed us to meet the requirements, but without putting unnecessary restrictions on the nature of the formalism. This has resulted in a very general formalism (also described in Chapter 4) which allows the description of a large number of phenomena.
%\item \emph{Development of Applications.} Applications were developed in parallel with the theory, and allowed us to determine which features were of importance to be retained in the general formalism.
%\end{itemize}

%
% \bibliographystyle{plainnat}
% \bibliography{contexts}
% 
% 
% 
% \end{document}