 %Bismillahi-r-Rahmani-r-Rahim
 \documentclass[12pt]{report}
 
% \newenvironment{itemize2}{\vspace{0.1cm}}{\\}
 %\newcommand{\items}{\\ \vspace{0.1cm}\indent}

\input{head}

\usepackage{subfigure}
\usepackage{enumerate}

 \begin{document}
 

 \chapter{Mathematical Methods for Computational Linguistics}
% \section{Introduction}
 
The purpose of this chapter is to introduce mathematical concepts that we believe are of importance to computational linguistics. These fall into three broad categories:

\begin{itemize}
\item foundational concepts such as semigroups, groups and the notion of completeness; these form the basis of later definitions;
\item those that are already made use of in computational linguistics, such as vector spaces, partial orderings and the notions of metric, norms and inner product;
\item those that we believe will be of use in computational linguistics in the future including vector lattices and the notions of algebra over a field and non-commutative probability.
\end{itemize}
In the latter of these categories we concentrate especially on those concepts that are of importance in understanding what follows, however we believe them to be of value to computational linguistics in their own right.

A large part of our work has been to distinguish out of the vast body of research in mathematics those areas which we believe to be particularly promising for application to computational linguistics, guided by our requirements of what we would expect from a natural language algebra (discussed in the previous chapter). This chapter is thus representative of this aspect of our work, and for this reason we have placed it in this central position in the thesis, rather than relegating the relevant definitions to an appendix. In addition to this, we wish to emphasise our belief that the concepts discussed in this chapter should be of central importance to computational linguists, and that a thorough understanding of these concepts would be of benefit to them. For this reason, we try and motivate the concepts as they are introduced by discussing their importance both in existing applications and for what follows, and by the use of examples.

\section{Semigroups, Groups and Fields}

The most general mathematical structure we will consider is a semigroup. Most structures we will consider can be considered as a semigroup in at least one way. Despite having a very simple definition, the theory of semigroups is quite developed, with complex structure theorems that shall not concern us here; our main interest in semigroups is as a building block towards more complex structures.

\begin{defn}[Semigroup]
A \emph{binary operation} on a set $S$ is a function from $S\times S$ to $S$. The value of the binary operation $\cdot$ on two elements $x$ and $y$ in $S$ is denoted $x \cdot y$. A semigroup $(S,\cdot)$ is a set $S$ with a binary operation $\cdot$ which is \emph{associative}:
$$(x\cdot y)\cdot z = x \cdot (y \cdot z).$$
This product is often denoted $x\cdot y \cdot z$ or simply $xyz$.

An element $e$ of $S$ is called \emph{unity} if $es = se = s$ for all $s \in S$. There can only ever be at most one unity in $S$: if $e_1$ and $e_2$ are unities then $e_1e_2 = e_1 = e_2$. A semigroup with unity is often called a \emph{monoid}.
\end{defn}
 
 The following example demonstrates the importance of semigroups for computational linguistics: at the most basic level we can think of language as sequences of symbols; such sequences form a semigroup under concatenation.
 
 \begin{example}[Free Semigroup]
Let $A$ be a set. The set $A^*$ is the set of all finite sequences of symbols of elements of $A$. Then $A^*$ is a semigroup (called the \emph{free semigroup} on $A$) under concatenation of sequences, $x\cdot y = xy$ for $x,y \in A^*$.
 \end{example}
 
 The next most basic structure is that of a \emph{group}, which can be viewed as a special type of semigroup with unity, or monoid. The theory of groups is also highly developed; again we shall not need any of the theory in what follows.
 
 \begin{defn}[Group]
 A group $G$ is a monoid with unity $e$ such that for each element $x \in G$ there is an element $x^{-1}$, called the \emph{inverse} of $x$, such that $xx^{-1} = x^{-1}x = e$. A group is called \emph{abelian} or \emph{commutative} if $xy = yx$ for all $x,y\in G$.
 \end{defn}
 
 The most familiar examples of a group are the integers or the real numbers under addition.
 \begin{example}[Addition of Real Numbers]
 Consider the set $\R$ of real numbers. $\R$ forms an abelian group with group operation $+$ and unity $0$; the inverse of $x$ is given by $-x$.
 \end{example}
 This is also an important example for us because it is closely related to the definition of a vector space, since vector spaces are also groups under addition. The real numbers in fact have more structure when multiplication is considered: they form a \emph{field}.
 \begin{defn}[Field]
 A field is a set $F$ together with two operations $+$ and $\cdot$ called addition and multiplication such that $F$ is an abelian group under addition with (additive) identity $0 \in F$, and a commutative monoid under multiplication, with (multiplicative) identity $1 \in F$, with $1 \neq 0$, such that every element $x \in F$ except $0$ has a multiplicative inverse $x^{-1}$ (that is, $F - \{0\}$ is an abelian group under multiplication) and multiplication distributes over addition:
$$x\cdot(y + z) = x\cdot y + x \cdot z$$
\end{defn}
Fields are important for us since they are used in the definition of a vector space. Other than the real numbers, the most familiar example of a field is given by the complex numbers.
 
\section{Vector Spaces}
\label{vectors}

In this section, we define a series of concepts relating to different types of vector space. The most important is the immediately following general definition. The remainder of this section defines concepts that are of varying importance in different parts of the thesis.

It is our view that a definition in itself is of little value on its own, and that in trying to understand them time is much better spent getting a feel for concepts by examining examples; we try where possible therefore to introduce these following definitions.

\begin{defn}[Vector Space]
A vector space over a field $F$ is a set $V$ with two operations: addition, $V \times V \rightarrow V$, denoted $u + v$ where $u,v \in V$, and scalar multiplication: $F \times V \rightarrow V$, denoted $\alpha v$ where $\alpha \in F$ and $v \in V$,
satisfying the following conditions:
\begin{itemize}
\item $V$ is closed under addition and scalar multiplication;
\item the vector space under addition forms an \emph{abelian group}: addition is associative and commutative and there is an additive identity $0 \in V$ such that for every element $v \in V$ there is an element $-v$ such that $v + (-v) = 0$;
\item scalar multiplication is associative: $\alpha (\beta v) = (\alpha \beta) v$ for $\alpha, \beta \in F$ and $v \in V$;
\item $1v = v$ where $1$ is the multiplicative identity of $F$;
\item scalar multiplication is distributive with respect to vector and scalar addition:
\begin{eqnarray*}
\alpha(u + v) & = & \alpha u + \alpha v\\
(\alpha + \beta)v & = & \alpha v + \beta v
\end{eqnarray*}
\end{itemize}
When the field $F$ is that of the complex numbers $\mathbb{C}$, the vector space is called `complex'.
\end{defn}

\begin{example}[Finite-dimensional Real Vector Spaces]
The most important examples for computational linguists are the $n$-dimensional real vector spaces, denoted $\R^n$. An element of $\R^n$ is denoted
$$x = (x_1,x_2,\ldots x_n),$$
where the $x_i$ are the real valued \emph{components} of $x$. The operations on $\R^n$ are defined as follows:
\begin{eqnarray*}
x + y & = & (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n)\\
\alpha x & = & (\alpha x_1, \alpha x_2, \ldots, \alpha x_n)\\
0 & = & (0,0,\ldots, 0)\\
-x & = & (-x_1, -x_2, \ldots, -x_n)
\end{eqnarray*}
Given a finite set $S$, we write $\R^S$ for the vector space $\R^{|S|}$; then each element of $S$ corresponds to a dimension in $\R^S$.
\end{example}

\subsection{Notions of Distance}

The following sequence of definitions are to do with the notion of ``distance'' and ``size'' of objects. These concepts are of key importance in computational linguistics because we are often interested in ``distances'' between words---for example semantic distance. The types of space, in order of generality, are \emph{metric space}, \emph{normed space} and \emph{inner product space}.
\begin{defn}[Metric]
A metric $d$ is a function on a set $X$ satisfying:
\begin{center}
\begin{tabular}{ll}
$d(x,y) \ge 0$ & (non-negativity)\\
$d(x,y) = 0$ if and only if $x=y$ & (identity of indiscernibles)\\
$d(x,y) = d(y,x)$ & (symmetry)\\
$d(x,z) \le d(x,y) + d(y,z)$ & (triangle inequality)
\end{tabular}
\end{center}
for all $x,y,z \in X$. A metric space is a set $X$ together with a metric $d$.
\end{defn}

The definition of metric is very general: it does not require the set $X$ to be a vector space. In contrast, a more common way of defining distances on a vector space is via a \emph{norm}:
\begin{defn}[Norm]
If $V$ is a vector space over a field $F$ which is a subfield of the complex numbers, a norm $\|\cdot\|$ is a function from $V$ to the real numbers satisfying:
\begin{center}
\begin{tabular}{ll}
$\|x\| \ge 0$ & (positivity)\\
$\|\alpha x\| = |\alpha|\cdot\|x\|$ & (positive scalability)\\
$\|x + y\| \le \|x\| + \|y\|$ & (triangle inequality)\\
$\|x\| = 0$ if and only if $x = 0$ & (positive definiteness)
\end{tabular}
\end{center}
A \emph{normed vector space} is a vector space together with a norm.
\end{defn}
It is fairly straightforward to see that a norm $\|\cdot\|$ on a vector space $V$ defines a metric $d$ on $V$ by $d(x,y) = \|x - y\|$.

\begin{example}[$l^p$ Norms]
The most important examples are given by the $l^p$ norms, for $p$ a real number $\ge 1$. For the vector space $\R^n$, the $l^p$ norm of an element $x = (x_1,x_2,\ldots,x_n)$ is given by
$$\|x\|_p = \left(|x_1|^p + |x_2|^p + \ldots + |x_n|^p\right)^{1/p}$$
The $l^\infty$ norm of $x$ is defined as the supremum of $|x_i|$ over all components $x_i$ of $x$.
\end{example}

Some of the most important instances of vector spaces, namely the Hilbert spaces, are those with an \emph{inner product}, which corresponds to the familiar dot product on finite dimensional vector spaces. We give the definition here in terms of complex numbers for generality; we shall only ever need real vector spaces.
\begin{defn}[Inner Product]
An inner product on a complex vector space is a function $\langle \cdot ,\cdot \rangle : V \times V \rightarrow \mathbb{C}$ satisfying for all $u,v,w \in V$ and $\alpha \in F$:
\begin{center}
\begin{tabular}{lc}
Additivity: &
$\inprod{u}{v+w}  =  \inprod{u}{v} + \inprod{u}{w}$\\
\vspace{0.1cm}
&$\inprod{u+v}{w}  =  \inprod{u}{w} + \inprod{v}{w}$\\
\vspace{0.1cm}
Nonnegativity: &
$\inprod{v}{v} \ge 0$\\
\vspace{0.1cm}
Nondegeneracy: &
$\inprod{v}{v} = 0\quad \textrm{iff}\quad  v = 0$\\
\vspace{0.1cm}
Conjugate symmetry: &
$\inprod{u}{v} = \overline{\inprod{v}{u}}$\\
\vspace{0.1cm}
Sesquilinearity: &
$ \inprod{u}{\alpha v} = \alpha\inprod{u}{v}$
%& $ \inprod{u}{v + w} = \inprod{u}{v} + \inprod{u}{w}$
\end{tabular}
\end{center}
where $\overline{\alpha}$ denotes the complex conjugate of $\alpha$. The definition clearly also holds when $V$ is a real vector space. A vector space with an inner product defined is called an \emph{inner product space}.

Note that conjugate symmetry implies that $\inprod{x}{x}$ is real for all $x$, and that conjugate symmetry and sesquilinearity together imply that
$$\inprod{\alpha x}{y} = \overline{\alpha}\inprod{x}{y}.$$
An inner product naturally defines a norm $\|\cdot\|$ on a vector space, by $\|x\| = \sqrt{\inprod{x}{x}}$.
\end{defn}

\begin{example}[Dot Product]
The inner product or dot product on $\R^n$ is defined by
$$\inprod{x}{y}  = \sum_{i = 1\ldots n} x_iy_i.$$
%The complex valued functions on a real interval $[a,b]$ is an inner product space (it is a vector space: for example addition is defined by $(f + g)(x) = f(x) + g(x)$), with the inner product
%$$\inprod{f}{g} = \int_a^b \overline{f(x)}g(x) \mathrm{d}x$$
The norm of a vector in $\R^n$ corresponds to its length: $\|x\| = \sqrt{\sum_{i=1\ldots n} x_i^2}$.
\end{example}

\subsection{Bases}

Almost every vector space considered in computational linguistics comes with some basis, which can usually be conceptually linked to the notion of context. The notion of a \emph{basis} in a vector space is also very important in relation to \emph{vector lattices} (see section \ref{vector-lattices}).

\begin{defn}[Basis]
A basis is a set $B$ of elements of a vector space $V$ over a field $F$, such that the elements are \emph{independant}, i.e., if
$$\sum_{b_i \in B} \alpha_i b_i = 0$$
for some set of $\alpha_i \in F$, then necessarily $\alpha_i = 0$ for all $i$; and $B$ \emph{spans} $V$, i.e., for each element $x \in V$,
$$x = \sum_{b_i \in B} \beta_i b_i$$
for some set of values $\beta_i \in F$.

Two elements $x,y$ in an inner product space $V$ are called \emph{orthogonal} if $\inprod{x}{y} = 0$. An orthonormal basis for $V$ is a basis $B$ such that any two distinct elements of $B$ are orthogonal and the magnitude of each element in $B$ is 1, i.e. $\inprod{b}{b} = 1$ for all $b\in B$.
\end{defn}

\begin{example}[Orthonormal Basis for $\R^n$]
An orthonormal basis for the vector space $\R^n$ is given by the set $\{e_1,e_2,\ldots e_n\}$ where $e_1 = (1,0,0,\dots 0)$, $e_2 = (0,1,0,\dots 0)$, \ldots, $e_n = (0,0,0,\dots 1)$. In this way, for the vector space $\R^S$, we can associate a basis element $e_s$ with each element $s \in S$.
\end{example}

\subsection{Completeness}

Completeness is a property of vector spaces which is difficult to grasp conceptually, and is not that important to understand in relation to applications in computational linguistics. However, it is a property that is possessed by a lot of interesting vector spaces, and is often required of vector spaces since it leads to things being mathematically very ``well behaved''.

\begin{defn}[Limit]
Let $a_1,a_2\ldots$ be an infinite sequence of real numbers. A real number $a$ is said to be the limit of the sequence if and only if for every real number $\epsilon > 0$, there is a natural number $n_0$ such that for all $n > n_0$, $|a_n - a| < \epsilon$.
\end{defn}

\begin{defn}[Completeness]
Given a metric space $X$ with metric function $d$, a sequence $x_1, x_2, \ldots$ is called \emph{Cauchy} if for every positive real number $a$, there is an integer $n_0$ such that for all integers $m,n > n_0$, $d(x_m,x_n) < a$. If every Cauchy sequence has a limit in $X$, the metric space is called \emph{complete}.

A \emph{Banach space} is a normed vector space which is complete with respect to the metric $d$ defined by $d(x,y) = \|x - y\|$. A \emph{Hilbert space} is a vector space with an inner product which is complete with respect to the metric defined by the inner product norm, $d(x,y) = \sqrt{\inprod{x-y}{x-y}}$. A Hilbert space is thus a special kind of Banach space.
\end{defn}

\begin{example}[$l^p$ Spaces]
We shall often need to deal with infinite dimensional vector spaces, for example, we shall often want to associate a dimension with each sequence in a set of sequences $A^*$. When we do this, not all vectors will have finite norm, and precisely which ones do depends on which norm we use. We can thus categorise subspaces according to which norms are guaranteed to be finite. For $p \ge 1$ we define the $l^p$ space to be the vector space of all infinite sequences $x$ of real numbers $x = (x_1,x_2,\ldots)$ such that $\sum_i |x_i|^p$ is finite, together with the $l^p$ norm. The $l^\infty$ space is the set of all vectors with finite components, together with the $l^\infty$ norm.
All the $l^p$ spaces are Banach spaces, and only the $l^2$ space is a Hilbert space.
\end{example}

\subsection{New vector spaces from old}
\begin{defn}[Direct Sum]
Given two vector spaces $U$ and $V$ we can construct a vector space $U \oplus V$ called the \emph{direct sum} of $U$ and $V$. The direct sum is simply the cartesian product $U \times V$ with vector operations defined component-wise:
\begin{eqnarray*}
(u_1,v_1) + (u_2,v_2) & = & (u_1+u_2,v_1+v_2)\\
\alpha(u,v) & = & (\alpha u, \alpha v)
\end{eqnarray*}
where $u_i \in U, v_i \in V, \alpha \in F$. If $U$ and $V$ are Hilbert spaces, then $U \oplus V$ denotes the Hilbert space with the inner product defined by
$$\inprod{(u_1,v_1)}{(u_2,v_2)} = \inprod{u_1}{u_2} + \inprod{v_1}{v_2}$$
The dimension of $U\oplus V$ is equal to the sum of the dimensions of $U$ and $V$.
\end{defn}
\begin{defn}[Tensor Product]

The \emph{tensor product} $U \otimes V$ of two vector spaces $U$ and $V$ is constructed by taking the vector space generated by the cartesian product $U \times V$ and factoring out the subspace generated by the equations:
\begin{eqnarray*}
(u_1 + u_2) \otimes v & = & u_1 \otimes v + u_2 \otimes v\\
u \otimes (v_1 + v_2) & = & u \otimes v_1 + u \otimes v_2\\
\alpha u\otimes v & =  &u \otimes \alpha v = \alpha(u \otimes v) 
\end{eqnarray*}
where $u_i,u \in U$, $v_i,v \in V$ and $\alpha \in F$.

If $U$ and $V$ are Hilbert spaces, the tensor product is again a Hilbert space, with inner product defined by
$$\inprod{u_1\otimes v_1}{u_2 \otimes v_2} = \inprod{u_1}{u_2}\inprod{v_1}{v_2}.$$
The dimension of $U \otimes V$ is equal to the product of the dimensions of $U$ and $V$.
\end{defn}
%\begin{defn}[Fock Space]
%\index{Fock space|textbf}
%Let $H$ be a finite dimensional complex Hilbert space and $\Omega$ a distinguished vector in $H$ with norm 1. The Fock space $\mathcal{F}$ of $H$ is then defined as
%$$\mathcal{F} = \mathbb{C}\Omega \oplus H \oplus (H \otimes H) \oplus (H \otimes H \otimes H) \oplus \cdots$$
%i.e.~it is the direct sum of all finite tensor product powers of $H$, where $\mathbb{C}\Omega$ is a one dimensional Hilbert space which is viewed as the zeroth power of $H$.
%\end{defn}

\subsection{Linear Operators}
\label{operators}

The linear operators on a vector space have proven to be of extreme importance in many applications. For example in imaging, scaling and rotation are represented as linear operators, while in quantum mechanics, an observable may be represented as a linear operator on a vector space with certain characteristics. Linear operators are defined as follows:
\begin{defn}[Linear Operator]
A linear operator from a vector space $U$ to a vector space $V$ both over a field $F$ is a function $A$ from $U$ to $V$ satisfying
$$A(\alpha x + \beta y) = \alpha Ax + \beta Ay$$
for all $x,y \in U$ and $\alpha, \beta \in F$.
\end{defn}
Note that the operation of $A$ on an element $x$ is denoted simply $Ax$ (without brackets). In addition, we shall often refer to a linear operator simply as an ``operator''---in this case linearity is assumed.

Linear operators themselves form a vector space, with vector space operations defined by
\begin{eqnarray*}
(A + B)x&=&Ax + Bx\\
(\alpha A)x&=&\alpha Ax\\
0x&=&0
\end{eqnarray*}

\subsection{Algebras}

Because operators themselves form a vector space, it is sometimes useful to abstract away from operators by observing their properties. The resulting abstract mathematical definition is called an \emph{algebra over a field} or simply ``an algebra''.
\begin{defn}
An algebra is a vector space $A$ over a field $K$ together with a binary operation $(a,b)\mapsto ab$ on $A$ that is bilinear, i.e.
\begin{align*}
a(\alpha b + \beta c) &= \alpha ab + \beta ac\\
(\alpha a+\beta b)c &= \alpha ac + \beta bc
\end{align*}
and associative, i.e. $(ab)c = a(bc)$ for all $a,b,c\in A$ and all $\alpha,\beta \in K$.
\end{defn}

\section{Lattice Theory}
\label{lattices}

The concepts described in this section deal with relationships between objects. One of the most important types of relationship that we consider on sets of objects is that of a \emph{partial ordering}. An example of this is the hypernymy relation between words (or equivalently the \textbf{is-a} or subsumption relation between concepts), discussed in section \ref{taxonomy}. Another example is the subset relation on a set of sets.

These relations often satisfy much stronger conditions, which we classify in sequence: semilattices, lattices, modular lattices, distributive lattices and Boolean algebras. All of these have important characteristics which may also be expressed in algebraic terms.

%\begin{example}
%A \emph{directed graph} is a set $V$ of ``vertices'' or ``nodes'' together with a relation $E$ on $V$. The set $E$ describes ``edges'' between nodes. For example, the graph $V = \{a,b,c\}$ and $E$
%\end{example}

\begin{defn}[Partial Ordering]
\index{partial ordering|textbf}
A partial ordering on a set $S$ is a relation $\le$ that satisfies, for all $x,y,z \in S$:
%\begin{itemizes}
%\items \emph{reflexive}: $x \le x$,
%\items \emph{antisymmetric}: if $x \le y$ and $y \le x$ then $x = y$, and
%\items \emph{transitive}: if $x \le y$ and $y \le z$ then $x \le z$.
%\end{itemizes}
\begin{center}
\begin{tabular}{ll}
$x \le x$ & (reflexivity)\\
if $x \le y$ and $y \le x$ then $x = y$ & (antisymmetry) \\
if $x \le y$ and $y \le z$ then $x \le z$ & (transitivity)
\end{tabular}
\end{center}
\end{defn}
If $a \le b$ then we say $a$ is \emph{contained in} or \emph{is less than} $b$. An example of a partial ordering is the set inclusion relation, $\subseteq$ on a set of subsets of a set, or the `less than or equal' relation on the natural numbers.

The following definition is useful for describing properties of partial orderings, and drawing diagrams of them:
\begin{defn}[Preceding elements]
    Write $x < y$ if $x \le y$ and $x \neq y$ in $L$. We say that $x$ 
    \emph{precedes} $y$ and write $x \prec y$ if $x < y$ and there is no 
    element $z$ such that $x < z < y$.
\end{defn}

Partial orderings are often depicted using \emph{Hasse diagrams}. Some examples are shown in figure \ref{hasse}. Elements of the lattice are shown as nodes, while the relation $\prec$ between elements is shown by connecting nodes with an edge, such that the lesser element is below the greater element in the diagram. For example, figure \ref{hasse:notlattice} shows a four element set with a partial ordering which may be described by the relation $\le$ on the set $\{a,b,c,d\}$ defined by $a \le c$, $b \le c$, $a \le d$, $b\le d$. Hasse diagrams such as these are used to show partial orderings \emph{up to isomorphism}, that is, when we are not interested in the labeling of the nodes, only the nature of the partial order itself. 
\begin{figure}
\begin{center}

%\input{pentagon.tex}
\subfigure[A partial ordering that is not a lattice]{
	\label{hasse:notlattice}
	\begin{graph}(4,3.5)(-0.25,-3.5)
	\graphnodesize{0.15}
	\roundnode{x1}(1,-1)
	\roundnode{x2}(2.5,-1)
	\roundnode{x3}(1,-2.5)
	\roundnode{x4}(2.5,-2.5)
	\edge{x1}{x3}
	\edge{x1}{x4}
	\edge{x2}{x3}
	\edge{x2}{x4}
	\end{graph}
}
\hfill
\subfigure[An embedding of the partial ordering in a lattice]{
	\label{hasse:lattice}
	\begin{graph}(4,3.5)(-0.25,-3.5)
	\graphnodesize{0.15}
	\roundnode{x1}(1,-1)
	\roundnode{x2}(2.5,-1)
	\roundnode{x3}(1,-2.5)
	\roundnode{x4}(2.5,-2.5)
	\roundnode{x5}(1.75,-1.75)
	\edge{x1}{x5}
	\edge{x2}{x5}
	\edge{x5}{x3}
	\edge{x5}{x4}
	\end{graph}
}
\hfill
\subfigure[The five element non-modular lattice]{
	\label{hasse:notmodular}
	\begin{graph}(4,3.5)(0,-3.25)
	\graphnodesize{0.15}
	%Nodes:
	\roundnode{x1}(2,-0.25)
	\roundnode{x2}(1,-1)
	\roundnode{x3}(1,-2)
	\roundnode{x4}(3,-1.5)
	\roundnode{x5}(2,-2.75)
	%Edges:
	\edge{x1}{x2} \edge{x2}{x3} \edge{x3}{x5}
	\edge{x1}{x4} \edge{x4}{x5}
	\end{graph}}
	%\caption{Hasse diagrams.}}
	%\label{hasse}}
\hfill
\caption{Hasse diagrams}
\label{hasse}
\end{center}
\end{figure}

\begin{defn}[Semilattice and Lattice]
An \emph{upper bound} of a subset $T$ of a partially ordered set $S$ is an element $s$ such that $t \le s$ for all $t \in T$. The \emph{least upper bound} of $T$ if it exists (also called \emph{supremum} or \emph{join}) is the upper bound which contains every upper bound. The join of a set $T$ is denoted $\bigvee T$, or if $T$ consists of two elements $x$ and $y$ their join is denoted $x \lor y$.

Similarly a \emph{lower bound} of $T$ is an element $s'$ such that $s' \le t$ for all $t \in T$. The \emph{greatest lower bound} if it exists (also called the \emph{infimum} or \emph{meet} of $T$) is the lower bound which is contained in every other lower bound. The meet of $T$ is denoted $\bigwedge T$; the meet of two elements $x$ and $y$ is denoted $x\land y$.

\index{lattice|textbf}
A \emph{meet semilattice} (or simply \emph{semilattice}) is a partially ordered set in which every pair of elements has a greatest lower bound. Similarly, a \emph{join semilattice} is a partially ordered set in which every pair of elements has a greatest lower bound.

A \emph{lattice} is a partially ordered set in which any two elements have a least upper bound and a greatest lower bound; a lattice is thus both a join and a meet semilattice. A lattice is called \emph{complete} if every subset of $S$ has a least upper bound and greatest lower bound; all finite lattices are complete.
\end{defn}

Figure \ref{hasse:notlattice} shows a partial ordering that is \emph{not} a lattice: the join of the two lesser elements is not well defined, similarly, the meet of the two greater elements is not defined. Figure \ref{hasse:lattice} does show a lattice: the new element acts as the missing join and meet.

A semilattice can be characterised as a semigroup $S$ with the binary operation $\land$ satisfying \emph{idempotence} and \emph{commutativity}:
\begin{eqnarray*}
x \land x & = & x\\
x \land y & = & y \land x
\end{eqnarray*}
respectively. The partial ordering can be recovered by defining $x \le y$ iff $x\land y = x$. Similarly, a lattice can be characterised as a set $S$ together with two operations $\land$ and $\lor$ such that $(S,\land)$ and $(S,\lor)$ are semilattices (according to the above characterisation), satisfying the \emph{absorption} laws:
\begin{eqnarray*}
x \lor (x \land y) & = & x\\
x \land (x \lor y) & = & x
\end{eqnarray*}

\begin{defn}[Modularity]
\index{lattice!modular|textbf}
A modular lattice is a lattice $L$ satisfying the \emph{modular identity}: if $x \le z$ then
$$x \lor (y \land z) = (x \lor y) \land z,$$
for all $x,y,z \in L$.
\end{defn}

Figure \ref{hasse:lattice} shows a five element modular lattice, while \ref{hasse:notmodular} shows a lattice that is not modular; it is the only five element non-modular lattice (up to isomorphism).

The proof of the following proposition is in \cite{Birkhoff:48}:
\begin{prop}
The modular lattices are those which do not have the five element non-modular lattice of figure \ref{hasse:notmodular} as a sub-lattice.
\end{prop}

\begin{defn}[Distributivity, Complement and Boolean Algebra]
\index{lattice!distributive|textbf}
A lattice is called \emph{distributive} if it satisfies
\begin{eqnarray*}
x\lor(y\land z) & = & (x\lor y)\land(x\lor z)\\
x\land(y\lor z) & = & (x\land y)\lor(x\land z)
\end{eqnarray*}
A lattice is \emph{complemented} if for every element $a$ there is an element $a'$ such that $a \lor a' = 1$ and $a \land a' = 0$. A complemented distributive lattice is called a \emph{Boolean algebra}.
\end{defn}

\subsection{Functions between partial orders}

It is very important for our work to characterise the nature of functions between partial orderings. Of special importance are those that preserve the partial ordering, and in the case of lattices, preserve meets and joins. We define some important types of functions, and give examples.

\begin{defn}[Order Embeddings]
A function $f$ from one partially ordered set $S$ to another $T$ is called \emph{monotone} or \emph{order-preserving} if $a \le b$ in $S$ implies $f(a) \le f(b)$ in $T$. Conversely, if $f(a) \le f(b)$ implies $a \le b$ then $f$ is called \emph{order-reflecting}. An \emph{order embedding} is a function that is both order-preserving and order-reflecting. A \emph{completion} of a partially ordered set $S$ is an order embedding of $S$ into a complete lattice.
\end{defn}

\begin{defn}[Lattice Homomorphisms]
If $S$ and $T$ are semilattices, a function $f$ from $S$ to $T$ is a \emph{semilattice homomorphism} if $f(a \land b) = f(a) \land f(b)$ (where $\land$ can represent the meet or the join operation). If $S$ and $T$ are lattices, a \emph{lattice homomorphism} is a function that is both a meet semilattice and join semilattice homomorphism, i.e.~$f(a \land b) = f(a) \land f(b)$, and $f(a \lor b) = f(a) \lor f(b)$. A \emph{lattice isomorphism} is a bijective lattice homomorphism, i.e.~for each element $b$ in $T$ there is exactly one element $a$ in $S$ such that $f(a) = b$. If a lattice isomorphism exists between two lattices they are said to be \emph{isomorphic}.
\end{defn}

Often we may be dealing with partial orders but require something with more structure than that relation provides. For example, we may like to be able to define meets and joins to make the partial ordering into a lattice. The concepts of \emph{principal ideals} and their duals, \emph{principal filters}, allow us to do this:

\begin{defn}[Ideals and Filters]
A \emph{lower set} in a partially ordered set $S$ is a set $T$ such that for all $x,y \in T$, if $y \le x$ then $y \in T$. Similarly, an upper set in $S$ is a set $T'$ such that for all $x,y \in T'$, if $y \ge x$ then $y \in T$.

The \emph{principal ideal generated by an element $x$} in a partially ordered set $S$ is defined to be the lower set $\down{x} = \{y \in S : y \le x\}$. Similarly, the \emph{principal filter generated by $x$} is the upper set $\up{x} = \{y \in S : y \ge x\}$.
\end{defn}

\begin{prop}[Ideal Completion]
If $S$ is a partially ordered set, then $\down{\cdot}$ can be considered as a function from $S$ to the powerset $2^S$. Under the partial ordering defined by set inclusion, the set of lower sets form a complete lattice, and $\down{\cdot}$ is a completion of $S$, the \emph{ideal completion}. Similarly, the function $\up{\cdot}$ is the \emph{filter completion} of $S$: it is an embedding into the complete lattice of upper sets, again ordered by inclusion.
\end{prop}

%Dedekind completion?

\section{Riesz Spaces and Positive Operators}
 \label{vector-lattices}
 
The previous sections have described formalisms commonly used to describe meaning: broadly speaking, that of vector spaces and that of lattices. Until now, little attention within computational linguistics has been paid to how to combine these two areas. There is a large body of research within mathematical analysis into an area which merges the two formalisms: the study of \emph{partially ordered vector spaces}, \emph{vector lattices} (or \emph{Riesz spaces}), and \emph{Banach lattices}, and a special class of operators on these spaces called \emph{positive operators}.

It is our belief that this area provides exciting new opportunities for combining the new, vector based representations, with old, ontological representations of meaning, and also provides a means of defining entailment between vector based representations.

The definitions and propositions of this section can be found in \cite{Abramovich:02} and \cite{Aliprantis:85}.

\begin{defn}[Partially ordered vector space]
A partially ordered vector space $V$ is a real vector space together with a partial ordering $\le$ such that:
\vspace{0.1cm}\\
\indent if $x \le y$ then $x + z \le y + z$\\
\indent if $x \le y$ then $\alpha x \le \alpha y$
\vspace{0.1cm}\\
for all $x,y,z \in V$. Such a partial ordering is called a \emph{vector space order} on $V$. If $\le$ defines a lattice on $V$ then the space is called a \emph{vector lattice} or \emph{Riesz space}.

A vector $x$ in $V$ is called \emph{positive} if $x \ge 0$. The \emph{positive cone} of a partially ordered vector space $V$ is the set $V^+ = \{x \in V : x \ge 0\}$
\end{defn}

The positive cone has the following properties:
\begin{eqnarray*}
X^+ + X^+ \subseteq X^+\\
\alpha X^+ \subseteq X^+\\
X^+ \cap (-X^+) = \{0\}
\end{eqnarray*}
Any subset $C$ of $V$ satisfying the above three properties is called a \emph{cone} of $V$.

\begin{prop}
If $C$ is a cone in a real vector space $V$, then the relation $\le$ defined by $x \le y$ iff $y - x \in C$ is a vector space order on $V$, with $X^+ = C$.
\end{prop}

Part of our thesis is that meanings can be represented as positive elements of a vector space. If this is the case, then we can view words as operating on these meanings. Operators which map positive elements to positive elements are called \emph{positive}; there is a large body of work studying such operators.

This idea leads to some useful definitions of particular positive elements of a vector lattice corresponding to an arbitrary element $x$. The \emph{positive part} of $x$ is denoted $x^+$ and is defined by $x^+ = x \lor 0$. Similarly the \emph{negative part} is $x^- = (-x)\lor 0$, and the \emph{absolute value} is $|x| = x \lor -x$. There are a number of useful identities concerning these definitions:
\begin{prop}
The following identities hold for elements $x,y$ in a vector lattice:
\begin{enumerate}[\indent(a).]
\item $x = x^+ - x^-$
\item $|x| = x^+ + x^-$
\item $x\land y = \frac{1}{2}(x + y - |x - y|)$
\item $x\lor y =  \frac{1}{2}(x + y + |x - y|)$
\end{enumerate}
\end{prop}

\begin{defn}[Positive Operators]
An operator $A$ on a vector space $V$ is called \emph{positive} if $x \ge 0$ implies $Ax \ge 0$. It is called \emph{regular} if it can be denoted as the difference between two positive operators.
\end{defn}


Surprisingly, the set of regular operators on a vector lattice themselves form a vector lattice:
\begin{prop}[Riesz-Kantarovi\v{c}]
The positive cone defines a vector space order on the vector space of operators on $V$. This order makes the space of regular operators a vector lattice. Specifically the meet and join of two regular operators $A$ and $B$ are given by
\begin{eqnarray*}
(A \land B)x & = & \inf\{Ay + Bz : y,z \in V^+ \text{and } y + z = x\}\\
(A \lor B)x & = & \sup\{Ay + Bz : y,z \in V^+ \text{and } y + z = x\}.
\end{eqnarray*}
\end{prop}

\begin{defn}[Lattice Homomorphism]
A positive operator $A$ between two vector lattices is called \emph{lattice homomorphism} if $A(x \lor y) = Ax \lor Ay$. An lattice homomorphism that is a one-to-one function is called a \emph{lattice isomorphism}.
\end{defn}

The following proposition shows the importance of lattice homomorphisms:
\begin{prop}
For a positive operator $A$ between two Riesz spaces $U$ and $V$, the following statements are equivalent:
\begin{enumerate}[\indent(a).]
\item $A$ is a lattice homomorphism.
\item $A(x^+) = (Ax)^+$ for each $x \in U$.
\item $A(x \land y) = Ax \land Ay$ for all $x,y \in U$.
\item $|Ax| = A|x|$ for each $x \in U$.
\item $x\land y = 0$ in $U$ implies $Ax \land Ay = 0$ in $V$. 
\end{enumerate}
\end{prop}


 \bibliographystyle{plainnat}
 \bibliography{contexts}
 
 \end{document}