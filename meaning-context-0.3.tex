%Bismillahi-r-Rahmani-r-Rahim
\documentclass[12pt]{report}

 \newcommand{\Cont}{\mathrm{Cont}}
\include{head}

\begin{document}

\chapter{Meaning as Context}

%\abstract{We examine the consequences of treating meaning purely as context. Specifically, we assume some (possibly infinite) ``corpus'' and define the meaning of a string of words in terms of their occurrence in this corpus. We use this definition to define an entailment relation on strings, and show that the relevant properties of the resulting formalism can be incorporated into \emph{positive operators} on some vector space.}

\section{Introduction}

What is meaning? The idea that sentences can be translated into some ``logical form'' that somehow describes the meaning of the sentence does not square with modern methods in computational linguistics. Methods such as latent semantic analysis and distributional similarity treat words as though they can be represented as vectors; these have proven useful in many applications. Such methods rely on examining the context that words occur in in order to determine something about their meaning. However, confusion arises when we try and make use of these representations. If we wish to maintain a vector representation, there appears to be no standard way in which representations of words can be combined to build representations of phrases and sentences.

Our approach to this problem is to formalise the notion of ``meaning as context''. The idea is that a string can be represented purely by \emph{the contexts in which it occurs in some large corpus}. The construction is theoretical, as we even allow the corpus to be infinite, but it gives us insight into how to combine vector representations of meanings of words.

\section{Definition of Corpus}

In order to arrive at our definition, we will make a sequence of statements explaining our assumptions and reasoning in arriving at the definition.
\begin{enumerate}
\item \emph{Knowing the ``meaning'' of an expression, for our purposes, means knowing the relationship between that expression and all other expressions.} Thus for example, if we know the meaning of two expressions, we should know if one entails the other or if they are contradictory, but we do not assume that we are able to connect them to objects or events in the real world. We argue that this relative interpretation of meaning is entirely sufficient for computational applications, since if further information is required this can be supplied, for example by attaching an image of a dog to the word ``dog'', and so on.
\item \emph{The language in question can be written using a finite number of symbols. These symbols can be concatenated to eventually form \emph{documents}.}
\item \emph{Everything that is of interest to us about a language can be deduced from a set of documents written in that language.} We call such a set of documents a \emph{corpus}. Thus for example, we assume that all aspects of the meanings of expressions, rules of syntax and morphology can be determined by examining the corpus without prior knowledge of the language. Such a corpus may consist of an infinite set of documents, indeed this is likely to be the case if it describes everything about the language.
\item \emph{The order of documents in a corpus is unimportant.} While the documents in a real world corpus may be ordered by date, subject or even lexicographically, we assume that the nature of the corpus is such that the meaning of a document is unaffected by any other document being read before or after it. Stated another way, meanings of documents are ``commutative'': the meaning of document $a$ concatenated with document $b$ is the same as the meaning of document $b$ followed by document $a$.
\item \emph{Finite (real-world) corpora are random samples of the infinite (hypothetical) corpus.} It is reasonable to assume that most real world corpora can be extended indefinitely given the ever increasing abundance of written knowledge. The question then arises how we are to view the finite corpora with respect to our hypothetical infinite corpus. Since the documents are not chosen methodically from the infinite corpus (for example by choosing them in lexicographic order) we describe our lack of knowledge about how they are chosen by saying they are chosen at random.

\item \emph{There is an inherent bias towards the selection of certain documents in the corpus.} While we may wish to assume that each document in the infinite corpus is equally likely to be selected for the finite corpus, this is not in fact possible since there is no uniform distribution over a countably infinite set.\footnote{If the elements in such a set are assigned equal values, the sum of these values will either be 0 or $\pm\infty$ and so cannot form a probability distribution.} The finite corpus (sample) must therefore be associated with a method of choosing documents from the infinite corpus which biases certain documents over others. That is, there must be some probability distribution over the documents in the infinite corpus by which it was sampled to produce the finite corpus. There are at least two interpretations that can be given to this probability distribution:
\begin{itemize}
\item the probability distribution is a feature of the particular method of sampling from the infinite corpus;
\item the probability distribution is an inherent feature of the corpus and indicates that certain documents have more prominence or importance than others.
\end{itemize}
According to the first of these, the infinite  corpus is just a set, and the probability distribution is just a feature of the sampling technique; according to the second, a corpus is inherently a probabilistic object and should be represented as a probability distribution over documents.

Alternatively a combination of the two hypotheses is possible: the hypothetical infinite corpus may prefer certain documents over others, and the sampling technique may then bias the sample in unexpected ways. It is this explanation that we prefer: in our view it is reasonable to suppose that some documents should naturally be given more prominence in the hypothetical corpus, but that real world corpora are rarely truly and comprehensively a representative sample of the language in question and hence the hypothetical corpus. 

Thus we believe that there is a bias towards the selection of certain documents that is inherent in the nature of the documents and is representative of their importance for the language. For example, there is arguably a bias towards the selection of the works of Shakespeare for a corpus of English because they are widely available and considered of importance for the English language. Similarly, it would be very difficult to construct a corpus of documents from the world wide web which was not biased towards those parts of the web which are readily accessible. 

%\item \emph{There must be a way to sample documents from the infinite corpus.} We view the documents in a real world corpus as a finite sample of the infinite hypothetical corpus.

%In order for the finite sample to be representative of the infinite corpus, we must assume that the documents are sampled at random from the infinite corpus. While we may wish to assume that each document in the infinite corpus carries equal weight, this is not in fact possible since there is no uniform distribution over a countably infinite set.\footnote{If the elements in such a set are assigned equal values, the sum of these values will either be 0 or $\pm\infty$ and so cannot form a probability distribution.} The finite corpus (sample) must therefore be associated with a method of choosing documents from the infinite corpus which biases certain documents over others. That is, there must be some probability distribution over the documents in the infinite corpus by which it was sampled to produce the finite corpus. There are at least two interpretations that can be given to this probability distribution:
%\begin{itemize}
%\item the probability distribution is a feature of the particular method of sampling from the infinite corpus;
%\item the probability distribution is an inherent feature of the corpus and indicates that certain documents have more prominence or importance than others.
%\end{itemize}
%According to the first of these, the infinite  corpus is just a set, and the probability distribution is just a feature of the sampling technique; according to the second, a corpus is inherently a probabilistic object and should be represented as a probability distribution over documents.
%
%Alternatively a combination of the two hypotheses is possible: the hypothetical infinite corpus may prefer certain documents over others, and the sampling technique may then bias the sample in unexpected ways. It is this explanation that we prefer: in our view it is reasonable to suppose that some documents should naturally be given more prominence in the hypothetical corpus, but that real world corpora are rarely truly and comprehensively a representative sample of the language in question and hence the hypothetical corpus. Our final assumption, then is
%\item Documents in the corpus are of varying importance
\end{enumerate}

These assumptions can be summarised by the statement: \emph{The correct representation of an infinite corpus is a probability distribution over the strings of symbols of the language.}
We formalise this in the following definition:
\begin{defn}[Corpus]
Let $A$ be the set of symbols of the language in question. A corpus $p$ is a probability distribution over $A^*$, the set of all strings of symbols from $A$ including the empty string $\epsilon$. That is, $p$ is a function from $A^*$ to $[0,1]$ satisfying $\sum_{x\in A^*} p(x) = 1$.
\end{defn}
The documents of the corpus are those strings $x$ in $A^*$ for which $p(x) > 0$; thus we see that the notion of a corpus as a set of documents is contained in the definition.

\section{Defining Meaning as Context}

We continue with our set of assumptions:

\begin{enumerate}
\item \emph{Data sparseness is not a problem in the hypothetical infinite corpus.} This is an assumption since it is possible to imagine infinite corpora in which data sparseness is still a problem: merely being infinite doesn't guarantee enough data!

\item \emph{The context of an expression in a particular location is everything occurring on either side.} Because of the problem of data sparseness, all techniques that build vectors from contexts that expressions occur make limited use of context as we mean it, for example using a limited window and ignoring the order of words in this window. Instead, we suggest that, since data sparseness is not a problem in our hypothetical situation, we make use of the whole document surrounding the string except the string itself. Specifically we suggest the definition:
\begin{defn}
The context vector $\hat{x}$ of a string $x \in A^*$ in a corpus defined by $p$ is a real-valued function on the set of contexts $A^* \times A^*$, defined by
$$\hat{x}(u,v) = p(uxv).$$
\end{defn}
\end{enumerate}

%We have considered two definitions of meaning as context. The first is non-probabilistic: it deals only with sets of strings. This definition is used extensively in the area of formal languages and automata, and is used to define the ``syntactic monoid''. However it is not flexible enough for our purposes as it does not give a probabilistic treatment, hence we arrived at the second definition.
 
% \subsection{Non-probabilistic Definition}
% 
% In the non-probabilistic definition, meaning is considered to be defined by a language $L \subseteq A^*$ for some finite alphabet $A$. The language can be interpreted as the set of all sentences in a natural language, or an infinite set of documents in some (theoretical) corpus consisiting of all documents that could ever be written.
% 
%The meaning of a string $x \in A^*$ is then defined to be the set
%$$\Cont_L(x) = \{(u,v) : uxv \in L\},$$
%for $u,v \in A^*$. A string $x$ is said to entail a string $y$ if $\Cont_L(x) \subseteq \Cont_L(y)$. This definition is fine, but it does not easily extend to defining degrees of entailment. For that reason, we developed the probabilistic definition.

%\subsection{Probabilistic Definition}

%We define meaning of a string in terms of where it occurs in some theoretical construct which we call a ``corpus''. This corpus is just a set of strings, with a probability attached to each string. The strings can be considered to represent documents in the corpus; in our definition we are not interested in the order in which documents occur in the corpus. The probability value attached to each document can be supposed to indicate the probability of observing a particular document.
%
%The corpus is thus represented by a discrete probability distribution $p$ over strings in $A^*$, for some set $A$, the ``alphabet''. The strings that actually occur in the corpus are those with a non-zero probability.
%
%Our definition of meaning of a string incorporates \emph{all surrounding context within the document}. A ``context'' of a string $s$ then is a pair of strings $(x,y)$ such that $xsy$ occurs in the corpus. The set of all contexts is the set $A^* \times A^*$.The ``meaning of a string'' $x$ with respect to the corpus described by $p$, denoted $\hat{x}$, according to this notion of context can then be formalised as a function on the set of contexts:
%$$\hat{x}(u,v) = p(uxv).$$
%
%These functions can be considered to be elements of the vector lattice of all functions on contexts. They are thus partially ordered by
%$$x \le y \iff \hat{x}(u,v) \le \hat{y}(u,v),$$
%for all $u,v \in A^*$. Clearly, because of the nature of the probabilistic definition, these functions are all positive.
%
%In the subsequent sections, we examine the properties of this definition, aiming towards finding an abstract mathematical characterisation that captures its essential features. This abstract characterisation provides us with a framework in which to explore algebraic representations of meaning, allowing us to define entailment between expressions in a consistent manner and allowing us to interpret algebraic characterisations of language in terms of ``meaning as context''.

%\subsection{Vector Properties}

The context vector $\hat{x}$ is a special case of a function on the set of contexts. The set of functions on a set form a vector space, thus meanings can be thought of as elements of the vector space $\R^{A^* \times A^*}$, with a basis formed from the set of contexts. Since the meaning is formed from a probability distribution, in the vector space they are \emph{positive elements}; that is, each component is greater than or equal to zero with respect to the context basis.

%\section{Strings as operators}

%%A string $w$ can be considered to operate on the vector space of contexts as follows. Let $c$ be a vector in context space, i.e.
%%$c = \sum\alpha_{(x,y)} (x,y)$, where the sum ranges over all contexts $(x,y) \in A^* \times A^*$. Define an operator $\hat{w}$ on a basis context

%We can define an operator $\hat{w}$ on the vector space of contexts, corresponding to a string $w$ by specifying how it acts on basis elements $(x,y)$ of the context space. Specifically, we define
%$$\hat{w}(x,y) = \left\{ \begin{array}{ll}
%(u,y) & \textrm{if $x = uw$ for some $u$}\\
%0 & \textrm{otherwise.}
%\end{array} \right.$$
%This operator captures the properties of the string $w$ with respect to contexts occurring on the right, in the following way:
%\begin{prop}[Context Operators] If $x$ and $y$ are strings in some corpus described by $p$ then the following holds:
%\begin{enumerate}[1.]
%\item The context operator $\hat{x}$ is a lattice homomorphism.
%\item The context vector of $x$ is given in terms of its context operator $\hat{x}$ by:
%$$\Cont_p(x) = \hat{x}\Omega$$
%where $\Omega$ is a vector representing the contexts of the empty string:
%$\Omega(u,v) = p(uv).$
%\item It holds that $\widehat{xy} = \hat{x}\hat{y}$ under the normal composition of operators.
%\end{enumerate}
%\end{prop}

%\begin{proof}

%\end{proof}

\section{Entailment}

This section deals with the question of how to interpret the representations just discussed. What does it mean for meaning to be determined purely in terms of context? How do these relate to logical perceptions of meaning?

The distributional hypothesis states that words with similar meanings occur in similar contexts. We extend that idea in two directions: firstly to strings of words of arbitrary length, and secondly with regards to the directionality of entailment. Specifically, we propose:
\begin{enumerate}
\item \emph{A string fully entails another string if the first occurs with lower probability in all the contexts that the second occurs in.}
\item \emph{If the strings have no contexts in common then the strings do not entail one another.}
\item \emph{If there is some intermediate situation, then there is a \emph{degree} of entailment.}
\end{enumerate}
This is captured mathematically in the following definition of the \emph{degree of entailment}
\begin{defn}[Degree of Entailment]
The degree of entailment $\Ent(x,y)$ between two strings $x$ and $y$ in a corpus defined by $p$ is defined as
$$\Ent(x,y) = \frac{\|\hat{x}\land\hat{y}\|_1}{\|\hat{x}\|_1}.$$
\end{defn}
Entailment in the sense described above then, is specified by the above definition in that full entailment corresponds to a degree of entailment of value 1, no entailment corresponds to a value of 0, and there are degrees between these two extremes.

\section{The Context Algebra}

We now show how, given any corpus defined by $p$, we can construct an algebra which retains all the vector and lattice properties of the contexts of words as they are defined in the original corpus.

Suppose that three sequences $x$, $y$ and $z$ satisfy
$$p(uxv) = p(uyv) + p(uzv)$$
for all sequences $u$ and $v$. In this case, as vectors, $\hat{x} = \hat{y} + \hat{z}$. Now consider prefixing a string $w$ to each of these. Setting $u = u'w$ we have $p(u'wxv) = p(u'wyv) + p(u'wzv)$ for all $u',v$, so $\widehat{wx} = \widehat{wy} + \widehat{wz}$. Clearly this also applies to suffixes; in addition we can generalise this property to any sums of the representations of words, and it is this that allows us to form an algebra from a corpus.

Consider the vector subspace of $\R^{A^* \times A^*}$ generated by the context representations of sequences in corpus $p$; that is the set of vectors that can be written in the form $\sum_i \alpha_i \hat{x}_i$ for some $\alpha_i \in \R$  and $x_i \in A^*$; we call this subspace $\mathcal{A}_0(p)$. Because of the way we define the subspace, there will always exist some basis $\mathcal{B} = \{\hat{b} : b \in B\}$ where $B \subseteq A^*$, and we can define multiplication on this basis by $\hat{b}\cdot\hat{c} = \widehat{bc}$ where $b,c \in B$. Defining multiplication on the basis defines it for the whole vector subspace, where we define multiplication to be linear, making $\mathcal{A}_0$ an algebra.
Then we have
\begin{prop}[Context Algebra]
Multiplication on $\mathcal{A}_0$ is independent of the choice of basis $B$.
\end{prop}
\begin{proof}
%Given two bases given by $B, C \subseteq A^*$, an arbitrary vector $x$ in $\mathcal{A}_0(p)$ can be written as $$x = \sum_i \beta_i \hat{b}_i = \sum_j \xi_j \hat{c}_j$$
%for some $b_i \in B$, $c_j \in C$ and $\beta_i, \xi_j \in \R$.
Given two bases $\mathcal{B , C}$ derived from subsets $B$ and $C$ of $A^*$, we need to show that multiplication in one basis is the same as in the other. We represent two basis elements $\hat{b}_1$ and $\hat{b}_2$ of $\mathcal{B}$ in terms of basis elements of $\mathcal{C}$:
$$\hat{b}_1 = \sum_i \alpha_i \hat{c}_i \quad\text{and}\quad
\hat{b}_2 = \sum_j \beta_j \hat{c}_j,$$
for some $b_i \in B$, $c_j \in C$ and $\alpha_i, \beta_j  \in \R$.
Note that $\hat{b}_1 = \sum_i \alpha_i \hat{c}_i$ means that $p(xb_1y) = \sum_i \alpha_i p(xc_iy)$ for all $x,y \in A^*$. This includes the special case where $y = b_2y'$ so $$p(xb_1b_2y') = \sum_i \alpha_i p(xc_ib_2y')$$ for all $x, y' \in A^*$.
%, or $\widehat{b_1b_2} = \sum_i \alpha_i \widehat{c_ib_2}$.
Similarly, we have $p(xb_2y) = \sum_j \beta_j p(xc_jy)$ for all $x,y \in A^*$ which includes the special case $x = x'c_i$, so $p(x'c_ib_2y) = \sum_j \beta_j p(x'c_ic_jy)$ for all $x',y \in A^*$. Inserting this into the above expression yields
$$p(xb_1b_2y) = \sum_i \alpha_i\beta_j p(xc_ic_jy)$$
for all $x,y \in A^*$ which we can rewrite as
$$\hat{b}_1\cdot\hat{b}_2 = \widehat{b_1b_2} = \sum_{i,j}\alpha_i\beta_j (\hat{c}_i\cdot\hat{c}_j)
= \sum_{i,j}\alpha_i\beta_j \widehat{c_ic_j};$$
thus showing that multiplication is defined independently of what we choose as the basis.
\end{proof}


\end{document} 