%Bismillahi-r-Rahmani-r-Rahim
\documentclass[12pt]{report}

 \newcommand{\Cont}{\mathrm{Cont}}
\include{head}

\begin{document}

\chapter{Meaning as Context}

%\abstract{We examine the consequences of treating meaning purely as context. Specifically, we assume some (possibly infinite) ``corpus'' and define the meaning of a string of words in terms of their occurrence in this corpus. We use this definition to define an entailment relation on strings, and show that the relevant properties of the resulting formalism can be incorporated into \emph{positive operators} on some vector space.}

\section{Introduction}

What is meaning? The idea that sentences can be translated into some ``logical form'' that somehow describes the meaning of the sentence does not square with modern methods in computational linguistics. Methods such as latent semantic analysis and distributional similarity treat words as though they can be represented as vectors; these have proven useful in many applications. Such methods rely on examining the context that words occur in in order to determine something about their meaning. However, confusion arises when we try and make use of these representations. If we wish to maintain a vector representation, there appears to be no standard way in which representations of words can be combined to build representations of phrases and sentences.

Our approach to this problem is to formalise the notion of ``meaning as context''. The idea is that a string can be represented purely by \emph{the contexts in which it occurs in some large corpus}. The construction is theoretical, as we even allow the corpus to be infinite, but it gives us insight into how to combine vector representations of meanings of words.

\section{Defining Meaning as Context}
 
%We have considered two definitions of meaning as context. The first is non-probabilistic: it deals only with sets of strings. This definition is used extensively in the area of formal languages and automata, and is used to define the ``syntactic monoid''. However it is not flexible enough for our purposes as it does not give a probabilistic treatment, hence we arrived at the second definition.
 
% \subsection{Non-probabilistic Definition}
% 
% In the non-probabilistic definition, meaning is considered to be defined by a language $L \subseteq A^*$ for some finite alphabet $A$. The language can be interpreted as the set of all sentences in a natural language, or an infinite set of documents in some (theoretical) corpus consisiting of all documents that could ever be written.
% 
%The meaning of a string $x \in A^*$ is then defined to be the set
%$$\Cont_L(x) = \{(u,v) : uxv \in L\},$$
%for $u,v \in A^*$. A string $x$ is said to entail a string $y$ if $\Cont_L(x) \subseteq \Cont_L(y)$. This definition is fine, but it does not easily extend to defining degrees of entailment. For that reason, we developed the probabilistic definition.

%\subsection{Probabilistic Definition}

%We define meaning of a string in terms of where it occurs in some theoretical construct which we call a ``corpus''. This corpus is just a set of strings, with a probability attached to each string. The strings can be considered to represent documents in the corpus; in our definition we are not interested in the order in which documents occur in the corpus. The probability value attached to each document can be supposed to indicate the probability of observing a particular document.
%
%The corpus is thus represented by a discrete probability distribution $p$ over strings in $A^*$, for some set $A$, the ``alphabet''. The strings that actually occur in the corpus are those with a non-zero probability.
%
%Our definition of meaning of a string incorporates \emph{all surrounding context within the document}. A ``context'' of a string $s$ then is a pair of strings $(x,y)$ such that $xsy$ occurs in the corpus. The set of all contexts is the set $A^* \times A^*$.The ``meaning of a string'' $x$ with respect to the corpus described by $p$, denoted $\hat{x}$, according to this notion of context can then be formalised as a function on the set of contexts:
%$$\hat{x}(u,v) = p(uxv).$$
%
%These functions can be considered to be elements of the vector lattice of all functions on contexts. They are thus partially ordered by
%$$x \le y \iff \hat{x}(u,v) \le \hat{y}(u,v),$$
%for all $u,v \in A^*$. Clearly, because of the nature of the probabilistic definition, these functions are all positive.
%
%In the subsequent sections, we examine the properties of this definition, aiming towards finding an abstract mathematical characterisation that captures its essential features. This abstract characterisation provides us with a framework in which to explore algebraic representations of meaning, allowing us to define entailment between expressions in a consistent manner and allowing us to interpret algebraic characterisations of language in terms of ``meaning as context''.

\subsection{Vector Properties}

The meaning $\hat{x}$ is a special case of a function on the set of contexts. The set of functions on a set form a vector space, thus meanings can be thought of as elements of the vector space $\R^{A^* \times A^*}$, with a basis formed from the set of contexts. Since the meaning is formed from a probability distribution, in the vector space they are \emph{positive elements}; that is, each component is greater than or equal to zero with respect to the context basis.

%\section{Strings as operators}

%%A string $w$ can be considered to operate on the vector space of contexts as follows. Let $c$ be a vector in context space, i.e.
%%$c = \sum\alpha_{(x,y)} (x,y)$, where the sum ranges over all contexts $(x,y) \in A^* \times A^*$. Define an operator $\hat{w}$ on a basis context

%We can define an operator $\hat{w}$ on the vector space of contexts, corresponding to a string $w$ by specifying how it acts on basis elements $(x,y)$ of the context space. Specifically, we define
%$$\hat{w}(x,y) = \left\{ \begin{array}{ll}
%(u,y) & \textrm{if $x = uw$ for some $u$}\\
%0 & \textrm{otherwise.}
%\end{array} \right.$$
%This operator captures the properties of the string $w$ with respect to contexts occurring on the right, in the following way:
%\begin{prop}[Context Operators] If $x$ and $y$ are strings in some corpus described by $p$ then the following holds:
%\begin{enumerate}[1.]
%\item The context operator $\hat{x}$ is a lattice homomorphism.
%\item The context vector of $x$ is given in terms of its context operator $\hat{x}$ by:
%$$\Cont_p(x) = \hat{x}\Omega$$
%where $\Omega$ is a vector representing the contexts of the empty string:
%$\Omega(u,v) = p(uv).$
%\item It holds that $\widehat{xy} = \hat{x}\hat{y}$ under the normal composition of operators.
%\end{enumerate}
%\end{prop}

%\begin{proof}

%\end{proof}

\section{Entailment}

This section deals with the question of how to interpret the representations just discussed. What does it mean for meaning to be determined purely in terms of context? How do these relate to logical perceptions of meaning?

The distributional hypothesis states that words with similar meanings occur in similar contexts. We extend that idea in two directions: firstly to strings of words of arbitrary length, and secondly with regards to the directionality of entailment. Specifically, we propose:
\begin{quote}
\emph{A string fully entails another string if the first occurs with lower probability in all the contexts that the second occurs in. If the strings have no contexts in common then the strings do not entail one another. If there is some intermediate situation, then there is a \emph{degree} of entailment.}
\end{quote}
This is captured mathematically in the following definition of the \emph{degree of entailment}
\begin{defn}[Degree of Entailment]
The degree of entailment $\Ent(x,y)$ between two strings $x$ and $y$ in a corpus defined by $p$ is defined as
$$\Ent(x,y) = \frac{\|\hat{x}\land\hat{y}\|_1}{\|\hat{x}\|_1}.$$
\end{defn}
Entailment in the sense described above then, is specified by the above definition in that full entailment corresponds to a degree of entailment of value 1, no entailment corresponds to a value of 0, and there are degrees between these two extremes.

\section{The Context Algebra}

We now show how, given any corpus defined by $p$, we can construct an algebra which retains all the vector and lattice properties of the contexts of words as they are defined in the original corpus.

Suppose that three sequences $x$, $y$ and $z$ satisfy
$$p(uxv) = p(uyv) + p(uzv)$$
for all sequences $u$ and $v$. In this case, as vectors, $\hat{x} = \hat{y} + \hat{z}$. Now consider prefixing a string $w$ to each of these. Setting $u = u'w$ we have $p(u'wxv) = p(u'wyv) + p(u'wzv)$ for all $u',v$, so $\widehat{wx} = \widehat{wy} + \widehat{wz}$. Clearly this also applies to suffixes; in addition we can generalise this property to any sums of the representations of words, and it is this that allows us to form an algebra from a corpus.

Consider the vector subspace of $\R^{A^* \times A^*}$ generated by the context representations of sequences in corpus $p$; that is the set of vectors that can be written in the form $\sum_i \alpha_i \hat{x}_i$ for some $\alpha_i \in \R$  and $x_i \in A^*$; we call this subspace $\mathcal{A}_0(p)$. Because of the way we define the subspace, there will always exist some basis $\mathcal{B} = \{\hat{b} : b \in B\}$ where $B \subseteq A^*$, and we can define multiplication on this basis by $\hat{b}\cdot\hat{c} = \widehat{bc}$ where $b,c \in B$. Defining multiplication on the basis defines it for the whole vector subspace, where we define multiplication to be linear, making $\mathcal{A}_0$ an algebra.
Then we have
\begin{prop}[Context Algebra]
Multiplication on $\mathcal{A}_0$ is independent of the choice of basis $B$.
\end{prop}
\begin{proof}
%Given two bases given by $B, C \subseteq A^*$, an arbitrary vector $x$ in $\mathcal{A}_0(p)$ can be written as $$x = \sum_i \beta_i \hat{b}_i = \sum_j \xi_j \hat{c}_j$$
%for some $b_i \in B$, $c_j \in C$ and $\beta_i, \xi_j \in \R$.
Given two bases $\mathcal{B , C}$ derived from subsets $B$ and $C$ of $A^*$, we need to show that multiplication in one basis is the same as in the other. We represent two basis elements $\hat{b}_1$ and $\hat{b}_2$ of $\mathcal{B}$ in terms of basis elements of $\mathcal{C}$:
$$\hat{b}_1 = \sum_i \alpha_i \hat{c}_i \quad\text{and}\quad
\hat{b}_2 = \sum_j \beta_j \hat{c}_j,$$
for some $b_i \in B$, $c_j \in C$ and $\alpha_i, \beta_j  \in \R$.
Note that $\hat{b}_1 = \sum_i \alpha_i \hat{c}_i$ means that $p(xb_1y) = \sum_i \alpha_i p(xc_iy)$ for all $x,y \in A^*$. This includes the special case where $y = b_2y'$ so $$p(xb_1b_2y') = \sum_i \alpha_i p(xc_ib_2y')$$ for all $x, y' \in A^*$.
%, or $\widehat{b_1b_2} = \sum_i \alpha_i \widehat{c_ib_2}$.
Similarly, we have $p(xb_2y) = \sum_j \beta_j p(xc_jy)$ for all $x,y \in A^*$ which includes the special case $x = x'c_i$, so $p(x'c_ib_2y) = \sum_j \beta_j p(x'c_ic_jy)$ for all $x',y \in A^*$. Inserting this into the above expression yields
$$p(xb_1b_2y) = \sum_i \alpha_i\beta_j p(xc_ic_jy)$$
for all $x,y \in A^*$ which we can rewrite as
$$\hat{b}_1\cdot\hat{b}_2 = \widehat{b_1b_2} = \sum_{i,j}\alpha_i\beta_j (\hat{c}_i\cdot\hat{c}_j)
= \sum_{i,j}\alpha_i\beta_j \widehat{c_ic_j};$$
thus showing that multiplication is defined independently of what we choose as the basis.
\end{proof}


\end{document} 