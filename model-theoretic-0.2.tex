%Bismillahi-r-Rahmani-r-Rahim
%\documentclass[12pt]{report}

%\include{head}

%\begin{document}


\chapter{Uncertainty in Model Theoretic Semantics}

In this chapter we examine context theoretic semantics as a way of incorporating information about uncertainty of meaning into model-theoretic representations of meaning. We will show that given a way of translating natural language expressions into logical forms, we can define an algebra which represents the meaning equivalently. Given also a way of attaching probabilities to logical forms (which can be given a Bayesian interpretation), we can define a context theory allowing us to deduce degrees of entailment between expressions.

The context theoretic interpretation of ambiguity of meaning provides us with a recipe for representing ambiguity of words as well as syntactic ambiguity within the algebra, providing us with a principled way of reasoning with uncertainty of meaning within the model theoretic framework.

\section{From Logical Forms to Algebra}

Model-theoretic approaches generally deal with a subset of all possible strings, the language under consideration, translating sequences in the language to a logical form, expressed in another, logical language. Relationships between logical forms are expressed by an entailment relation on this logical language.
%This is summarised formally in the following definition:
%\begin{defn}[Translation to Logical Form]
%A \emph{translation to logical form} for some alphabet $A$ is a subset $L$ of $A^*$, together with a logical language $L' \subseteq B^*$ for some alphabet $B$ and a function $\lambda$ that assigns an element of $L'$ to each element of $L$ and a reflexive and transitive relation $\vdash$ on $L'$ called entailment.
%\end{defn}

%Given a translation to logical form for $A$, we can associate with each sequence in the language $L$ a projection on the space $\R^{L'}$ that expresses the information contained in the translation.

This section is about the algebraic representation of logical languages. By a \emph{logical language} we mean a language $L' \subset B^*$ for some alphabet $B$, together with a relation $\vdash$ on $L'$ that is reflexive and transitive; this relation is interpreted as entailment on the logical language. We will show how each element $u \in L'$ can be associated with a projection on a vector space; it is these projections that define the algebra. Later we will show how this can be related to strings in the natural language $L$ that we are interested in.

For a subset $T$ of a set $S$, we define the projection $P_T$ on $\R^S$ by 
$$P_T e_s = \left\{
\begin{array}{ll}
e_s & \text{if $s \in T$}\\
0 & \text{otherwise}
\end{array}
 \right.$$
 Where $e_s$ is the basis element of $\R^S$ corresponding to the element $s\in S$.
Given $u \in L'$, define $\left\downarrow_\vdash(u)\right. = \{v : v \vdash u\}$. As a shorthand we write $P_u$ for the projection $P_{\left\downarrow_\vdash(u)\right.}$ on the space $\R^{L'}$.

The projection $P_u$ can be thought of as projecting onto the space of logical statements that entail $u$. This is made formal in the following proposition:
\begin{prop}\label{modelth}
$P_u \le P_v$ if and only if $u \vdash v$.
\end{prop}
\begin{proof}
%Since $P_x$ and $P_y$ are band projections, they are commutative, and satisfy $P_x \le P_y$ if and only if $P_xP_y = P_x$.
%If $\lambda(x) \vdash \lambda(y)$ then
Clearly
\begin{equation}
\tag{$*$}\label{projectionprod}
P_uP_v e_w = \left\{
\begin{array}{ll}
e_w & \text{if $w \vdash u$ and $w \vdash v$}\\
0 & \text{otherwise}
\end{array},
 \right.
 \end{equation}
so if $u \vdash v$ then since $\vdash$ is transitive, if $w \vdash u$ then $w \vdash v$, so we must have $P_uP_v = P_u$. The projections $P_u$ and $P_v$ are band projections, so $P_uP_v = P_u$ if and only if $P_u \le P_v$.

Conversely, if $P_uP_v = P_u$ then it must be the case that $w \vdash u$ implies $w \vdash v$ for all $w \in L$, including $w = u$. Since $\vdash$ is reflexive, we have $u \vdash u$, so $u \vdash v$ which completes the proof.
\end{proof}

To help us understand this representation better, we wil show that it is closely connected to the ideal completion of partial orders. Define a relation $\equiv$ on $L'$ by $u \equiv v$ if and only if $u \vdash v$ and $v \vdash u$. Clearly $\equiv$ is an equivalence relation; we denote the equivalence class of $u$ by $[u]$. Equivalence classes are then partially ordered by $[u] \le [v]$ if and only if $u \vdash v$. Then note that $\bigcup \down{[u]} = \left\downarrow_\vdash(u)\right.$, thus $P_u$ projects onto the space generated by the basis vectors corresponding to the elements $\bigcup \down{[u]}$, the ideal completion representation of the partially ordered equivalence classes.

\subsection{Application: Propositional Calculus}

Suppose we choose as our logical language $L'$ the language of a propositional calculus with the usual connectives $\lor$, $\land$ and $\lnot$, the logical constants $\top$ and $\bot$ representing ``true'' and ``false'' respectively, with $u \vdash v$ meaning ``infer $v$ from $u$'', behaving in the usual way. Then:
\begin{align*}
P_{u\land v} &= P_uP_v
	& P_{\lnot u} &= 1 - P_u + P_\bot\\
P_{u\lor v} &= P_u + P_v - P_uP_v
	& P_{\top} &=1
\end{align*}
%\begin{align}
%\tag{1} \label{and} P_{u\land v} &= P_uP_v\\
%\tag{2} \label{or} P_{u\lor v} &= P_u + P_v - P_uP_v\\
%\tag{3} \label{not} P_{\lnot u} &= 1 - P_u + P_\bot\\
%\tag{4} \label{true} P_{\top} &=1
%\end{align}

To see this, note that the equivalence classes of $\vdash$ form a Boolean algebra under the partial ordering induced by $\vdash$, with
\begin{align*}
[u\land v] & = [u] \land [v]
 & [u\lor v] & = [u] \lor [v]
  & [\lnot u] & = \lnot[u].
\end{align*}
Note that while the symbols $\land$, $\lor$ and $\lnot$ refer to logical operations on the left hand side, on the right hand side they are the operations of the Boolean algebra of equivalence classes; they are completely determined by the partial ordering associated with $\vdash$.\footnote{In the context of model theory, the Boolean algebra of equivalence classes of sentences of some theory $T$ is called the \emph{Lindenbaum-Tarski} algebra of $T$ \citep{Hinman:05}.}

Since the partial ordering carries over to the ideal completion we must have
\begin{align*}
\downnb{[u\land v]} &= \downnb{[u]} \cap \downnb{[v]}
& \downnb{[u\lor v]} &= \downnb{[u]} \cup \downnb{[v]}
\end{align*}
Since $u \vdash \top$ for all $u\in L'$, it must be the case that $\downnb{[\top]}$ contains all sets in the ideal completion. However the Boolean algebra of subsets in the ideal completion is larger than the Boolean algebra of equivalence classes; the latter is embedded as a Boolean sub-algebra of the former. Specifically, the least element in the completion is the empty set, whereas the least element in the equivalence class is represented as $\downnb{[\bot]}$. Thus negation carries over with respect to this least element:
$$\downnb{[\lnot u]} = (\downnb{[\top]} - \downnb{[u]})\cup \downnb{[\bot]}.$$

We are now in a position to prove the original statements:
\begin{itemize}
\item Since $\downnb{[\top]}$ contains all sets in the completion, $\bigcup\downnb{[\top]} = \downe{\top} = L'$, and $P_\top$ must project onto the whole space, that is $P_\top = 1$.
\item Using the above expression for $\downnb{[u \land v]}$, taking unions of the disjoint sets in the equivalence classes we have $\downe{u\land v} = \downe{u} \cap \downe{v}$. Making use of \eqref{projectionprod} in the proof to Proposition \ref{modelth}, we have $P_{u\land v} = P_uP_v$.
\item In the above expression for $\downnb{[\lnot u]}$, note that $\downnb{[\top]} \supseteq \downnb{[u]} \supseteq{\downnb{[\bot]}}$. This allows us to write, after taking unions and converting to projections, $P_{\lnot u} = 1 - P_u + P_\bot$, since $P_\top = 1$.
\item Finally, we know that $u\lor v \equiv \lnot(\lnot u \land \lnot v)$, and since equivalent elements in $L'$ have the same projections we have
\begin{align*}
P_{u\lor v} &= 1 - (P_{\lnot u \land \lnot v}) + P_\bot\\
		&= 1 - (P_{\lnot u}P_{\lnot v}) + P_\bot\\
		&= 1 - (1 - P_u + P_\bot)(1 - P_v + P_\bot) + P_\bot\\
		&= P_u + P_v - P_u P_v - 2P_\bot + P_\bot P_u + P_\bot P_v\\
		&= P_u + P_v - P_u P_v
\end{align*}
\end{itemize}

It is also worth noting that in terms of the vector lattice operations $\lor$ and $\land$ on the space of operators on $\R^{L'}$, we have $P_{u\lor v} = P_u \lor P_v$ and $P_{u\land v} = P_u \land P_v$.


\section{Representing Uncertainty}

There are many situations in computational linguistics where we are inevitably faced with uncertainty. For example, given a long sentence in English, the chances of a parser choosing one of the parses that would be identified by humans as correct is minimal; with modern statistical parsers we are left instead with a probability distribution over parses. Similarly, the task of word sense disambiguation is a long way from being solved; again, the best we can do is assign a probability distribution over the senses of a word given a particular context. Thus it seems that syntactic and lexical ambiguity are problems that can only be partially solved, at least using current techniques: there is no indication that a satisfying solution to these problems will be found in the near future. One can also debate whether attempting to resolve these ambiguities completely is entirely correct; some text is arguable inherently ambiguous.

This syntactic and lexical ambiguity results in \emph{semantic ambiguity} --- we are uncertain of the intended meaning of an expression. We believe the correct approach to this problem (in the context of traditional, logical representations of language) is to incorporate ambiguity into the representation of meaning itself. This would allow us to reason about our uncertainty of the intended meaning of expressions in a logical manner, and ultimately lead to \emph{semantic disambiguation}, in which the semantic ambiguity of an expression is reduced by analysing the possible meanings resulting from syntactic and lexical ambiguity.

We are particularly interested in incorporating statistical information about uncertainty. As far as we are aware, the problem of representing ambiguity in the context of statistical methods has not been tackled theoretically. We will show how the context theoretic approach, together with the ideas developed in the last section lead almost directly to a method of dealing with semantic ambiguity.

\subsection{Requirements for Representing Ambiguity}

In the context of logical representations of meaning, there are certain properties that we would expect from representations of ambiguity; we list and discuss these here.

\subsubsection*{Bayesianism}

We would expect our representation to be tied closely to Bayesian reasoning, since this is the standard approach to reasoning with uncertainty. Bayesianism asserts that the correct calculus for modelling uncertainty is the mathematics of probability theory. A ``probability'' assigned to a sentence is then merely taken as an indication of our certainty of the truth of the sentence; it is not intended to be a scientifically measurable quantity in the sense that probabilities are often assumed to be. We would expect to be able to incorporate such probabilities into our system

\subsubsection*{Ambiguity and Logic}

When dealing with ambiguity in the context of logical representations, we expect certain relationships between the representation of ambiguity and the logical representations. Specifically, if we have an ambiguous expression with two meanings, we would expect the ambiguous representation to entail the logical disjunction of two expressions. In general, we would not expect the converse, since the two are not equivalent. To see this, for example, consider the sentence $s$ = ``He saw a plant''. We wish to represent the lexical ambiguity in the word ``plant'' which we will consider can either mean an industrial plant or an organism. The two disambiguated meanings roughly correspond to the sentences $s_1$ = ``He saw an industrial plant'' and $s_2$ = ``He saw a plant organism''. We expect that each disambiguated sentence $s_i$ entails the ambiguous sentence $s$, if not completely, then to some degree. 

\subsubsection*{Statistical Features}

Similarly, we would expect the ambiguous sentence to entail the disambiguated meanings to the degree that we expect the ambiguous word to carry the relevant sense. For example, if ``plant'' is used in the sense of industrial plant 40\% of the time, then we would expect that $s$ entails $s_1$ to degree 0.4.

\subsection{Representing Bayesian Uncertainty}

The projection representation of translations to logical form allows us to associate an algebra (of projections) with the logical language $L'$, however it does not quite give us a context theory. For that, we need a linear functional on the algebra of projections, and we will show how this can be done if we take a Bayesian approach to reasoning.

We need to associate probabilities with (logical) sentences in a way that is compatible with their logical structure. This can be done using a probability distribution over the sentences of the logical language:
%\begin{defn}

%\end{defn}
%%Then it is easy to see the following:
%%\begin{prop}
%%$p$ defines a probability measure $P'$ on the ideal completion of classes of equivalent statements in $L'$ by $P'(X) = \sum_{u \in \bigcup X} p(u)$ where $X$ is a set of equivalence classes in the ideal completion of equivalence classes; then $P'(\downnb{[u]}) = P(u)$.
%%\end{prop}
%%Thus $P$ is compatible with the logical structure defined by $\vdash$, and if $u \equiv v$ in $L'$ then $P(u) = P(v)$.

%Then $p_\vdash$ behaves like a probability measure with respect to the partial order structure of equivalence classes:

Let $L'$ be a logical language with entailment relation $\vdash$, a probability distribution $p$ over elements of $L'$ and a distinguished element $\bot \in L'$ such that
\begin{itemize}
\item $\bot \vdash u$, and
\item $p(u) = 0$ if $u \vdash \bot$,
\end{itemize}
for all $u \in L'$. For an arbitrary subset $X$ of $L'$, define $p(X) = \sum_{u\in X}p(u)$. For $u \in L'$ define $p_\vdash(u) = p(\downe{u})$. Then:
\begin{prop}
The function $p_\vdash$ defines a probability measure on the lattice defined by $\downarrow_\vdash$. Specifically:
\begin{enumerate}
\item $p_\vdash(\bot) = 0$
\item if $\downe{u} \cap \downe{v} = \downe{\bot}$ then $p(\downe{u} \cup \downe{v}) = p_\vdash(u) + p_\vdash(v)$
\end{enumerate}
\end{prop}
\begin{proof}
\mbox{}
\begin{enumerate}
\item $p_\vdash(\bot) = \sum_{u\vdash\bot}p(u) = 0$.
\item if $\downe{u} \cap \downe{v} = \downe{\bot}$ then $p(\downe{u} \cup \downe{v}) = p_\vdash(u) + p_\vdash(v) - p_\vdash(\bot) =  p_\vdash(u) + p_\vdash(v)$
\end{enumerate}
\end{proof}
That is, $p_\vdash$ behaves like a probability measure with respect to the partial order structure induced by $\vdash$. We can now define a linear functional on the algebra of projections:
\begin{defn}
Given a probability distribution $p$ over a logical language $L'$, we define a vector $\hat{p}$ in $\R^{L'}$ by
$$\hat{p} = \sum_{u \in L'} p(u)e_u$$
We define a linear functional $\phi$ on  the space of bounded operators on $\R^{L'}$  by
$$\phi(F) = \|F_+(\hat{p})\|_1 - \|F_-(\hat{p})\|_1$$
where $F_+$ and $F_-$ are the positive and negative parts of the bounded operator $F$ respectively.
\end{defn}
\begin{prop} If $u \in L'$ for some logical language $L'$ with probability distribution $p$, then
$\phi(P_u) = p_\vdash(u)$
\end{prop}
\begin{proof}
$$\phi(P_u) = \|P_u\hat{p}\|_1 = \sum_{v \in \downe{u}}p(v) = p_\vdash(u)$$
\end{proof}

%$$\phi(F) = \sum_{u \in L'} (F\hat{p})_u$$
%Where by $a_u$ we mean the component of the vector $a$ in the direction $e_u$; i.e.~write $a = \sum_{u \in \R^{L'}} \alpha_u e_u$, then $a_u = \alpha_u$.
%\end{defn}
%The linear functional $\phi$ is such that for positive operators $F$, $\phi(F) = \|F\hat{p}\|_1$.

\subsection{Context-theoretic Logic}

Let us first consider a language $L\subseteq A^*$ which is unambiguous: we can associate with each $x \in L$ a single element $\mu(x) \in L'$ for some logical language $L'$ which represents the meaning of $x$. We will also assume that if $x,y \in L$ then $xy \notin L$, for all $x,y \in L$, so that we can write each element of $L^*$ as a string of elements of $L$ in one way only, and that we have a probability distribution $p$ over elements of $L'$.

\begin{defn}[Pseudo-Corpus Model]
We define a function $C$ on $A^*$ in the following way. If $x \in L^*$, we write $x = x_1x_2\ldots x_n$ where $x_i \in L$ for $1\le i \le n$, and define
$$C(x) = \phi(P_{\mu(x_1)}P_{\mu(x_2)}\ldots P_{\mu(x_n)}).$$
If $x \in A^* - L^*$ we define $C(x) = 0$.
\end{defn}
If $L'$ is a language with conjunction $\land$ which satisfies $P_{u}P_{v} = P_{u\land v}$ then $C(x)$ represents the probability of the conjunction of the individual sentences that make up $x$.

In general, we can't use $C$ to define a corpus model, since, for example, $C(x^n) = C(x)$ for all $x\in L^*$ and $n \ge 1$, thus it is impossible for us to renormalise $C$ to get a probability distribution over elements of $L^*$. It is, however, possible for us to define a context algebra in terms of $C$ in exactly the same way as before.

\begin{prop}
In the context algebra defined by $C$, $\hat{x} \le \hat{y}$ if and only if $\mu(x) \vdash \mu(y)$, for $x, y \in L$.
\end{prop}

\begin{proof}
We may ignore contexts of the form $(a,b)$ where either $a$ or $b$ (or both) are elements of $A^* - L^*$ since in this case $C(axb)$ will be zero for all $x \in L^*$.

If $\mu(x) \vdash \mu(y)$ then $P_{\mu(x)} \le P_{\mu(y)}$. Thus, for all $a,b \in L^*$, $C(axb) = \phi(P_aP_{\mu(x)}P_b) \le \phi(P_aP_{\mu(y)}P_b) = C(ayb)$, where $P_a$ and $P_b$ are the products of the projections representing the individual sentences in $a$ and $b$ respectively (so $P_a = P_{\mu(a_1)}\ldots P_{\mu(a_n)}$) and thus $\hat{x} \le \hat{y}$. Conversely, if $\hat{x} \le \hat{y}$, then it must be the case that for all $a,b \in L^*$, $\phi(P_aP_{\mu(x)}P_b) \le \phi(P_aP_{\mu(y)}P_b)$.

which can only be true if $P_x \le P_y$ and hence $x\vdash y$.
\end{proof}

%We then define a context theory for $A^*$ in terms of $\mu$ by defining $\hat{x} \in L^1(\R^{L'})$ by
%$$\hat{x} = 

\subsection{Representing Lexical Ambiguity}

\subsection{Representing Syntactic Ambiguity}

%\bibliographystyle{plainnat}
%\bibliography{contexts.bib}

%\end{document} 